<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="xiaolongc">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="xiaolongc">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="xiaolongc">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/">





  <title>xiaolongc</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">xiaolongc</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/27/Decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/27/Decision-tree/" itemprop="url">1. Decision-tree</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-27T08:16:21+08:00">
                2019-07-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>目前，工作中接触最多的就是树模型，为了加深对树模型的理解，是时候将树模型总结总结了。</p>
<p>决策树：可以认为是 if-then 规则集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</p>
<p>学习时，利用训练数据，根据损失函数最小化建立决策树模型；预测时，利用决策树模型对新数据分类。</p>
<p>决策树学习通常包括3个步骤：</p>
<font color="red">特征选择、决策树生成和决策树修剪。</font>

<p>决策树主要思想来自于：Quinlan 在1986提出的 ID3 和 1993年提出的C4.5，以及由 Breiman 等人在1984年提出的CART算法。</p>
<h1 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h1><p>决策树由结点(node)和有向边(directed edge)组成。结点有两类：内部结点(internal node, 表示一个特征或属性)和叶结点(leaf node, 表示一个类)。</p>
<p>可以将决策树看做是一个 if-then 规则：即决策树的根节点到叶结点的每一条路径为一条规则：路径上内部结点特征对应着规则的条件，而叶结点的类对应则规则的结论。</p>
<p>也可以将决策树表示给定特征条件下类的条件概率分布：这一条件概率分布定义在特征空间的一个划分上，将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布，即构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。</p>
<p>李航博士将统计学习的三要素为：方法=模型+策略+算法</p>
<ul>
<li><font color="red">模型：</font>决策树。</li>
<li><font color="red">策略：</font>以损失函数(通常是正则化的极大似然函数)为目标函数的最小化。</li>
<li><font color="red">算法：</font>通常采用启发式方法，近似求解最优化问题。</li>
</ul>
<p>决策树学习的过程通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类过程，这一过程对应着对特征空间的划分，也对应着决策树的构建。</p>
<p>决策树学习算法包含<font color="Tomato">特征选择、决策树生成、决策树剪枝过程</font>。其中决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。</p>
<p>常用的算法包含有 ID3、C4.5 与 CART，下面分别介绍三种算法的决策树学习的特征选择、决策树的生成和剪枝过程。</p>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>直观上，如果一个特征具有更好的分类能力，或者按照这个特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就应该选择这个特征，信息增益(information gain)就能够很好地表示这一直观准则。</p>
<p>下面依次介绍与信息增益有关的几个概念：</p>
<font color="red">熵(entropy): </font>用来表示随机变量的不确定性的度量。设X是一个取值有限的离散变量，概率分布为:<br><center>$$\mathit{P(X = x_{i})=p_{i}, i=1,2,…,n}$$</center><br>则随机变量 X 的熵定义为：<br><center>$$\mathit{H(X)=-\sum_{i=1}^{n}p_{i}logp_{i}}$$</center><br>如果 p<sub>i</sub>=0, 则定义 0log0=0. 由上面可以看出，熵只依赖于 <em>X</em> 的分布，而与 <em>X</em> 的取值无关，所以也可以将 X 的熵记为 <em>H(p)</em>, 即：<br><center>$$\mathit{H(p)=-\sum_{i=1}^{n}p_{i}logp_{i}}$$</center><br>熵越大，随机变量的不确定性就越大，从定义可以看出：<br><center>$$\mathit{0\leqslant H(p)\leqslant logn}$$</center><br>当随机变量只有两个取值时，则 <em>X</em> 的分布为：<br><center>$$\mathit{P(X=1)=p,P(X=0)=1-p,0\leqslant p\leqslant 1}$$</center><br>熵为：<br><center>$$\mathit{H(p)=-plog_{2}p-(1-p)log_{2}(1-p)}$$</center><br>其中 <em>H(p)</em> 随概率 <em>p</em> 变化的曲线如图所示:<br><br><img src="/images/2019/picture/Decision-tree/1.png" alt><br><br><font color="red">联合概率分布: </font>设有随机变量 <em>(X, Y)</em>，其联合概率分布为：<br><center>$$\mathit{P(X=x_{i},Y=y_{i})=p_{ij},i=1,2,…,n;j=1,2,…,m}$$</center><br><font color="red">条件熵: </font> <em>H(Y|X)</em>表示在已知随机变量 <em>X</em> 的条件下随机变量 <em>Y</em> 的不确定性. 随机变量 <em>X</em> 给定条件下的随机变量 <em>Y</em> 的条件熵(conditional entropy) <em>H(Y|X)</em>, 定义为 <em>X</em> 给定条件下 <em>Y</em> 的条件概率分布的熵对 X 的数学期望：<br><center>$$\mathit{H(Y|X)=\sum_{i=1}^{n}p_{i}H(Y|X=x_{i})}$$</center><br>其中，$$\mathit{p_{i}=P(X=x_{i}),i=1,2,…,n.}$$<br><br>当熵和条件熵中的概率由 <strong>数据估计(特别是极大似然估计)</strong> 得到时，所对应的的熵分别为经验熵(empirical entropy) 和经验条件熵(empirical conditional entropy).<br><br><font color="red">信息增益: </font>特征<em>A</em>对训练数据集<em>D</em>的信息增益<em>g(D,A)</em>, 定义为集合D的经验熵<em>H(D)</em>与特征<em>A</em>给定条件下<em>D</em>的经验熵<em>H(D/A)</em>之差，即：<br><center>$$\mathit{g(D,A)=H(D)-H(D|A)}$$</center><br>信息增益表示了得知特征<em>X</em>的信息，使得类<em>Y</em>的信息不确定性减少的程度。<br><br>一般地，熵<em>H(Y)</em>与条件熵<em>H(Y|X)</em>之差称为 <strong>互信息(mutual information)</strong>. 决策树学习中的信息增益等价于训练数据集中类与特征的互信息。<br><br><font color="orange"><strong>决策树学习应用信息增益准则选择特征。</strong> 给定训练数据集D和特征A，<code>经验熵 H(D)</code> 表示对数据集D进行分类的不确定性，而 <code>经验条件熵 H(D|A)</code> 表示在特征A给定条件下对数据集D进行分类的不确定性。那么他们的差，即 <strong>信息增益，表示由于特征A而使得数据集D的分类不确定性减少的程度。</strong></font>

<p>设训练数据集为D，|D|表示其样本容量，即样本个数。设有 <em>K</em> 个类 $ C_{k},k=1,2,…,K, |C_{k}| $ 为属于类 $ C_{k} $ 的样本个数，$ \mathit{\sum_{k=1}^{K}|C_{k}|=|D|.} $ 设特征 <em>A</em> 有n个不同的取值 $ \mathit{(a_{1},a_{2},…,a_{n}.)}, $ 根据特征 A 的取值将D划分为 n 个子集  $ \mathit{(D_{1},D_{2},…,D_{n}), |D_{i}|} $ 为 $ D_{i} $ 的样本个数， $ \mathit{\sum_{i=1}^{n}|D_{i}|=|D|}. $ 记子集 $ D_{i} $ 中属于类 $ C_{k} $ 的样本集合为 $ D_{ik}, $ 即 $ D_{ik}=D_{i}\bigcap C_{k} $ , $ |D_{ik}| $ 为 $ D_{ik} $ 的样本个数。于是信息增益的算法为:</p>
<font color="gray"><br>输入：训练数据集D和特征A;<br><br>输出：特征A对训练数据集D的信息增益 g(D,A).<br><br>(1) 计算数据集 D的经验熵 H(D)<br><center>$$ \mathit{H(D)=-\sum_{i=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}} $$</center><br>(2) 计算特征A对数据集D的 <code>经验条件熵H(D|A)</code><br><center>$$\mathit{H(D|A)=\sum_{i=1}^{n}p_{i}H(D|A=a_{i})=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D|A=a_{i})}$$</center><br>即：<br><center>$$\mathit{H(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D_{i})=- \sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_{i}|}log_{2}\frac{|D_{ik}|}{|D_{i}|}}$$</center><br>(3) 计算信息增益<br><center>$$\mathit{g(D,A)=H(D)-H(D,A)}$$</center><br></font>

<h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><font color="red">信息增益比: 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比(information gain ratio)可以对这一问题进行纠正。</font> 

<p>特征A对训练数据集的信息增益比 $ g_{R}(D,A) $ 定义为信息增益 $ g(D,A) $ 与训练数据集D关于特征A的值的的经验熵 $ H_{A}(D) $ 之比：</p>
<center>$$\mathit{g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}}$$</center>

<p>其中：</p>
<center>$$\mathit{H_{A}(D)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}}$$</center>


<h1 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h1><h2 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h2><p>输入：训练数据集D，特征集A, 阈值 $ \varepsilon $ ;</p>
<p>输出：决策树T.</p>
<ul>
<li>(1) 若D中所有实例属于同一类 $ C_{k} $ ,则T为单结点树，并将类 $ C_{k} $ 作为结点的类标记，返回T;</li>
<li>(2) 若 $ A=\Phi $ ,则T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(3) 否则，计算A中各个特征对D的<font color="red">信息增益</font>，选择信息增益最大的特征 $ A_{g} $ ;</li>
<li>(4) 如果 $ A_{g} $ 的信息增益小于阈值 $ \Phi $ , 则置T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(5) 否则，对 $ A_{g} $ 的每一可能值 $ a_{i} $ , 依 $ A_{g}=a_{i} $ 将D分割为若干非空子集 $ D_{i} $ , 将 $ D_{i} $ 中实例最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T;</li>
<li>(6) 对第i个子结点，以 $ D_{i} $ 为训练集，以 $ A-{A_{g} } $ 为特征集，递归地调用步(1)~(5), 得到子树 $ T_{i} $ , 返回 $ T_{i} $ .</li>
</ul>
<h2 id="ID3算法的不足"><a href="#ID3算法的不足" class="headerlink" title="ID3算法的不足"></a>ID3算法的不足</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a></p>
<ul>
<li>1) 无法处理连续特征。</li>
<li>2) 采用信息增益的特征缺点：取值比较多的特征比取值较少的特征的信息增益大。</li>
<li>3) 无法处理缺失值。</li>
<li>4) 没有考虑过拟合问题。</li>
</ul>
<h2 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h2><p>输入：训练数据集D，特征集A, 阈值 $ \varepsilon $ ;</p>
<p>输出：决策树T.</p>
<ul>
<li>(1) 若D中所有实例属于同一类 $ C_{k} $ ,则T为单结点树，并将类 $ C_{k} $ 作为结点的类标记，返回T;</li>
<li>(2) 若 $ A=\Phi $ ,则T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(3) 否则，计算A中各个特征对D的<font color="red">信息增益比</font>，选择信息增益最大的特征 $ A_{g} $ ;</li>
<li>(4) 如果 $ A_{g} $ 的信息增益小于阈值 $ \Phi $ , 则置T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(5) 否则，对 $ A_{g} $ 的每一可能值 $ a_{i} $ , 依 $ A_{g}=a_{i} $ 将D分割为若干非空子集 $ D_{i} $ , 将 $ D_{i} $ 中实例最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T;</li>
<li>(6) 对第i个子结点，以 $ D_{i} $ 为训练集，以 $ A-{A_{g} } $ 为特征集，递归地调用步(1)~(5), 得到子树 $ T_{i} $ , 返回 $ T_{i} $ .</li>
</ul>
<h2 id="C4-5对ID3的改进"><a href="#C4-5对ID3的改进" class="headerlink" title="C4.5对ID3的改进"></a>C4.5对ID3的改进</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a></p>
<ul>
<li>1) 不能处理连续特征的解决方法：C4.5将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为 $ a_{1},a_{2},…,a_{m}, $ 则C4.5取相邻两样本的平均值，一共取得m-1个划分点，其中第i个划分点 $ T_{i} $ 表示为： $ \mathit{T_{i}=\frac{a_{i}+a_{i+1}}{2}}. $ 对于这 m-1 个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的<font color="orange">二元离线分类点</font>。比如取到的增益最大的点为 $ a_{t}, $ 则小于 $ a_{t} $ 的值为类别1，大于 $ a_{t} $ 的值为类别2，这样我们就做到了<font color="orange">连续特征的离散化</font>。要注意的是，与离散属性不同的是，<font color="orange">如果当前节点为连续属性，则该属性后面还可以参与子结点的产生选择过程。</font> <strong>这里应该也采用的是信息增益比吧？</strong></li>
<li>2) 采用信息增益比来解决。</li>
<li>3) 对于确实值的处理，主要解决是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。</li>
<li><ul>
<li>3.1）对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。</li>
</ul>
</li>
<li><ul>
<li>3.2) 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。</li>
</ul>
</li>
<li>4) C4.5引入了正则化系数进行初步的剪枝。</li>
</ul>
<h2 id="C4-5的不足"><a href="#C4-5的不足" class="headerlink" title="C4.5的不足"></a>C4.5的不足</h2><ul>
<li>1) 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝.思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝.后面在下篇讲CART树的时候我们会专门讲决策树的减枝思路，主要采用的是后剪枝加上交叉验证选择最合适的决策树。</li>
<li>2) C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。</li>
<li>3) C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</li>
<li>4) C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。</li>
</ul>
<h1 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h1><p>决策树生成算法递归地产生决策树，容易出现过拟合，其原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构造出过于复杂的决策树。</p>
<p>在决策树学习中将已生成的树进行简化的过程称为剪枝。</p>
<font color="orange">决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)来实现。</font>

<p>设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点有 $ N_{t} $ 个样本点，其中k类的样本点有 $ N_{tk} $ 个，k=1,2,…,K，$ H_{t}(T) $ 为叶结点t上的经验熵，$ \alpha \geq 0 $ 为参数，则决策树学习的损失函数可以定义为：<br>$$ \mathit{C_{\alpha}(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)+\alpha|T|} $$<br>其中经验熵为：<br>$$ \mathit{H_{t}(T)=-\sum_{k}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}} $$<br>令：<br>$$ \mathit{C(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}} $$<br>这时有：<br>$$ \mathit{C_{\alpha}(T)=C(T)+\alpha|T|} $$</p>
<p>上式中，C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数 $ \alpha \geq 0 $ 控制两者之间的影响。较大的 $ \alpha $ 促使选择较简单的模型，较小的 $ \alpha $ 促使选择复杂的模型， $ \alpha =0 $ 意味只考虑与训练数据的拟合程度。</p>
<font color="orange">剪枝，就是当 $ \alpha $ 确定时，选择损失函数最小的模型。上式定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</font>

<p>树的剪枝算法：</p>
<p>输入：生成算法产生的整个树T，参数 $ \alpha $ ;</p>
<p>输出：剪枝后的子树 $ T_{\alpha}. $</p>
<ul>
<li>(1) 计算每个结点的经验熵</li>
<li>(2) 递归地从树的叶结点向上回缩：设一组叶结点回缩到父节点之前与之后的整体树分别为 $ T_{B} $ 与 $ T_{A} $ , 其对应的损失函数分别是 $ C_{\alpha}(T_{B}) $ 与 $ C_{\alpha}(T_{A}) $ , 如果 $ C_{\alpha}(T_{A}) \geq C_{\alpha}(T_{B}) $ 则进行剪枝，即将父节点变为新的叶结点.</li>
<li>(3) 返回(2), 直到不能继续为止，得到损失函数最小的子树 $ T_{\alpha}. $</li>
</ul>
<h1 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h1><p>CART 假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。</p>
<h2 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>为何CART要采用基尼系数：</p>
<p>我们知道，在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。</p>
<p>能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！<font color="orange">CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。</font></p>
<p>具体的，在分类问题中，假设有K个类别，第k个类别的概率为 $ p_{k} $ , 则基尼系数的表达式为：</p>
<center>$$ \mathit{Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}} $$</center><br>如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：<br><center>$$ \mathit{Gini(p)=2p(1-p)} $$</center><br>对于给定的样本D,假设有K个类别, 第k个类别的数量为 $ C_{k} $ ,则样本D的基尼系数表达式为：<br><center>$$ \mathit{Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_{k}|}{|D|})^{2}} $$</center><br>特别的，对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：<br><center>$$ \mathit{Gini(D, A)=\frac{|D_{1}|}{|D|}Gini(D_{1})+\frac{|D_{2}|}{|D|}Gini(D_{2})} $$</center>


<h2 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h2><p>决策树的生成就是递归地构建二叉决策树的过程。特征选择的标准为：</p>
<ul>
<li>对回归树用平方误差最小化准则；</li>
<li>对分类树用基尼指数最小化准则；</li>
</ul>
<h3 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h3><p>回归树的特征处理，连续特征和离线特征同样采用下面的启发式的方法，即选择第j个变量 $ x^{j} $ 和它的取值 s,作为切分变量(splitting variable) 和切分点(splitting point), 并定义两个区域：</p>
<center>$$ \mathit{R_{1}(j,s)=[x|x^{j}\leqslant s], R_{2}(j,s)=[x|x^{j}&gt;s]} $$</center><br>然后寻找最优切分变量j和最优切分点s，具体地，求解：<br><center>$$ \mathit{\min_{j,s}[\min_{c_{1}}\sum_{x_{i}\epsilon R_{1}(j,s)}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\epsilon R_{2}(j,s)}(y_{i}-c_{2})^{2}]} $$</center><br>对于固定输入变量j可以找到最优切分点s.<br><center>$$ c_{1}^{*}=avg(y_{i}|x_{i}\epsilon R_{1}(j,s)),c_{2}^{*}=avg(y_{i}|x_{i}\epsilon R_{2}(j,s)) $$</center>

<p>输入：训练数据集D</p>
<p>输出：回归树f(x)</p>
<font color="orange">在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</font>

<ul>
<li>(1) 寻找最优切分变量 j 与切分点 s, 求解：<center>$$ \mathit{\min_{j,s}[\min_{c_{1}}\sum_{x_{i}\epsilon R_{1}(j,s)}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\epsilon R_{2}(j,s)}(y_{i}-c_{2})^{2}]} $$</center></li>
<li>(2) 用选定的对(j,s)划分区域并决定相应的输出值：<center>$$ \mathit{R_{1}(j,s)=[x|x^{j}\leqslant s], R_{2}(j,s)=[x|x^{j} &gt; s]} $$</center><br><center>$$ \mathit{c_{m}^{*}=\frac{1}{N_{m}}\sum_{x_{i}\epsilon R_{m}(j,s)}y_{i},x\epsilon R_{m},m=1,2} $$</center></li>
<li>(3) 继续对两个子区域调用步骤(1),(2),直到满足停止条件.</li>
<li>(4) 将输入空间划分为M个区域 $ R_{1},R_{2},…,R_{M}, $ 并生成决策树：<center>$$ \mathit{f(x)=\sum_{m=1}^{M}c_{m}^{*}I(x\epsilon R_{m})} $$</center>

</li>
</ul>
<font color="orange">还有个疑问是：回归树对于离线的特征是如何处理的？1.计算基尼系数，这样如何和连续特征进行比较(排除);2.也采用均方差最小的方式，这也有两种情况:1)按是否大于某一值划分为两部分，2)按是否等于某一值划分为两部分。还有个问题是：在使用时模型如何区别连续特征和离散特征(看 sklearn 库中并没有传入标记离散特征或者连续特征的参数。)</font>

<h3 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h3><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>连续值的处理：</p>
<p>具体的思路如下，比如m个样本的连续特征A有m个，从小到大排列为 $ a_{1},a_{2},…,a_{m}, $ 则CART算法取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$ Ti=\frac{a_{i}+a_{i+1}}{2} $ 。对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为 $ a_{t} $ ,则小于 $ a_{t} $ 的值为类别1，大于 $ a_{t} $ 的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与ID3或者C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。</p>
<p>离散值的处理：</p>
<p>回忆下ID3或者C4.5，如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点。这样导致决策树是多叉树。但是CART分类树使用的方法不同，他采用的是不停的二分，还是这个例子，CART分类树会考虑把A分成{A1}和{A2,A3}, {A2}和{A1,A3}, {A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3},然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。</p>
<p>CART 生成算法：</p>
<p>输入：训练数据集D，停止计算的条件(样本个数小于阈值，或者没有特征，或者基尼系数小于阈值。)；</p>
<p>输出：CART决策树。</p>
<ul>
<li>(1) 设结点的训练数据集为D，计算现有特征对该数据集的基尼指数，此时，对每个特征A，将数据集划分为两部分，根据上面介绍的连续值、离散值计算基尼指数。</li>
<li>(2) 选择基尼指数最小的特征A及其对应的切分点a，依最优特征与最优切分点，从现结点生成两个子结点，将训练集依特征分配到两个子结点中去。</li>
<li>(3) 对两个子结点递归调用(1),(2)直到满足停止条件。</li>
<li>(4) 生成CART树。 </li>
</ul>
<font color="orange">还有个疑问是：分类树如何确定离线特征或者连续特征。</font>

<h3 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a>CART 剪枝</h3><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数，算法基本完全一样，这里我们一起来讲。</p>
<p>由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。但是，有很多的剪枝方法，我们应该这么选择呢？CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。</p>
<p>也就是说，CART树的剪枝算法可以概括为两步，第一步是从原始决策树生成各种剪枝效果的决策树，第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。</p>
<p>首先我们看看剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树T,其损失函数为：</p>
<p><center>$$ \mathit{C_{\alpha }(T_{t})=C(T_{t})+\alpha |T_{t}|} $$</center><br>其中，α为正则化参数，这和线性回归的正则化一样。 $ C(T_{t}) $ 为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。 $ |T_{t}| $ 是子树T的叶子节点的数量。</p>
<p>当α=0时，即没有正则化，原始的生成的CART树即为最优子树。当α=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的α，一定存在使损失函数 $ C_{α}(T) $ 最小的唯一子树。</p>
<p>看过剪枝的损失函数度量后，我们再来看看剪枝的思路，对于位于节点t的任意一颗子树Tt，如果没有剪枝，它的损失是:</p>
<p><center>$$ \mathit{C_{\alpha }(T_{t})=C(T_{t})+\alpha |T_{t}|} $$</center><br>如果将其剪掉，仅仅保留根节点，则损失是:</p>
<p><center>$$ \mathit{C_{\alpha }(T)=C(T)+\alpha} $$</center><br>当α=0或者α很小时， $ C_{α}(T_{t}) &lt; C_{α}(T) $ , 当α增大到一定的程度时: $ C_{α}(T_{t}) = C_{α}(T) $ .</p>
<p>当α继续增大时不等式反向，也就是说，如果满足下式：</p>
<p><center>$$ \mathit{\alpha =\frac{C(T)-C(T_{t})}{|T_{t}|-1}} $$</center><br>$ T_{t} $ 和T有相同的损失函数，但是T节点更少，因此可以对子树 $ T_{t} $ 进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点T。</p>
<p>最后我们看看CART树的交叉验证策略。上面我们讲到，可以计算出每个子树是否剪枝的阈值α，如果我们把所有的节点是否剪枝的值α都计算出来，然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。</p>
<p>好了，有了上面的思路，我们现在来看看CART树的剪枝算法。</p>
<p>输入是CART树建立算法得到的原始决策树T。</p>
<p>输出是最优决策子树 $ T_{α} $ 。</p>
<p>算法过程如下：</p>
<ul>
<li>(1) 初始化 $ \alpha _{min}=\infty  $， 最优子树集合 w={T}。</li>
<li>(2) 从叶子节点开始自下而上计算各内部节点t的训练误差损失函数 $ C_{\alpha}(T_{t}) $ (回归树为均方差，分类树为基尼系数)，叶子结点树 $ |T_{t}| $ ,以及正则化阈值 $ \alpha=min(\frac{C(T)-C(T_{t})}{|T_{t}|-1},\alpha _{min}) $ ,更新 $ \alpha _{min} = \alpha $ .</li>
<li>(3) 得到所有节点的α值的集合M。</li>
<li>(4) 从M中选择最大的值 $ \alpha _{k} $, 自上而下的访问子树t的内部结点，如果 $ \frac{C(T)-C(T _{t})}{|T _{t}|-1} \leq \alpha _{k} $ 时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到 $ \alpha _{k} $ 对应的最优子树 $ T _{k} $ .</li>
<li>(5) 最优子树集合 $ \omega = \omega \cup T _{k}, M=M-\alpha _{k} $ .</li>
<li>(6) 如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合 $ \omega $ .</li>
<li>(7) 采用交叉验证在 $ \omega $ 选择最优子树 $ T _{\alpha} $ .</li>
</ul>
<h1 id="对3种不同的树进行总结"><a href="#对3种不同的树进行总结" class="headerlink" title="对3种不同的树进行总结"></a>对3种不同的树进行总结</h1><table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值处理</th>
<th>剪枝</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类，回归</td>
<td>二叉树</td>
<td>基尼系数，均方差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><ul>
<li>总结《统计学习方法》-李航博士笔记</li>
<li>之前看过刘建平大佬写过的系列博客，在此强烈推荐 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树原理</a>。总结过程中，会参考大佬的文章。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/06/Programming-Hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/06/Programming-Hive/" itemprop="url">Hive 编程指南</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-06T15:31:43+08:00">
                2019-07-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index">
                    <span itemprop="name">big data</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>前言</strong></p>
<p>Hive 是 Hadoop 生态系统中必不可少的一个工具，它提供了一种 SQL 语言，可以查询存储在 Hadoop 分布式文件系统(HDFS)中的数据或其他和 Hadoop 集成的文件系统， 如 MapR-FS，Amazon的S3和像 HBase(Hadoop数据库) 和 Cassandra 这样的数据库中的数据。</p>
<h1 id="第1章-基础知识"><a href="#第1章-基础知识" class="headerlink" title="第1章 基础知识"></a>第1章 基础知识</h1><p>搜索引擎公司、电子商务公司、社交网络等许多组织意识到他们所收集的数据是让他们了解他们的用户，提高业务在市场上的表现以及提高基础架构效率的一个宝贵的资源。</p>
<p>Hive提供了一个被称为 Hive 查询语言(HiveQL 或 HQL)的SQL语言，来查询存储在 Hadoop 集群中的数据。Hive 可以将大多数的查询转换为MapReduce任务(job)，进而在介绍一个令人熟悉的 SQL 抽象的同时，拓宽 Hadoop 的课扩展性。</p>
<p>Hive 的劣势：</p>
<ul>
<li>Hive 不支持记录级别的更新，插入或者删除操作。</li>
<li>因为Hadoop是一个面向批处理的系统，而MapReduce任务(job)的启动过程需要消耗较长时间，所以 Hive 查询延时比较严重。</li>
<li>Hive 不支持事务。因此，Hive 不支持OLTP(联机事务处理)所需的关键功能。</li>
</ul>
<p>如果用户需要对大规模数据使用OLTP功能的话，那么应该选择使用一个NoSQL数据库，例如，和 Hadoop 结合使用的 HBase 及 Cassandra。</p>
<h2 id="Hadoop-和-MapReduce-综述"><a href="#Hadoop-和-MapReduce-综述" class="headerlink" title="Hadoop 和 MapReduce 综述"></a>Hadoop 和 MapReduce 综述</h2><p>参考Tom White 《Hadoop权威指南》一书。</p>
<p>MapReduce</p>
<p>MapReduce是一种计算模型，该模型可以将大型数据处理任务分解为很多单个的，可以在服务器集群中并行执行的任务，这些任务的计算结果可以合并在一起来计算最终的结果。</p>
<p>MapReduce编程模型由谷歌开发，两篇经典的论文为：《MapReduce: 大数据之上的简化数据处理》，以及《Google 文件系统》。这两篇论文启发了道-卡丁开发了 Hadoop。</p>
<p>MapReduce这个术语来自于两个基本的数据转换操作: Map过程和reduce过程。MapReduce 计算框架中的输入和输出的基本数据结构是键-值对。</p>
<p>下图介绍了一种 Word Count程序，左边的每个 Input(输入) 框都表示一个单独的文件，默认情况下，每个文档都会触发一个 Mapper 进程进行处理。而在实际场景中，大文件可能会划分为多个部分，每个部分都会被发送给一个 Mapper 进行处理。同时，也有将多个小文件合并为一个部分供某个 Mapper进行处理。</p>
<p><img src="/images/2019/picture/Programming-Hive/1-1.png" alt></p>
<p>Hadoop 神奇的地方一部分在于后面要进行的Sort和Shuffle过程，Hadoop会按照键来对键-值进行排序，然后Shuffle，将所有具有相同键的键-值对分发到同一个Reducer中。这里有很多方式可以决定哪个Reducer获取哪个范围内的键对应的数据。</p>
<h2 id="Hadoop生态系统中的Hive"><a href="#Hadoop生态系统中的Hive" class="headerlink" title="Hadoop生态系统中的Hive"></a>Hadoop生态系统中的Hive</h2><p>下图显示了Hive的主要模块，以及Hive是如何与Hadoop交互工作的。有好几种方式可以与Hive进行交互，例如命令行。或者图形界面：Karmasphere发布的一个商业产品，Cloudera提供的开源 Hue 项目，以及 Qubole 提供的 “Hive即服务”。</p>
<p><img src="/images/2019/picture/Programming-Hive/1-2.png" alt></p>
<p>Hive 发行版中附带的模块有CLI(命令行)，一个称为 Hive 网页界面(HWI)的简单网页界面，以及可通过 JDBC、ODBC 和一个 Thrift 服务器进行编程访问的几个模块。</p>
<p>所有的命令和查询都会进入到Driver模块，通过该模块对输入进行解析编译，对需求的计算进行优化，然后按照指定的步骤执行(通常是启动多个MapReduce任务(job)来执行)。当需要启动 MapReduce job 时，Hive本身是不会生成 Java MapReduce 算法程序的。相反，Hive 通过一个表示 “job执行计划”的XML 文件驱动执行内置的、原生的 Mapper 和 Reducer 模块。</p>
<p>Hive通过和 JobTracker通信来初始化 MapReduce任务(job)，而不必部署在 JobTracker 所在的管理节点上执行。</p>
<p>Metastore(元数据存储)是一个独立的关系型数据库(通常是一个MySQL实例)，Hive 会在其中保存表模式和其他系统元数据。</p>
<h3 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a>Pig</h3><p>Hive的替代工具中最有名的就是 Pig，假设用户的输入数据具有一个或者多个源，而用户需要进行一组复杂的转换来生成一个或者多个输出数据集。如果使用 Hive, 用户可能会使用嵌套查询，但是在某些时刻会需要重新保存临时表来控制复杂度。</p>
<p>Pig被描述为一种数据流语言，而不是一种查询语言，因此，Pig常用于ETL(数据抽取，数据转换，和数据装载)过程的一部分.</p>
<p>参考 Alan Gates 《Pig 编程指南》</p>
<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>如果用户需要 Hive 无法提供的数据特性(如行级别的更新，快速查询响应时间，以及支持事务)的话，那么该怎么办呢？ HBase 是一个分布式的、可伸缩的数据存储，其支持行级别的数据更新，快速查询和行级事务(但不支持多行事务)。</p>
<p>HBase支持的一个重要特性就是列存储，可以像键-值存储一样来使用HBase。HBase使用HDFS(或其他某种分布式文件系统)来持久化存储数据。Hive 现在已经可以和 HBase 结合使用了。</p>
<h3 id="Cascading、Crunch-及其他"><a href="#Cascading、Crunch-及其他" class="headerlink" title="Cascading、Crunch 及其他"></a>Cascading、Crunch 及其他</h3><h2 id="Java-和-Hive-词频统计算法"><a href="#Java-和-Hive-词频统计算法" class="headerlink" title="Java 和 Hive: 词频统计算法"></a>Java 和 Hive: 词频统计算法</h2><p>统计词频：</p>
<pre><code>CREATE TABLE docs (line STRING);
LOAD DATA INPATH &apos;docs&apos; OVERWRITE INTO TABLE docs;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
    (SELECT explode(split(line, &apos;\s&apos;)) AS word FROM docs) w
GROUP BY word
ORDER BY word;
</code></pre><h1 id="第2章-基础操作"><a href="#第2章-基础操作" class="headerlink" title="第2章 基础操作"></a>第2章 基础操作</h1><h2 id="安装方式"><a href="#安装方式" class="headerlink" title="安装方式"></a>安装方式</h2><p><a href="https://xiaolongc929.github.io/2019/03/12/hadoop-spark-install/" target="_blank" rel="noopener">macOS hadoop-spark安装方式</a></p>
<h3 id="本地模式、伪分布式模式、分布式模式"><a href="#本地模式、伪分布式模式、分布式模式" class="headerlink" title="本地模式、伪分布式模式、分布式模式"></a>本地模式、伪分布式模式、分布式模式</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/23/Theory-of-Generalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/23/Theory-of-Generalization/" itemprop="url">6. Theory-of-Generalization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-23T09:55:02+08:00">
                2019-06-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><p>希望成长函数 m<sub>H</sub>(N) 来代替实际的 M ?</p>
<h2 id="Restriction-of-Break-Point"><a href="#Restriction-of-Break-Point" class="headerlink" title="Restriction of Break Point"></a>Restriction of Break Point</h2><p>growth function m<sub>H</sub>(N): max number of dichotomies（即最多能够产生几种不同的 xx 或 oo）。即 hypothesis set 在 N 个点上，最多能够产生多少种 dichotomy. </p>
<p>几种不同类型的成长函数形式：</p>
<p><img src="/images/2019/picture/Theory-of-Generalization/1.png" alt></p>
<p>上面的 2D perceptrons, 虽然不知道成长函数是什么样的类型，但是可以知道 break point 为4.</p>
<p> 下面一种情形为：当最小的 break point k = 2 时，当有 1个点，2个点，3个点的情形：可以证明，当有1个点 N = 1时，可以被成长函数 m<sub>H</sub> shatter(打散),当有2个点的时候，不能够被 shatter ，最大的 dichotomies 小于4，当 N = 3 时，不能被 shatter，最大的 dichotomies 等于4.</p>
<p> <img src="/images/2019/picture/Theory-of-Generalization/2.png" alt></p>
<p>小练习：</p>
<p> <img src="/images/2019/picture/Theory-of-Generalization/3.png" alt></p>
<h2 id="Bounding-Function-Basic-Cases"><a href="#Bounding-Function-Basic-Cases" class="headerlink" title="Bounding Function: Basic Cases"></a>Bounding Function: Basic Cases</h2><p>bounding function B(N, k): maximum possible m<sub>H</sub>(N) when point = k。这个成长函数在 K 那边漏出一线曙光，是它的 Break point, 那这个成长函数最多有多少种 dichotomy 的可能?</p>
<p>限制条件： 最大长度为 N 向量的(o, x)，然而 ‘no shatter’ 任何 长度为k 的子向量。就是不希望看到 2<sup>k</sup> 的组合，即不能出现 shatter，不能将 k 个点，统统都 ko 掉。</p>
<p>例如：上线函数 B(N, 3) 的地方边界有两种：① positive intervals(k=3)；② 1 D perceptrons(k=3)。即不去考虑 positive intervals 以及 1 D perceptrons 的成长函数，而是直接考虑他们的上线：B(N, 3)。</p>
<p>接下来就先证明，上线函数 bounding functions 是像多项式那样的成长。</p>
<p>new goal: B(N, k) ≤ ploy(N) ?</p>
<p>现在的 B function 有两个参数，一个是 N(有几个点), 一个是 K(一线曙光发生的地方).</p>
<p>前面我们知道的情形：</p>
<ul>
<li>B(N, 1) = 1</li>
<li>B(2, 2) = 3(maximum &lt; 4)</li>
<li>B(3, 2) = 4(‘pictorial’ proof previously)</li>
<li>容易知道，当 N &lt; K 时，B(N, k) = 2<sup>N</sup></li>
<li>当 N = K时，B(N, k) = 2<sup>N</sup> - 1</li>
</ul>
<p>B function(实际是成长函数的上线) 查表为：</p>
<p><img src="/images/2019/picture/Theory-of-Generalization/4.png" alt></p>
<h2 id="Bounding-Function-Inductive-Cases"><a href="#Bounding-Function-Inductive-Cases" class="headerlink" title="Bounding Function: Inductive Cases"></a>Bounding Function: Inductive Cases</h2><p>下面开始填上面的 B function 的左下角的部分。</p>
<h2 id="A-Pictorial-Proof"><a href="#A-Pictorial-Proof" class="headerlink" title="A Pictorial Proof"></a>A Pictorial Proof</h2><h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/26/Training-versus-Testing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/26/Training-versus-Testing/" itemprop="url">5. Training-versus-Testing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-26T10:48:34+08:00">
                2019-05-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><p><img src="/images/2019/picture/Training-versus-Testing/0.png" alt></p>
<h2 id="Recap-and-preview"><a href="#Recap-and-preview" class="headerlink" title="Recap and preview"></a>Recap and preview</h2><blockquote>
<p>复习：整个 learning 的流程图如下图所示， learning 从资料 training examples 出发，通过资料以及 hypothesis set，最终从里面选择了一个最终的 hypothesis g(assumption: 训练的资料和最终测试 hypothesis 方法，都来自于同样的一个 distribution)。</p>
</blockquote>
<ul>
<li><p>如果训练集和测试集都来自于同样的一个分布，且 hypothesis set 的集合是有限的，则对于 learning algorithm A 选择的任意一个 g，都可以保证 E<sup>out</sup>(g) ≈ E<sup>in</sup>(g)。</p>
</li>
<li><p>如果 learning algorithm A 找到一个 g, 使得 E<sup>in</sup>(g) ≈ 0，PAC 能够保证 E<sup>out</sup>(g) ≈ 0。（这使得机器学习时间可能的事情）。</p>
</li>
</ul>
<p><strong>这里的两个重要假设是：① 训练集和测试集都来自于同样的一个分布；② PAC 定理。</strong></p>
<p><img src="/images/2019/picture/Training-versus-Testing/1.png" alt></p>
<p>下图总结了前面的4节课中的主要内容：</p>
<ul>
<li>第一节课，期望找到一个 g，使得 g 与 f 很接近。</li>
<li>第二节课，期望找到使 E<sup>in(g) ≈ 0 的方法。</sup></li>
<li>第三节课，了解了批次数据、监督学习以及二元分类的情况(PLA)。</li>
<li>第四节课，将 E<sup>out</sup>(g) ≈ E<sup>in</sup>(g)，连接了一起。</li>
</ul>
<p><img src="/images/2019/picture/Training-versus-Testing/2.png" alt></p>
<p><strong>这里将 learning 拆分为了两个问题，如下图所示：</strong></p>
<p><img src="/images/2019/picture/Training-versus-Testing/3.png" alt></p>
<p>提出问题：M(hypothesis set 的大小) 在上述两个问题中扮演者什么样的角色？</p>
<p>Trade-off(权衡) M：</p>
<p>这里会有两种情况：</p>
<ul>
<li>M 很小的时候：①Yes -&gt; 坏事情 p[BAD] 发生的概率如下图所示，因此，当 M 很小的时候，坏事情(即 E<sup>out</sup>(g) 和 E<sup>in</sup>(g) 不接近)发生的概率就很小。②No -&gt; 当 M 很小的时候，可以选择的算法就很小，未必能够找到一个 g 使得 E<sup>in</sup> 足够小。</li>
<li>M 很大的时候：①No -&gt; 。②Yes -&gt; 选择更多了。</li>
</ul>
<p><img src="/images/2019/picture/Training-versus-Testing/4.png" alt></p>
<p>因此，如果 M 太大的话，未必是一件好事。</p>
<p>对比 M 是有限的还是无线的两种情况：</p>
<p><img src="/images/2019/picture/Training-versus-Testing/5.png" alt></p>
<p><strong>下面的问题也很有意思，他保证了要想 E<sup>out</sup>(g) 和 E<sup>in</sup>(g) 接近的话，至少需要的数据量。</strong></p>
<p><img src="/images/2019/picture/Training-versus-Testing/6.png" alt></p>
<h2 id="Effective-number-of-lines"><a href="#Effective-number-of-lines" class="headerlink" title="Effective number of lines"></a>Effective number of lines</h2><blockquote>
<p>复习：下面复习下 M 的来源，考虑 BAD events 的定义为  E<sup>out</sup>(g) 和 E<sup>in</sup>(g) 相差很远(大于 ε)的如下图所示，当有 M 个可用选择的 f，则可以使用联结的形式将这些 f 联结在一起。</p>
</blockquote>
<p><img src="/images/2019/picture/Training-versus-Testing/7.png" alt></p>
<p>下面的问题是，如果 M 的无限的，怎么办呢？(Uniform Bound Fail?)</p>
<p>现在想，如果有两个比较接近的 hypotheses h<sup>1</sup> ≈ h<sup>2</sup>(比如两条比较接近的线)，这两个比较接近的 h<sup>1</sup> 、 h<sup>2</sup> 的 E<sup>in</sup>是一样的，E<sup>out</sup> 也很接近。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/8.png" alt></p>
<p>这种坏事情是互相叠起来的，像上图中的右边的 B1, B2, B3 所示。但是，我们在使用 Uniform Bound 的时候，并没有考虑叠起来这件事情，所以造成上限是一个 over-estimating。</p>
<p>下面要做的事情就是找出这些坏事情重叠的部分：</p>
<ul>
<li>第一步，能否将无限多的 hypothesis set，分为有限多个类？</li>
<li>如何分类？</li>
</ul>
<p><img src="/images/2019/picture/Training-versus-Testing/9.png" alt><br><img src="/images/2019/picture/Training-versus-Testing/10.png" alt><br><img src="/images/2019/picture/Training-versus-Testing/11.png" alt><br><img src="/images/2019/picture/Training-versus-Testing/12.png" alt></p>
<p>在平面中的点和线而言，以将点划分为不同的类为区分：</p>
<ul>
<li>1个点，有 2 种线</li>
<li>2个点，有 4 种线</li>
<li>3个点，最多有 8 种线</li>
<li>4个点，最多有 14 种线</li>
</ul>
<p>如果将输入的点以线分开的话，得到的有效的线的数量是有限的。</p>
<p>使用 effective(N) 来代替 M。 effective(N) &lt;&lt; 2<sub>N</sub>。N 代表 N 个点(即 N 个样本)。</p>
<p>如果只从输入的点看的话，那么得到线的种类是有限的，把这种有效的线叫做： Effective Number of Lines。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/13.png" alt></p>
<p>就算有无限多条线 M，如果能够将无限多条线分为 effective(N), 比 2<sub>N</sub> 小很多的话，我们就可以保证可能学的到东西。</p>
<h2 id="Effective-number-of-hypotheses"><a href="#Effective-number-of-hypotheses" class="headerlink" title="Effective number of hypotheses"></a>Effective number of hypotheses</h2><h3 id="Dichotomies-Mini-hypotheses"><a href="#Dichotomies-Mini-hypotheses" class="headerlink" title="Dichotomies: Mini-hypotheses"></a>Dichotomies: Mini-hypotheses</h3><p>下面想象有这么一个 hypothesis，它将 X 空间分为 {x, o}，假如有 N 个点(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>)的话，这 N 个点产生了几种{x, 0}组合可能性，将这种组合叫做 <strong>Dichotomy</strong>。</p>
<ul>
<li>a dichotomy(二分的意思): </li>
<li>下面想看看一个 hypothesis set 可以做出多少种 dichotomy 出来，例如其中一个 dichotomy 将所有的点都分为 o。</li>
<li>H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>) :将所有的 dichotomy 放在一起，形成一个集合，用 H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>) 表示。这是 dichotomies，而不是原来的 hypothesis set。</li>
<li>具体的情况，如下图所示：hypotheses H 的大小最多有无限多个，而 dichotomies H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>) 的上限为 2<sub>N</sub>。</li>
<li>接着用 |H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>)| 的大小来代替 M.</li>
</ul>
<p><img src="/images/2019/picture/Training-versus-Testing/14.png" alt></p>
<h3 id="Growth-Function"><a href="#Growth-Function" class="headerlink" title="Growth Function"></a>Growth Function</h3><p>其实，dichotomies size |H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>)| 的大小，是取决于先选好的 (x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>)(想下前面当有3个点时，可能有 8 种组合，在一条线上时有6种组合)。</p>
<p>如果想要剔除掉对先选好的点(即对 X 的依赖)，就算下最终最大有几种就可以了：</p>
<p><img src="/images/2019/picture/Training-versus-Testing/15.png" alt></p>
<p>将最大的 dichotomies size 记录为 m<sup>H</sup>(N)，并将 m<sup>H</sup>(N) 成为 Growth Function(成长函数)。Growth Function 的输出一定是有限的，因为最多为 2<sub>N</sub>。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/16.png" alt></p>
<p>下面要解决的问题是，如何计算 Growth function?</p>
<h4 id="Growth-Function-for-positive-rays"><a href="#Growth-Function-for-positive-rays" class="headerlink" title="Growth Function for positive rays"></a>Growth Function for positive rays</h4><p>输入：一维的实数，<br>hypothesis: 大于阈值为正，小于阈值的为负。</p>
<p>给N个点，最多可以切出多少不一样的 dichotomy？ 可以有 N+1 种 dichotomy。即成长函数为：m<sup>H</sup>(N) = N + 1</p>
<p><img src="/images/2019/picture/Training-versus-Testing/17.png" alt></p>
<p>核心目标是：利用成长函数 m<sup>H</sup>(N) 来代替 M。</p>
<h4 id="Growth-Function-for-positive-intervals"><a href="#Growth-Function-for-positive-intervals" class="headerlink" title="Growth Function for positive intervals"></a>Growth Function for positive intervals</h4><p>m<sup>H</sup>(N) = 1/2 * N<sup>2</sup> + 1/2 N  + 1</p>
<h4 id="Growth-Function-for-convex-sets"><a href="#Growth-Function-for-convex-sets" class="headerlink" title="Growth Function for convex sets"></a>Growth Function for convex sets</h4><p>m<sup>H</sup>(N) = 2<sup>N</sup>，即 “shattered”, 即这 N 个点，被下面这种 hypothesis 打散了。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/18.png" alt></p>
<p><strong>四个不同的成长函数为：</strong></p>
<p><img src="/images/2019/picture/Training-versus-Testing/19.png" alt></p>
<h2 id="Break-point"><a href="#Break-point" class="headerlink" title="Break point"></a>Break point</h2><p>成长函数第一个看起来有一些希望的点在哪里？把这个点叫做 Break point。例如在 perceptrons 中，3个点的时候有8种情况，4个点的时候有14种情况，将第4个点叫做 perceptrons 的 break point。</p>
<p>从 k 是 break point 的时候，k 以后都是 break point。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/20.png" alt></p>
<p>2D perceptrons 的 break point 为：4</p>
<p>break point 和成长函数有一定的关系，见下图的左下角。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/21.png" alt></p>
<h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/05/Feasibility-of-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/05/Feasibility-of-learning/" itemprop="url">4. Feasibility-of-learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-05T08:25:43+08:00">
                2019-04-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><h2 id="learning-is-impossible"><a href="#learning-is-impossible" class="headerlink" title="learning is impossible?"></a>learning is impossible?</h2><p>No free lunch: NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论”什么学习算法更好“毫无意义，因为若考虑所有潜在的问题，则所有的算法一样好. 要谈论算法的相对优劣，必须要针对具体问题；在某些问题上表现好的学习算法，在另一问题上却可能不尽如人意，学习算法自身的归纳偏好与问题是否相配，往往会起到决定性作用. </p>
<h2 id="probability-to-the-rescue"><a href="#probability-to-the-rescue" class="headerlink" title="probability to the rescue"></a>probability to the rescue</h2><p>inferring something unknow:</p>
<p>在不知道瓶子中各种颜色弹珠数量的前提下，考虑一个瓶子中橘色弹珠的比例。</p>
<p>解决方式： sample:</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/1.png" alt></p>
<p>霍夫定理：</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/2.png" alt></p>
<h2 id="connection-to-learning"><a href="#connection-to-learning" class="headerlink" title="connection to learning"></a>connection to learning</h2><p>将下图中的一颗一颗的弹珠，想象为一个一个的样本x, 如果 hypothesis h 与 target f 对样本 x 表现不一致，则将弹珠染为橘色的，否则染为绿色的。</p>
<p>假设现在有一个固定的 h 在手上，则就可以按照上面的规则将弹珠染为橘色或者绿色。</p>
<p>从瓶子中将染好颜色的弹珠抓出来，比如抓出100个弹珠，就相当于抓取出了100个样本 x 出来了，这个做的好处是可以检验 h 在数据集 D 上的表现，有多少样本 x 与 f(x) 与真实值 y 变现的不同。</p>
<p>这样就能从看见的资料中，衡量 hypothesis  h 与 target f 之间的偏离程度。</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/3.png" alt></p>
<blockquote>
<p>个人理解，林老师想表达的意思是：瓶子中的弹珠就像整个数据集，我们没有办法将整个瓶子的弹珠倒出来检查一遍颜色(即不能在整个数据集上验证 hypothesis h )，因此采用独立同分布的方式从瓶子中取出一部分弹珠(即在一部分数据集上验证 h)，来评价 h 与 f 之间的相似程度。</p>
</blockquote>
<p>下图中，有很多的原件，其中 unknown P on X(相当于瓶子)，以一定的几率从瓶子中取样会用在两个地方(即图中的两个虚线)，一个是 training examples，另一个是 h ≈ f，其中 E<sub>in</sub> 表示 in-sample(即从瓶子中取出来的弹珠，手上的弹珠)，E<sub>out</sub> 表示 out-of-sample(即没有采用出来的弹珠，手上以外的瓶子里面的弹珠)，通过 E<sub>in</sub> 能够根据霍夫不等式求出 E<sub>out</sub>。</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/4.png" alt></p>
<ul>
<li>霍夫不等式对所有的 N 与 ε 都是正确的</li>
<li>霍夫不等式不依赖于 E<sub>out</sub>， 即不需要知道 E<sub>out</sub>，也不需要 f 和 P.</li>
<li>根据 probably approximately correct(PAC) 定理， E<sub>in</sub> = E<sub>out</sub>。这个保证来源于E<sub>in</sub> 和 E<sub>out</sub> 都是从分布 P 中产生的。</li>
</ul>
<p><img src="/images/2019/picture/Feasibility-of-learning/5.png" alt></p>
<p>如果在组合的 h 中， E<sub>in</sub>(h) 足够小， learning algorithm A 拿出来一个 h 做为 g  =&gt;  ‘g = f’ PAC。</p>
<p>如果只有一个 hypothesis h，怎么来验证 g 与 f 是否接近呢？ 首先可以将唯一的 h 作为 g, 然后从 P 中抽取一部分样本，来验证 g 与 f 之间是否接近。</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/6.png" alt></p>
<h2 id="connection-to-real-learning"><a href="#connection-to-real-learning" class="headerlink" title="connection to real learning"></a>connection to real learning</h2><p>上面假设了只有 1 个 hypothesis h 时候的验证方式，假设现在有多个 hypothesis h，应该怎么处理呢？</p>
<p>例如下图，有 M 个 hypothesis，其中 h<sub>M</sub> 在 E<sub>in</sub>(h<sub>M</sub>) 中表现都是正确的(即训练集的误差为0)，你要不要选 h<sub>M</sub>。</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/7.png" alt></p>
<blockquote>
<p>图中也形象的说明了， h<sub>2</sub> 中的绿色弹珠最对，即  h<sub>2</sub> 在所有数据上误差最小。因此，如果选择 h<sub>M</sub> 尽管 E<sub>in</sub>(h<sub>M</sub>) 全是正确的(即训练集误差为0), 但是 h<sub>M</sub> 确不是最好的 g（过拟合了）。</p>
</blockquote>
<p>Bad sample (不好的抽样): E<sub>in</sub> 与 E<sub>out</sub> 非常不接近。 hypothesis 变大的时候(即 h 变多的时候)，是会加大 Bad sample 的概率。例如，只有 1 个硬币(即只有1个 hypothesis)，丢5次全为正面的概率为 1/32，但是如果有 150 个硬币(即有150个 h)，其中有5次全为正面的概率为 1 - (31/32)<sup>150</sup>。</p>
<p>霍夫不等式给出了 Bad sample 的概率为，下图是只有一个 h 时候的概率，说明 Bad sample 的概率很小：</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/8.png" alt></p>
<p>但是，如果有多个 h 的时候，Bad sample 的概率就变大很多：</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/9.png" alt></p>
<p>从上图可以看出，霍夫不等式是对一个一个的 h 的保证，即每行的 Bad sample 的概率都很小，但是 learning algorithm A，是从多个 h 中选出一个做为 g(即通过资料 D<sub>1</sub> ，从 h<sub>1</sub> 到 h<sub>M</sub> 中，选择一个 g)，即每列遇到 Bad sample 的概率。</p>
<p>对于 M 个 h，遇到 Bad Data 的上限为：</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/10.png" alt></p>
<p>上面的图片也是表明了，选择 g 的原则为，使 E<sub>in</sub>(g) 尽量小。</p>
<p>下图是一个学习的过程，即主要由两点组成。</p>
<ul>
<li>if |H| = M 是有限的，并且样本数据个数 N 足够大，无论算法 A 怎么选择到的 g, 都能保证 E<sub>out</sub>(g) ≈ E<sub>in</sub>(g)。</li>
<li>if 算法A找到一个 g，使得 E<sub>in</sub>(g) ≈ 0，则 PAC 能够保证 E<sub>out</sub>(g) ≈ 0.</li>
</ul>
<p><img src="/images/2019/picture/Feasibility-of-learning/11.png" alt></p>
<blockquote>
<p>本节主要解决了，在有限的 hypothesis h 的条件下，机器学习有可能做到的证明。下面的几个将处理在无限的 hypothesis h 的条件下，机器学习的证明。</p>
</blockquote>
<h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/31/Types-of-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/31/Types-of-learning/" itemprop="url">3. Types-of-learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-31T22:51:06+08:00">
                2019-03-31
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><h2 id="Learning-with-different-output-space-Y"><a href="#Learning-with-different-output-space-Y" class="headerlink" title="Learning with different output space Y"></a>Learning with different output space Y</h2><ul>
<li>Classification, Multiclass Classification</li>
<li>Regression</li>
<li>Structured Learning</li>
</ul>
<h2 id="Learning-with-different-data-label-yn"><a href="#Learning-with-different-data-label-yn" class="headerlink" title="Learning with different data label yn"></a>Learning with different data label y<sub>n</sub></h2><ul>
<li>Supervised</li>
<li>Unsupervised</li>
<li>Other Unsupervised Learning Problems: clustering/density estimation/outlier detection</li>
<li>Semi-supervised</li>
<li>Reinforcement Learning</li>
</ul>
<h2 id="learning-with-different-protocal-f-xn-yn"><a href="#learning-with-different-protocal-f-xn-yn" class="headerlink" title="learning with different protocal f(xn,yn)"></a>learning with different protocal f(x<sub>n</sub>,y<sub>n</sub>)</h2><h2 id="learning-with-different-input-space-X"><a href="#learning-with-different-input-space-X" class="headerlink" title="learning with different input space X"></a>learning with different input space X</h2><ul>
<li>concrete: sophisticated (and related) physical meaning</li>
<li>raw: simple physical meaning</li>
<li>abstract: no (or little) physical meaning</li>
</ul>
<h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/17/Learning-to-answer-yes-no/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/17/Learning-to-answer-yes-no/" itemprop="url">2. Learning-to-answer-yes-no</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-17T21:42:55+08:00">
                2019-03-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><p>机器学习的过程就是 learning algorithm A 从两个输入：数据 data 以及算法集合 hypothesis set 中找到一个 hypothesis g，使得 g 与真实的 target function f 最接近，这节课的重点就是学习一个具体的 hypothesis H。</p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/1.png" alt></p>
<h2 id="Perceptron-hypothesis-set"><a href="#Perceptron-hypothesis-set" class="headerlink" title="Perceptron hypothesis set"></a>Perceptron hypothesis set</h2><p>一个简单的 Hypothesis Set: Perceptron: 如果决定授权信用卡，则 h(x) = +1， 如果不授权信用卡，则 h(x) = -1.</p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/2.png" alt></p>
<p>从上图中可以明显看出，一个具体的 h，与各个权重 w 以及设定的阈值 threshold 有关。这样的 h 在历史上被称作感知器 perceptron。这些字的来源由早期研究类神经网络而来。</p>
<p>其向量的表示为：</p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/3.png" alt></p>
<p>以2维的向量为例，h 的具体样子为:</p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/4.png" alt></p>
<p>从几何的角度讲，感知器 perceptron 实际上就是平面上的一条条线，因此又被称作 linear classifiers。即：</p>
<pre><code>perceptron &lt;=&gt; linear classifiers
</code></pre><h2 id="perceptron-learning-algorithm-PLA"><a href="#perceptron-learning-algorithm-PLA" class="headerlink" title="perceptron learning algorithm (PLA)"></a>perceptron learning algorithm (PLA)</h2><p>上一节，我们了解了一个可能的 H 的样子，也就是平面上所有的线(或者是高维空间里面的一个平面)。现在的问题是，如何设计一个算法，从所有的线 H 里面选出一条最好的线出来。</p>
<p>一条最好的线 g 的定义就是让 g ≈ f，但是难点在于 f 是未知的。唯一可以确定的是 data 是从 f 中产生的。所以，可以先让 g 在看过的 data 里面与 f 越接近越好，或者最好是一模一样(个人注释：over fitting)。</p>
<p>接下来就是在已经看过的数据 data 里面找一条线，这条线要能够正确的将 data 分开。难点在于 H 是无限的。</p>
<p>所以解决的方式是，任意找一条线，然后再依次去修正它(稍微移动一下)。</p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/5.png" alt></p>
<p>下面使用线的权重 w 来表示一条线，具体的算法过程为：</p>
<p>从 w<sub>0</sub> 开始，找到在数据 D 中分类错误的点(x<sub>n</sub>, y<sub>n</sub>)，即：</p>
<p>For t = 0,1,2…：</p>
<ol>
<li><p>找到一个 w<sub>t</sub> 分类错误的点 (x<sub>n(t)</sub>, y<sub>n(t)</sub>):  sign(w<sup>T</sup><sub>t</sub>) ≠ y<sub>n(t)</sub></p>
</li>
<li><p>修正这个错误通过：w<sub>t+1</sub> &lt;– w<sub>t</sub> + y<sub>n(t)</sub>X<sub>n(t)</sub></p>
</li>
<li><p>一直修正到所以的数据都分类正确了。</p>
</li>
</ol>
<p>修正的方法有两种：</p>
<ul>
<li>一种是想要的符号是正的，结果是负的：这种情况表明， W 与 X 的角度过大(超过90度了)，因此修正方法为将 W 转到 W 与 X 的中间位置上来。</li>
<li>另一种是想要的符号是负的，结果为正的：这种情况表明，W 与 X 的角度过小(小于90度了)，因此修正方法为将 W 转到远离 X 的位置上去。</li>
</ul>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/6.png" alt></p>
<p>本质上， W 为分类器(直线) 的法向量：</p>
<p>修正的过程为：</p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/7.png" alt><br><img src="/images/2019/picture/Learning-to-answer-yes-no/8.png" alt><br><img src="/images/2019/picture/Learning-to-answer-yes-no/9.png" alt><br><img src="/images/2019/picture/Learning-to-answer-yes-no/10.png" alt><br><img src="/images/2019/picture/Learning-to-answer-yes-no/11.png" alt><br><img src="/images/2019/picture/Learning-to-answer-yes-no/12.png" alt></p>
<p>两个问题：</p>
<ul>
<li>修正的过程一定停下来吗?</li>
<li>停下来的线是 g ≈ f 吗？</li>
</ul>
<h2 id="Guarantee-of-PLA"><a href="#Guarantee-of-PLA" class="headerlink" title="Guarantee of PLA"></a>Guarantee of PLA</h2><p>PLA 停下来的条件：数据线性可分。</p>
<p>思考：假设数据D线性可分(linear separable)，PLA一定会停下来吗？</p>
<p>W<sub>f</sub>：表示真实的线<br>W<sub>t</sub>：表示调整的线</p>
<p>衡量W<sub>t</sub>和W<sub>f</sub>很接近，可以对两条线做内积。内积越大，两条线越接近。</p>
<p>即：linear separable D &lt;=&gt; 存在 W<sub>f</sub> 使得：y<sub>n</sub> = sign(w<sup>T</sup><sub>f</sub>x<sub>n</sub>)</p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/13.png" alt></p>
<p>还要处理向量长度的问题：</p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/14.png" alt></p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/15.png" alt></p>
<h2 id="Non-Separable-data"><a href="#Non-Separable-data" class="headerlink" title="Non-Separable data"></a>Non-Separable data</h2><ul>
<li>线性可分表明：W<sub>t</sub> 和 W<sub>f</sub>会越来越接近。</li>
<li>用错误修正表明：W<sub>t</sub> 的长度会缓慢成长。</li>
</ul>
<p>如果数据D并不是线性可分怎么办？</p>
<p>最好的分割线：在数据 D 上，犯错误最小的线。</p>
<p>贪心算法：新的线与旧的线进行对比。</p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/16.png" alt></p>
<p><img src="/images/2019/picture/Learning-to-answer-yes-no/17.png" alt></p>
<h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/16/The-Learning-Problem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/16/The-Learning-Problem/" itemprop="url">1. The-Learning-Problem</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-16T15:42:22+08:00">
                2019-03-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本系列机器学习来自于台湾大学-林轩田老师的课堂的学习总结。</p>
<p><img src="/images/2019/picture/The-Learning-Problem/1.png" alt></p>
<h1 id="Foundation-Oriented-ML-Course"><a href="#Foundation-Oriented-ML-Course" class="headerlink" title="Foundation Oriented ML Course"></a>Foundation Oriented ML Course</h1><p>从基础开始学习，机器学习基石。</p>
<h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><h2 id="课程介绍"><a href="#课程介绍" class="headerlink" title="课程介绍"></a>课程介绍</h2><h2 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h2><p>学习是获取技巧的过程，学习是从观察开始的。</p>
<p>人类学习： 观察 -&gt; 学习 -&gt; 技巧。 机器学习： data -&gt; ML -&gt; skill。</p>
<p>技巧：某种表现的增进。</p>
<p>机器学习：improving some performance measure with experience computed from data.</p>
<p>一些应用：辨识图中是否有一颗树。树如何定义呢？（用什么样的规则呢？）让机器自己去分析资料，自己去学会如何辨识一颗树。</p>
<p>机器学习使用的场合：</p>
<ul>
<li>when human cannot program the system manually : navigating on Mars</li>
<li>when human cannot ‘define the solution’ easily : speech/visual recognition</li>
<li>when needing rapid decisions that humans cannot do : high-frequency trading</li>
<li>when needing to be user-oriented in a massive scale : consumer-targeted marketing</li>
</ul>
<p>机器学习的三个关键：</p>
<ul>
<li>exists <code>some &#39;underlying pattern&#39; to be learned</code> : so ‘performance measure’ can be improved.</li>
<li>but no programmable (easy) definition : so ‘ML’ is needed.</li>
<li>somehow there is data about the pattern : so ML has some ‘inputs’ to learn from.</li>
</ul>
<blockquote>
<p>个人理解：机器学习的三个关键就是说，机器学习需要从一堆数据中学习一些不容易定义的规则来提高他们的表现。</p>
</blockquote>
<h2 id="机器学习的应用"><a href="#机器学习的应用" class="headerlink" title="机器学习的应用"></a>机器学习的应用</h2><ul>
<li>Food<ul>
<li>data: Twitter data</li>
<li>skill: tell food poisoning likeliness of restaurant properly</li>
</ul>
</li>
<li>Clothing</li>
<li>Housing</li>
<li>Transportation</li>
<li>Recommender System</li>
<li>…</li>
</ul>
<h2 id="机器学习的组成"><a href="#机器学习的组成" class="headerlink" title="机器学习的组成"></a>机器学习的组成</h2><p>A takes D and H to get g.a</p>
<p>Basic Notations:</p>
<ul>
<li>input: x</li>
<li>output: y</li>
<li>unknown pattern to be learned &lt;=&gt; target function: f : x -&gt; y</li>
<li>data &lt;=&gt; training examples: D = {}</li>
<li>hypothesis &lt;=&gt; skill with hopefully good performance: g : x -&gt; y</li>
</ul>
<p>总结为：</p>
<p><img src="/images/2019/picture/The-Learning-Problem/2.png" alt></p>
<p>以是否授权信用卡为例：期待的效果是效能的增进，即 g 跟 f 越像越好，越像 g 就越棒。机器学习最后能够得到一个 g 和 f 很像。</p>
<ul>
<li>f 未知</li>
<li>但是 g 跟 f 越像越好</li>
<li><p>hypothesis set H: 包含 good 或者 bad hypotheses。</p>
<p>其中 g 属于 H(hypothesis set)， learning algorithm A 要做的事情就是从看到的数据 data 中，去集合 H 中，选一个最好的出来。</p>
</li>
</ul>
<p>因此，机器学习可以看做是有两个输入：一个是 data，另一个是允许它选哪些 hypothesis?</p>
<p>以后讲到的模型，就指的是演算法 learning algorithm A 以及 hypothesis set 使用的大 H 部分，这两个集合合起来叫做一个模型。</p>
<p>整个机器学习的流程如下图：机器学习就是从数据 data 出发，机器学习演算法 learning algorithm A 要算出一个 hypothesis g，希望这个 g 要和真正的 f 很接近。</p>
<p><img src="/images/2019/picture/The-Learning-Problem/3.png" alt></p>
<h2 id="机器学习和其他领域"><a href="#机器学习和其他领域" class="headerlink" title="机器学习和其他领域"></a>机器学习和其他领域</h2><ul>
<li>Machine Learning：use data to compute hypothesis g that approximates target f.</li>
<li>Data Mining: use (huge) data to find property that is interesting。Difficult to distinguish ML and DM in reality.</li>
<li>Artificial Intelligence: compute something that shows intelligent behavior. Ml is one possible route to realize AI.</li>
<li>Statistics: use data to make inference about an unknown process. statistics can be used to achieve ML.</li>
</ul>
<h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/12/hadoop-spark-install/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/12/hadoop-spark-install/" itemprop="url">macoc 下 hadoop spark 安装和配置</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-12T22:23:22+08:00">
                2019-03-12
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index">
                    <span itemprop="name">big data</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="安装-hadoop"><a href="#安装-hadoop" class="headerlink" title="安装 hadoop"></a>安装 hadoop</h1><p>brew install hadoop</p>
<h1 id="使用-ssh-登陆"><a href="#使用-ssh-登陆" class="headerlink" title="使用 ssh 登陆"></a>使用 ssh 登陆</h1><p>首先打开远程登陆权限：</p>
<p>系统偏好设置 -&gt; 共享 -&gt; 打开远程登录</p>
<p>cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys<br>ssh localhost</p>
<h1 id="测试Hadoop是否安装成功"><a href="#测试Hadoop是否安装成功" class="headerlink" title="测试Hadoop是否安装成功"></a>测试Hadoop是否安装成功</h1><p>Hadoop有三种安装模式：单机模式，伪分布式模式，分布式模式</p>
<h2 id="测试单机模式"><a href="#测试单机模式" class="headerlink" title="测试单机模式"></a>测试单机模式</h2><p>hadoop 的安装目录在 <code>/usr/local/Cellar</code> 中，进入 <code>hadoop</code> 目录，使用下面命令进行测试：</p>
<pre><code>cd /usr/local/Cellar/hadoop/3.1.1/
mkdir input
cd input
echo &apos;hello world&apos; &gt; file1.txt
echo &apos;hello hadoop&apos; &gt; file2.txt
cd ..
hadoop jar ./libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount ./input ./output

cat output/part-r-00000

//输出为：
hadoop 1
hello 2
world 1
</code></pre><p>注意：不要在运行前建立，output 文件，否则会提示下面信息，导致运行失败：</p>
<pre><code>org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/usr/local/Cellar/hadoop/3.1.1/output already exists
</code></pre><h2 id="测试伪分布式模式"><a href="#测试伪分布式模式" class="headerlink" title="测试伪分布式模式"></a>测试伪分布式模式</h2><h1 id="安装-spark"><a href="#安装-spark" class="headerlink" title="安装 spark"></a>安装 spark</h1><p>brew install apache-spark</p>
<h1 id="修改配置信息"><a href="#修改配置信息" class="headerlink" title="修改配置信息"></a>修改配置信息</h1><p>//安装后，还需要修改Spark的配置文件spark-env.sh<br>cd /usr/local/Cellar/apache-spark/2.4.0<br>cp ./libexec/conf/spark-env.sh.template ./libexec/conf/spark-env.sh</p>
<p>//编辑spark-env.sh文件(vim ./conf/spark-env.sh)，在第一行添加以下配置信息:<br>export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)</p>
<h1 id="验证spark"><a href="#验证spark" class="headerlink" title="验证spark"></a>验证spark</h1><pre><code>cd /usr/local/Cellar/apache-spark/2.4.0
bin/run-example SparkPi
bin/run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is&quot;
</code></pre><h1 id="使用spark-shell编写代码"><a href="#使用spark-shell编写代码" class="headerlink" title="使用spark shell编写代码"></a>使用spark shell编写代码</h1><pre><code>// 启动Spark Shell
spark-shell
// 退出Spark Shell
:quit
</code></pre><p>通过 <code>spark-shell</code> 启动时，可以查看到 Spark 以及 Scala 的版本信息。</p>
<h1 id="独立应用程序编程"><a href="#独立应用程序编程" class="headerlink" title="独立应用程序编程"></a>独立应用程序编程</h1><p>接着我们通过一个简单的应用程序 SimpleApp 来演示如何通过 Spark API 编写一个独立应用程序。使用 Scala 编写的程序需要使用 sbt 进行编译打包，相应的，Java 程序使用 Maven 编译打包，而 Python 程序通过 spark-submit 直接提交。</p>
<h2 id="Scala独立应用编程"><a href="#Scala独立应用编程" class="headerlink" title="Scala独立应用编程"></a>Scala独立应用编程</h2><pre><code>// 安装sbt
brew install sbt
// 验证安装
sbt version
// 显示信息为：
[info] Loading project definition from /Users/chenxiaolong/project
[info] Set current project to chenxiaolong (in build file:/Users/chenxiaolong/)
[info] 0.1.0-SNAPSHOT

mkdir ./ScalaProjects        # 创建应用程序根目录
mkdir -p ./ScalaProjects/src/main/scala     # 创建所需的文件夹结构
cd ./ScalaProjects/test/src/main/scala
vim SimpleApp.scala
</code></pre><p>SimpleApp.scala 内容为：</p>
<pre><code>/* SimpleApp.scala */
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp {
    def main(args: Array[String]) {
        val logFile = &quot;file:///usr/local/Cellar/apache-spark/2.4.0/README.md&quot; // Should be some file on your system
        val conf = new SparkConf().setAppName(&quot;Simple Application&quot;)
        val sc = new SparkContext(conf)
        val logData = sc.textFile(logFile, 2).cache()
        val numAs = logData.filter(line =&gt; line.contains(&quot;a&quot;)).count()
        val numBs = logData.filter(line =&gt; line.contains(&quot;b&quot;)).count()
        println(&quot;Lines with a: %s, Lines with b: %s&quot;.format(numAs, numBs))
    }
}
</code></pre><p>同时在 <code>test</code> 目录下，新建 <code>simple.sbt</code> 文件，添加内容为：</p>
<pre><code>name := &quot;Simple Project&quot;
version := &quot;1.0&quot;
scalaVersion := &quot;2.11.12&quot;
libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.4.0&quot;
</code></pre><p>查看 <code>test</code> 目录下的文件结构为：</p>
<pre><code>➜  test find .
.
./simple.sbt
./src
./src/main
./src/main/scala
./src/main/scala/SimpleApp.scala
</code></pre><p>在 <code>test</code> 目录下，使用 <code>sbt package</code> 命令打包。打包成功后，显示信息为：</p>
<p><img src="/images/2019/picture/Hadoop-spark-install/1.png" alt></p>
<p>通过 <code>spark-submit</code> 运行程序:</p>
<pre><code>spark-submit --class &quot;SimpleApp&quot; ./target/scala-2.11/simple-project_2.11-1.0.jar 2&gt;&amp;1 | grep &quot;Lines with a:&quot;
</code></pre><p>输出为：</p>
<pre><code>Lines with a: 62, Lines with b: 31
</code></pre><h2 id="Java独立应用编程"><a href="#Java独立应用编程" class="headerlink" title="Java独立应用编程"></a>Java独立应用编程</h2><p>参考资料：</p>
<ul>
<li><a href="http://dblab.xmu.edu.cn/blog/820-2/" target="_blank" rel="noopener">macOS 安装Hadoop教程-单机-伪分布式配置</a></li>
<li><a href="http://dblab.xmu.edu.cn/blog/1661-2/" target="_blank" rel="noopener">macOS 安装和配置Spark 学习指南</a></li>
</ul>
<p>其他链接：</p>
<ul>
<li><a href="http://dblab.xmu.edu.cn/blog/1177-2/" target="_blank" rel="noopener">Hadoop 2.7分布式集群环境搭建</a></li>
<li><a href="http://dblab.xmu.edu.cn/blog/1187-2/" target="_blank" rel="noopener">Spark 2.0分布式集群环境搭建</a></li>
<li><a href="http://dblab.xmu.edu.cn/blog/1217-2/" target="_blank" rel="noopener">在集群上运行Spark应用程序</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/10/Join-the-Internet-industry/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/Join-the-Internet-industry/" itemprop="url">加入互联网行业</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-10T11:15:43+08:00">
                2019-03-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Essay/" itemprop="url" rel="index">
                    <span itemprop="name">Essay</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="从哪开始呢"><a href="#从哪开始呢" class="headerlink" title="从哪开始呢"></a>从哪开始呢</h1><p>今天是2019年3月10号，周日，在北京顺义区。昨天游玩了香山，晚上转了南锣鼓巷。今天也是入职互联网行业的第45天，难得有些许时光。就把最近的想法简单记录一下。</p>
<h1 id="何种形式呈现呢"><a href="#何种形式呈现呢" class="headerlink" title="何种形式呈现呢"></a>何种形式呈现呢</h1><p>也不知道从何处说，也不知道如何说，那就用自问自答的形式吧。</p>
<ul>
<li>开始来点轻松的吧，来北京一个半月了，现在对北京感觉如何？</li>
</ul>
<p>北京，是中国的首都，又叫做帝都，像我一样的大多数人，被称为北漂。才来的时候，被自如甲醛房美丽的外表所迷惑，租了半个月，房子一开始开窗通风，只在最后3天入住，每天晚上开着窗户睡，被凛冽的寒风和马路上的车声安排的服服帖帖。后来，在美丽的外表与安全之间选择了安全，联系自如小管家换了房子。</p>
<p>[中午小插曲，顿点从老家带来了老母鸡，喝点鸡汤，整了星球杯大小的6盅牛栏山，喝的晕晕乎乎的~~~。]</p>
<p>北京，确实又到了想象的边缘。便利的交通，转不完的景点，酸爽的小吃，数不尽的小酒馆，让人流连忘返的各种夜景。能给人带来无尽的想象。更不用提高校、医院、建筑、文化、政治这种软实力。</p>
<p>要说感受，目前还是月光族，交了房租，每月开销，剩下的需要还助学贷款。所谓在外面熙熙攘攘的环境下，还能够在小屋里享受片刻的安宁。</p>
<p>附张北京的小玩物：</p>
<p><img src="/images/2019/picture/11.jpg" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="chenxiaolong">
            
              <p class="site-author-name" itemprop="name">chenxiaolong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xiaolongc929" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:xiaolongc929@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://plus.google.com/xiaolongc929" target="_blank" title="Google">
                      
                        <i class="fa fa-fw fa-google"></i>Google</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenxiaolong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
