<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="xiaolongc">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="xiaolongc">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="xiaolongc">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>xiaolongc</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">xiaolongc</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2099/01/01/Contents/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2099/01/01/Contents/" itemprop="url">博客目录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2099-01-01T00:00:00+08:00">
                2099-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/目录/" itemprop="url" rel="index">
                    <span itemprop="name">目录</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="机器学习基石-amp-技巧"><a href="#机器学习基石-amp-技巧" class="headerlink" title="机器学习基石&amp;技巧"></a>机器学习基石&amp;技巧</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/03/16/The-Learning-Problem/" target="_blank" rel="noopener">The-Learning-Problem</a></li>
<li>2 <a href="https://xiaolongc929.github.io/2019/03/17/Learning-to-answer-yes-no/" target="_blank" rel="noopener">Learning-to-answer-yes-no</a></li>
<li>3 <a href="https://xiaolongc929.github.io/2019/03/31/Types-of-learning/" target="_blank" rel="noopener">Types-of-learning</a></li>
<li>4 <a href="https://xiaolongc929.github.io/2019/04/05/Feasibility-of-learning/" target="_blank" rel="noopener">Feasibility-of-learning</a></li>
<li>5 <a href="https://xiaolongc929.github.io/2019/05/26/Training-versus-Testing/" target="_blank" rel="noopener">Training-versus-Testing</a></li>
<li>6 <a href="https://xiaolongc929.github.io/2019/06/23/Theory-of-Generalization/" target="_blank" rel="noopener">Theory-of-Generalization</a></li>
</ul>
<h1 id="机器学习原理"><a href="#机器学习原理" class="headerlink" title="机器学习原理"></a>机器学习原理</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/07/27/Decision-tree/" target="_blank" rel="noopener">Decision-tree</a></li>
<li>2 <a href="https://xiaolongc929.github.io/2019/08/03/AdaBoost/" target="_blank" rel="noopener">AdaBoost</a></li>
<li>3 <a href="https://xiaolongc929.github.io/2019/08/07/GBDT/" target="_blank" rel="noopener">GBDT</a></li>
<li>4 <a href="https://xiaolongc929.github.io/2019/09/01/xgboost/" target="_blank" rel="noopener">xgboost</a></li>
<li>5 <a href="https://xiaolongc929.github.io/2019/09/01/fbprophet/" target="_blank" rel="noopener">fbprophet</a></li>
</ul>
<h1 id="时间序列"><a href="#时间序列" class="headerlink" title="时间序列"></a>时间序列</h1><h1 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h1><ul>
<li><a href="https://xiaolongc929.github.io/2019/03/12/hadoop-spark-install/" target="_blank" rel="noopener">macoc 下 hadoop spark 安装和配置</a></li>
<li><a href="https://xiaolongc929.github.io/2019/07/06/Programming-Hive/" target="_blank" rel="noopener">Hive 编程指南</a></li>
</ul>
<h1 id="运筹规划"><a href="#运筹规划" class="headerlink" title="运筹规划"></a>运筹规划</h1><h1 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a>云计算</h1><h1 id="Java-语言"><a href="#Java-语言" class="headerlink" title="Java 语言"></a>Java 语言</h1><h1 id="Python-语言"><a href="#Python-语言" class="headerlink" title="Python 语言"></a>Python 语言</h1><h1 id="C-语言"><a href="#C-语言" class="headerlink" title="C++ 语言"></a>C++ 语言</h1><h1 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h1><h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/02/17/Hexo-HelloWorld/" target="_blank" rel="noopener">Hexo-HelloWorld</a></li>
<li>2 <a href="https://xiaolongc929.github.io/2019/03/10/hexo-action/" target="_blank" rel="noopener">hexo-实战</a></li>
<li>3 <a href="https://xiaolongc929.github.io/2019/03/12/hadoop-spark-install/" target="_blank" rel="noopener">macoc 下 hadoop spark 安装和配置</a></li>
</ul>
<h1 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/03/08/maven-action/" target="_blank" rel="noopener">maven实战</a></li>
</ul>
<h1 id="阅读感想"><a href="#阅读感想" class="headerlink" title="阅读感想"></a>阅读感想</h1><h1 id="个人随笔"><a href="#个人随笔" class="headerlink" title="个人随笔"></a>个人随笔</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/03/10/Join-the-Internet-industry/" target="_blank" rel="noopener">加入互联网行业</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/28/Big-data/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/28/Big-data/" itemprop="url">1. Big-data</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-28T22:44:54+08:00">
                2019-10-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index">
                    <span itemprop="name">big data</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="大数据技术原理与应用（第2版）"><a href="#大数据技术原理与应用（第2版）" class="headerlink" title="大数据技术原理与应用（第2版）"></a>大数据技术原理与应用（第2版）</h1><h1 id="第2章-大数据处理架构-hadoop"><a href="#第2章-大数据处理架构-hadoop" class="headerlink" title="第2章 大数据处理架构 hadoop"></a>第2章 大数据处理架构 hadoop</h1><h2 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h2><p>Hadoop的核心是分布式文件系统（Hadoop Distributed File System，HDFS）和MapReduce。HDFS是针对谷歌文件系统（Google File System，GFS）的开源实现，是面向普通硬件环境的分布式文件系统，具有较高的读写速度、很好的容错性和可伸缩性，支持大规模数据的分布式存储，其冗余数据存储的方式很好地保证了数据的安全性。MapReduce是针对谷歌 MapReduce 的开源实现，允许用户在不了解分布式系统底层细节的情况下开发并行应用程序，采用MapReduce来整合分布式文件系统上的数据，可保证分析和处理数据的高效性。借助于Hadoop，程序员可以轻松地编写分布式并行程序，将其运行于廉价计算机集群上，完成海量数据的存储与计算。</p>
<p>它具有以下几个方面的特性。</p>
<ul>
<li>高可靠性</li>
<li>高效性。</li>
<li>高可扩展性</li>
<li>高容错性</li>
<li>成本低。</li>
<li>运行在Linux平台上。</li>
<li>支持多种编程语言。</li>
</ul>
<h2 id="2-2-Hadoop生态系统"><a href="#2-2-Hadoop生态系统" class="headerlink" title="2.2 Hadoop生态系统"></a>2.2 Hadoop生态系统</h2><p>除了核心的HDFS和MapReduce以外，Hadoop生态系统还包括Zookeeper、HBase、Hive、Pig、Mahout、Sqoop、Flume、Ambari等功能组件。需要说明的是，Hadoop 2.0中新增了一些重要的组件，即HDFS HA和分布式资源调度管理框架YARN等，</p>
<p><img src="/images/2019/picture/Big-data/1.jpeg" alt="Hadoop架构"></p>
<p>Hadoop分布式文件系统（Hadoop Distributed File System，HDFS）是Hadoop项目的两大核心之一，是针对谷歌文件系统（Google File System，GFS）的开源实现。</p>
<p>HBase是一个提供高可靠性、高性能、可伸缩、实时读写、分布式的列式数据库，一般采用HDFS作为其底层数据存储。HBase是针对谷歌BigTable的开源实现，二者都采用了相同的数据模型，具有强大的非结构化数据存储能力。HBase与传统关系数据库的一个重要区别是，前者采用基于列的存储，而后者采用基于行的存储。HBase具有良好的横向扩展能力，可以通过不断增加廉价的商用服务器来增加存储能力。</p>
<p>Hadoop MapReduce是针对谷歌MapReduce的开源实现。MapReduce是一种编程模型，用于大规模数据集（大于1 TB）的并行运算，它将复杂的、运行于大规模集群上的并行计算过程高度地抽象到了两个函数——Map 和 Reduce 上，并且允许用户在不了解分布式系统底层细节的情况下开发并行应用程序，并将其运行于廉价计算机集群上，完成海量数据的处理。</p>
<p>Hive是一个基于Hadoop的数据仓库工具，可以用于对Hadoop文件中的数据集进行数据整理、特殊查询和分析存储。</p>
<p>Pig是一种数据流语言和运行环境，适合于使用Hadoop和MapReduce平台来查询大型半结构化数据集。</p>
<p>Mahout是Apache软件基金会旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。</p>
<p>Zookeeper是针对谷歌Chubby的一个开源实现，是高效和可靠的协同工作系统，提供分布式锁之类的基本服务（如统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等），用于构建分布式应用，减轻分布式应用程序所承担的协调任务。</p>
<p>Flume是Cloudera提供的一个高可用的、高可靠的、分布式的海量日志采集、聚合和传输的系统</p>
<p>Sqoop是SQL-to-Hadoop的缩写，主要用来在Hadoop和关系数据库之间交换数据，可以改进数据的互操作性。通过Sqoop可以方便地将数据从MySQL、Oracle、PostgreSQL等关系数据库中导入Hadoop（可以导入HDFS、HBase或Hive），或者将数据从Hadoop导出到关系数据库，使得传统关系数据库和Hadoop之间的数据迁移变得非常方便。</p>
<p>Apache Ambari是一种基于Web的工具，支持Apache Hadoop集群的安装、部署、配置和管理。</p>
<p><img src="/images/2019/picture/Big-data/2.jpeg" alt="存储与管理"></p>
<h1 id="第3章-分布式文件系统-HDFS"><a href="#第3章-分布式文件系统-HDFS" class="headerlink" title="第3章 分布式文件系统 HDFS"></a>第3章 分布式文件系统 HDFS</h1><h2 id="3-1-分布式文件系统"><a href="#3-1-分布式文件系统" class="headerlink" title="3.1 分布式文件系统"></a>3.1 分布式文件系统</h2><p>相对于传统的本地文件系统而言，分布式文件系统（Distributed File System）是一种通过网络实现文件在多台主机上进行分布式存储的文件系统。分布式文件系统的设计一般采用“客户机/服务器”（Client/Server）模式，客户端以特定的通信协议通过网络与服务器建立连接，提出文件访问请求，客户端和服务器可以通过设置访问权来限制请求方对底层数据存储块的访问。目前，已得到广泛应用的分布式文件系统主要包括GFS和HDFS等，后者是针对前者的开源实现。</p>
<p>与普通文件系统类似，分布式文件系统也采用了块的概念，文件被分成若干个块进行存储，块是数据读写的基本单元，只不过分布式文件系统的块要比操作系统中的块大很多。比如，HDFS默认的一个块的大小是64 MB。与普通文件不同的是，在分布式文件系统中，如果一个文件小于一个数据块的大小，它并不占用整个数据块的存储空间。</p>
<p>这些节点分为两类：一类叫“主节点”（Master Node），或者也被称为“名称节点”（NameNode）；另一类叫“从节点”（Slave Node），或者也被称为“数据节点”（DataNode）。名称节点负责文件和目录的创建、删除和重命名等，同时管理着数据节点和文件块的映射关系，因此客户端只有访问名称节点才能找到请求的文件块所在的位置，进而到相应位置读取所需文件块。数据节点负责数据的存储和读取，在存储时，由名称节点分配存储位置，然后由客户端把数据直接写入相应数据节点；在读取时，客户端从名称节点获得数据节点和文件块的映射关系，然后就可以到相应位置访问文件块。数据节点也要根据名称节点的命令创建、删除数据块和冗余复制。</p>
<h2 id="3-2-HDFS简介"><a href="#3-2-HDFS简介" class="headerlink" title="3.2 HDFS简介"></a>3.2 HDFS简介</h2><p>总体而言，HDFS要实现以下目标：</p>
<ul>
<li>兼容廉价的硬件设备。</li>
<li>流数据读写。</li>
<li>大数据集。</li>
<li>简单的文件模型。</li>
<li>强大的跨平台兼容性</li>
<li>不适合低延迟数据访问</li>
<li>无法高效存储大量小文件</li>
</ul>
<p>首先，HDFS 采用名称节点（NameNode）来管理文件系统的元数据，这些元数据被保存在内存中，从而使客户端可以快速获取文件实际存储位置。通常，每个文件、目录和块大约占150字节，如果有1 000万个文件，每个文件对应一个块，那么，名称节点至少要消耗3 GB的内存来保存这些元数据信息</p>
<p>不支持多用户写入及任意修改文件。HDFS只允许一个文件有一个写入者，不允许多个用户对同一个文件执行写操作，而且只允许对文件执行追加操作，不能执行随机写操作。</p>
<p><img src="/images/2019/picture/Big-data/3.jpeg" alt="HDFS实现方式"></p>
<h2 id="3-3-HDFS的相关概念"><a href="#3-3-HDFS的相关概念" class="headerlink" title="3.3 HDFS的相关概念"></a>3.3 HDFS的相关概念</h2><p>本节介绍HDFS中的相关概念，包括块、名称节点、数据节点、第二名称节点。</p>
<p>HDFS也同样采用了块的概念，默认的一个块大小是64 MB。在HDFS中的文件会被拆分成多个块，每个块作为独立的单元进行存储。</p>
<p>HDFS这么做的原因，是为了最小化寻址开销。</p>
<p>当客户端需要访问一个文件时，首先从名称节点获得组成这个文件的数据块的位置列表，然后根据位置列表获取实际存储各个数据块的数据节点的位置，最后数据节点根据数据块信息在本地Linux 文件系统中找到对应的文件，并把数据返回给客户端。</p>
<p>块的大小也不宜设置过大，因为，通常MapReduce中的Map任务一次只处理一个块中的数据，如果启动的任务太少，就会降低作业并行处理速度。</p>
<p>在 HDFS 中，名称节点（NameNode）负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构（见图3-3），即FsImage和EditLog。FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据，操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作。</p>
<p><img src="/images/2019/picture/Big-data/4.jpeg" alt="节点"></p>
<p>名称节点记录了每个文件中各个块所在的数据节点的位置信息，但是并不持久化存储这些信息，而是在系统每次启动时扫描所有数据节点重构得到这些信息</p>
<p>名称节点在启动时，会将 FsImage 的内容加载到内存当中，然后执行 EditLog 文件中的各项操作，使得内存中的元数据保持最新。这个操作完成以后，就会创建一个新的 FsImage 文件和一个空的EditLog文件。名称节点启动成功并进入正常运行状态以后，HDFS中的更新操作都会被写入到 EditLog，而不是直接写入 FsImage，这是因为对于分布式文件系统而言，FsImage文件通常都很庞大（一般都是GB级别以上），如果所有的更新操作都直接往FsImage文件中添加，那么系统就会变得非常缓慢。相对而言，EditLog通常都要远远小于FsImage，更新操作写入到EditLog是非常高效的。名称节点在启动的过程中处于“安全模式”，只能对外提供读操作，无法提供写操作。启动过程结束后，系统就会退出安全模式，进入正常运行状态，对外提供读写操作。</p>
<p>数据节点（DataNode）是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表。每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中。</p>
<p>当名称节点重启时，需要将FsImage加载到内存中，然后逐条执行EditLog中的记录，使得FsImage保持最新。可想而知，如果EditLog很大，就会导致整个过程变得非常缓慢，使得名称节点在启动过程中长期处于“安全模式”，无法正常对外提供写操作，影响了用户的使用。</p>
<p>为了有效解决EditLog逐渐变大带来的问题，HDFS在设计中采用了第二名称节点（Secondary NameNode）。第二名称节点是HDFS架构的一个重要组成部分，具有两个方面的功能：首先，可以完成EditLog与FsImage的合并操作，减小EditLog文件大小，缩短名称节点重启时间；其次，可以作为名称节点的“检查点”，保存名称节点中的元数据信息。具体如下。</p>
<ul>
<li>（1）EditLog与FsImage的合并操作。</li>
<li>（2）作为名称节点的“检查点”。</li>
</ul>
<p><img src="/images/2019/picture/Big-data/5.jpeg" alt="节点"></p>
<h2 id="3-4-HDFS体系结构"><a href="#3-4-HDFS体系结构" class="headerlink" title="3.4 HDFS体系结构"></a>3.4 HDFS体系结构</h2><p>本节首先简要介绍HDFS的体系结构，然后介绍HDFS的命名空间管理、通信协议、客户端，最后指出HDFS体系结构的局限性。</p>
<p>HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群包括一个名称节点和若干个数据节点（见图3-5）。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的。每个数据节点会周期性地向名称节点发送“心跳”信息，报告自己的状态，没有按时发送心跳信息的数据节点会被标记为“宕机”，不会再给它分配任何I/O请求。</p>
<p><img src="/images/2019/picture/Big-data/6.jpeg" alt="命名空间管理"></p>
<p>HDFS的命名空间包含目录、文件和块。命名空间管理是指命名空间支持对HDFS中的目录、文件和块做类似文件系统的创建、修改、删除等基本操作。在当前的HDFS体系结构中，在整个HDFS 集群中只有一个命名空间，并且只有唯一一个名称节点，该节点负责对这个命名空间进行管理。</p>
<p><img src="/images/2019/picture/Big-data/7.jpeg" alt="读取数据"><br><img src="/images/2019/picture/Big-data/8.jpeg" alt="写入数据"></p>
<p>HDFS是一个部署在集群上的分布式文件系统，因此很多数据需要通过网络进行传输。所有的HDFS通信协议都是构建在TCP/IP协议基础之上的。客户端通过一个可配置的端口向名称节点主动发起TCP连接，并使用客户端协议与名称节点进行交互。名称节点和数据节点之间则使用数据节点协议进行交互。客户端与数据节点的交互是通过 RPC（Remote Procedure Call）来实现的。在设计上，名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求。</p>
<p>明显的局限性</p>
<ul>
<li>（1）命名空间的限制。名称节点是保存在内存中的，因此名称节点能够容纳对象（文件、块）的个数会受到内存空间大小的限制。</li>
<li>（2）性能的瓶颈</li>
<li>（3）隔离问题。</li>
<li>（4）集群的可用性</li>
</ul>
<h2 id="3-5-HDFS的存储原理"><a href="#3-5-HDFS的存储原理" class="headerlink" title="3.5 HDFS的存储原理"></a>3.5 HDFS的存储原理</h2><p>本节介绍HDFS的存储原理，包括数据的冗余存储、数据存取策略、数据错误与恢复。</p>
<ul>
<li>1.数据存放为了提高数据的可靠性与系统的可用性，以及充分利用网络带宽，HDFS采用了以机架（Rack）为基础的数据存放策略。HDFS 默认的冗余复制因子是 3，每一个文件块会被同时保存到 3 个地方，其中，有两份副本放在同一个机架的不同机器上面，第三个副本放在不同机架的机器上面，这样既可以保证机架发生异常时的数据恢复，也可以提高数据读写性能。 </li>
<li>2.数据读取HDFS提供了一个API可以确定一个数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID</li>
<li>3.数据复制HDFS的数据复制采用了流水线复制的策略，大大提高了数据复制过程的效率。</li>
</ul>
<p>HDFS具有较高的容错性，可以兼容廉价的硬件，它把硬件出错看成一种常态，而不是异常，并设计了相应的机制检测数据错误和进行自动恢复，主要包括以下3种情形。</p>
<p>(1).名称节点出错</p>
<p>Hadoop采用两种机制来确保名称节点的安全：第一，把名称节点上的元数据信息同步存储到其他文件系统（比如远程挂载的网络文件系统NFS）中；第二，运行一个第二名称节点，当名称节点宕机以后，可以把第二名称节点作为一种弥补措施，利用第二名称节点中的元数据信息进行系统恢复，但是从前面对第二名称节点的介绍中可以看出，这样做仍然会丢失部分数据。</p>
<p>(2).数据节点出错</p>
<p>每个数据节点会定期向名称节点发送“心跳”信息，向名称节点报告自己的状态。当数据节点发生故障，或者网络发生断网时，名称节点就无法收到来自一些数据节点的“心跳”信息，这时这些数据节点就会被标记为“宕机”，节点上面的所有数据都会被标记为“不可读”，名称节点不会再给它们发送任何I/O请求</p>
<p>(3).数据出错</p>
<p>客户端在读取到数据后，会采用md5和sha1对数据块进行校验，以确定读取到正确的数据。</p>
<h2 id="3-7-HDFS编程实践"><a href="#3-7-HDFS编程实践" class="headerlink" title="3.7 HDFS编程实践"></a>3.7 HDFS编程实践</h2><p>关于HDFS的Shell命令有一个统一的格式。hadoop command [genericOptions][commandOptions]<br>HDFS有很多命令，其中fs命令可以说是HDFS最常用的命令，利用fs命令可以查看HDFS文件系统的目录结构、上传和下载数据、创建文件等。该命令的用法如下。hadoop fs [genericOptions][commandOptions]</p>
<h1 id="第三篇-大数据处理与分析"><a href="#第三篇-大数据处理与分析" class="headerlink" title="第三篇 大数据处理与分析"></a>第三篇 大数据处理与分析</h1><p>分布式并行编程框架MapReduce可以大幅提高程序性能，实现高效的批量数据处理。基于内存的分布式计算框架 Spark，是一个可应用于大规模数据处理的快速、通用引擎</p>
<p>流计算框架Storm是一个低延迟、可扩展、高可靠的处理引擎</p>
<p>大数据中包括很多图结构数据，但是MapReduce不适合用来解决大规模图计算问题，因此新的图计算框架应运而生，Pregel 就是其中一种具有代表性的产品</p>
<p><img src="/images/2019/picture/Big-data/11.jpeg" alt="写入数据"></p>
<h2 id="第7章-MapReduce"><a href="#第7章-MapReduce" class="headerlink" title="第7章 MapReduce"></a>第7章 MapReduce</h2><p>大数据时代除了需要解决大规模数据的高效存储问题，还需要解决大规模数据的高效处理问题。</p>
<p>MapReduce是一种并行编程模型，用于大规模数据集（大于1 TB）的并行运算，它将复杂的、运行于大规模集群上的并行计算过程高度抽象到两个函数：Map 和 Reduce。</p>
<h2 id="7-1-概述"><a href="#7-1-概述" class="headerlink" title="7.1 概述"></a>7.1 概述</h2><p>MapReduce以及它的核心函数Map和Reduce。</p>
<p>谷歌公司最先提出了分布式并行编程模型MapReduce，Hadoop MapReduce是它的开源实现。谷歌的MapReduce运行在分布式文件系统GFS上，与谷歌类似，Hadoop MapReduce运行在分布式文件系统HDFS上。</p>
<p>谷歌在2003年～2006年连续发表了3篇很有影响力的文章，分别阐述了GFS、MapReduce和BigTable的核心思想。其中，MapReduce是谷歌公司的核心计算模型。MapReduce将复杂的、运行于大规模集群上的并行计算过程高度地抽象到两个函数：Map和Reduce，这两个函数及其核心思想都源自函数式编程语言。</p>
<p>在MapReduce中，一个存储在分布式文件系统中的大规模数据集会被切分成许多独立的小数据块，这些小数据块可以被多个Map任务并行处理。MapReduce框架会为每个Map任务输入一个数据子集，Map任务生成的结果会继续作为Reduce任务的输入，最终由Reduce任务输出最后结果，并写入分布式文件系统。特别需要注意的是，适合用MapReduce来处理的数据集需要满足一个前提条件：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理。</p>
<p>MapReduce编程之所以比较容易，是因为程序员只要关注如何实现Map和Reduce函数，而不需要处理并行编程中的其他各种复杂问题，如分布式存储、工作调度、负载均衡、容错处理、网络通信等，这些问题都会由MapReduce框架负责处理。</p>
<p>Map函数和Reduce函数都是以作为输入，按一定的映射规则转换成另一个或一批进行输出（见表7-1）</p>
<p><img src="/images/2019/picture/Big-data/12.jpeg" alt="写入数据"></p>
<h2 id="7-2-MapReduce的工作流程"><a href="#7-2-MapReduce的工作流程" class="headerlink" title="7.2 MapReduce的工作流程"></a>7.2 MapReduce的工作流程</h2><p>理解MapReduce的工作流程，是开展MapReduce编程的前提。本节首先给出工作流程概述，并阐述MapReduce的各个执行阶段，最后对MapReduce的核心环节——Shuffle过程进行详细剖析。</p>
<p>大规模数据集的处理包括分布式存储和分布式计算两个核心环节</p>
<p>MapReduce的核心思想可以用“分而治之”来描述，如图7-1所示，也就是把一个大的数据集拆分成多个小数据块在多台机器上并行处理，也就是说，一个大的MapReduce作业，首先会被拆分成许多个Map任务在多台机器上并行执行，每个Map任务通常运行在数据存储的节点上，这样，计算和数据就可以放在一起运行，不需要额外的数据传输开销。当Map任务结束后，会生成以形式表示的许多中间结果。然后，这些中间结果会被分发到多个Reduce任务在多台机器上并行执行，具有相同key的会被发送到同一个Reduce任务那里，Reduce任务会对中间结果进行汇总计算得到最后结果，并输出到分布式文件系统中。</p>
<p><img src="/images/2019/picture/Big-data/13.jpeg" alt="MapReduce的工作流程"> </p>
<p>需要指出的是，不同的Map任务之间不会进行通信，不同的Reduce任务之间也不会发生任何信息交换；用户不能显式地从一台机器向另一台机器发送消息，所有的数据交换都是通过MapReduce框架自身去实现的。</p>
<p>为了让Reduce可以并行处理Map的结果，需要对Map的输出进行一定的分区（Portition）、排序（Sort）、合并（Combine）、归并（Merge）等操作，得到形式的中间结果，再交给对应的 Reduce 进行处理，这个过程称为 Shuffle。从无序的到有序的，这个过程用Shuffle（洗牌）来称呼是非常形象的。</p>
<p><img src="/images/2019/picture/Big-data/14.jpeg" alt="Shuffle过程"> </p>
<p>所谓Shuffle，是指对Map输出结果进行分区、排序、合并等处理并交给Reduce的过程。因此，Shuffle过程分为Map端的操作和Reduce端的操作，如图7-3所示，主要执行以下操作</p>
<p><img src="/images/2019/picture/Big-data/15.jpeg" alt="Shuffle过程">  </p>
<p>在Map端的Shuffle过程Map的输出结果首先被写入缓存，当缓存满时，就启动溢写操作，把缓存中的数据写入磁盘文件，并清空缓存。当启动溢写操作时，首先需要把缓存中的数据进行分区，然后对每个分区的数据进行排序（Sort）和合并（Combine），之后再写入磁盘文件。每次溢写操作会生成一个新的磁盘文件，随着Map任务的执行，磁盘中就会生成多个溢写文件。在Map任务全部结束之前，这些溢写文件会被归并（Merge）成一个大的磁盘文件，然后通知相应的Reduce任务来领取属于自己处理的数据。（2）在Reduce端的Shuffle过程Reduce任务从Map端的不同Map机器领回属于自己处理的那部分数据，然后对数据进行归并（Merge）后交给Reduce处理。</p>
<p><strong> 2.Map 端的Shuffle过程 </strong> </p>
<ul>
<li>（1）输入数据和执行Map任务</li>
<li>（2）写入缓存</li>
<li>（3）溢写（分区、排序和合并）。并非所有场合都可以使用Combiner，因为Combiner的输出是Reduce任务的输入，Combiner绝不能改变Reduce任务最终的计算结果，一般而言，累加、最大值等场景可以使用合并操作。经过分区、排序以及可能发生的合并操作之后，这些缓存中的键值对就可以被写入磁盘，并清空缓存。每次溢写操作都会在磁盘中生成一个新的溢写文件，写入溢写文件中的所有键值对都是经过分区和排序的。</li>
<li>（4）文件归并每次溢写操作都会在磁盘中生成一个新的溢写文件，随着MapReduce任务的进行，磁盘中的溢写文件数量会越来越多。当然，如果Map输出结果很少，磁盘上只会存在一个溢写文件，但是通常都会存在多个溢写文件。最终，在Map任务全部结束之前，系统会对所有溢写文件中的数据进行归并（Merge），生成一个大的溢写文件，这个大的溢写文件中的所有键值对也是经过分区和排序的。</li>
</ul>
<p>所谓“归并”，是指对于具有相同key的键值对会被归并成一个新的键值对。具体而言，对于若干个具有相同 key 的键值对， …… 会被归并成一个新的键值对&gt;。</p>
<p><img src="/images/2019/picture/Big-data/16.jpeg" alt="Shuffle过程">  </p>
<p>经过上述4个步骤以后，Map端的Shuffle过程全部完成，最终生成的一个大文件会被存放在本地磁盘上。这个大文件中的数据是被分区的，不同的分区会被发送到不同的Reduce任务进行并行处理。JobTracker会一直监测Map任务的执行，当监测到一个Map任务完成后，就会立即通知相关的Reduce任务来“领取”数据，然后开始Reduce端的Shuffle过程。</p>
<p><strong> 3.Reduce端的Shuffle过程 </strong></p>
<p>Reduce端的Shuffle过程非常简单，只需要从Map端读取Map结果，然后执行归并操作，最后输送给Reduce任务进行处理。具体而言，Reduce端的Shuffle过程包括3个步骤，如图7-5所示。</p>
<ul>
<li>（1）“领取”数据</li>
</ul>
<p>Map端的Shuffle过程结束后，所有Map输出结果都保存在Map机器的本地磁盘上，Reduce任务需要把这些数据“领取”（Fetch）回来存放到自己所在机器的本地磁盘上。因此，在每个Reduce任务真正开始之前，它大部分时间都在从Map端把属于自己处理的那些分区的数据“领取”过来。每个Reduce任务会不断地通过RPC向JobTracker询问Map任务是否已经完成；JobTracker监测到一个Map任务完成后，就会通知相关的Reduce任务来“领取”数据；一旦一个Reduce任务收到JobTracker的通知，它就会到该Map任务所在机器上把属于自己处理的分区数据领取到本地磁盘中。一般系统中会存在多个Map机器，因此Reduce任务会使用多个线程同时从多个Map机器领回数据。</p>
<ul>
<li>（2）归并数据</li>
</ul>
<p>从Map端领回的数据会首先被存放在Reduce任务所在机器的缓存中，如果缓存被占满，就会像Map端一样被溢写到磁盘中。</p>
<p>当溢写过程启动时，具有相同key的键值对会被归并（Merge），如果用户定义了Combiner，则归并后的数据还可以执行合并操作，减少写入磁盘的数据量</p>
<p>最终，当所有的 Map 端数据都已经被领回时，和 Map端类似，多个溢写文件会被归并成一个大文件，归并的时候还会对键值对进行排序，从而使得最终大文件中的键值对都是有序的</p>
<p>假设磁盘中生成了50个溢写文件，每轮可以归并10个溢写文件，则需要经过5轮归并，得到5个归并后的大文件。</p>
<ul>
<li>（3）把数据输入给Reduce任务</li>
</ul>
<p><img src="/images/2019/picture/Big-data/17.jpeg" alt="Shuffle过程"> </p>
<p>磁盘中经过多轮归并后得到的若干个大文件，不会继续归并成一个新的大文件，而是直接输入给Reduce任务，这样可以减少磁盘读写开销。由此，整个Shuffle过程顺利结束。接下来，Reduce任务会执行 Reduce 函数中定义的各种映射，输出最终结果，并保存到分布式文件系统中（比如GFS或HDFS）。</p>
<h2 id="7-3-实例分析：WordCount"><a href="#7-3-实例分析：WordCount" class="headerlink" title="7.3 实例分析：WordCount"></a>7.3 实例分析：WordCount</h2><p>首先，需要检查WordCount程序任务是否可以采用MapReduce来实现。</p>
<p>MapReduce来处理的数据集需要满足一个前提条件：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理。</p>
<h2 id="7-6-本章小结"><a href="#7-6-本章小结" class="headerlink" title="7.6 本章小结"></a>7.6 本章小结</h2><p>MapReduce 执行的全过程包括以下几个主要阶段：从分布式文件系统读入数据、执行 Map任务输出中间结果、通过Shuffle阶段把中间结果分区排序整理后发送给Reduce任务、执行Reduce任务得到最终结果并写入分布式文件系统。在这几个阶段中，Shuffle阶段非常关键，必须深刻理解这个阶段的详细执行过程。</p>
<p><img src="/images/2019/picture/Big-data/18.jpeg" alt="Shuffle过程"> </p>
<h1 id="第8章-hadoop-再讨论"><a href="#第8章-hadoop-再讨论" class="headerlink" title="第8章 hadoop 再讨论"></a>第8章 hadoop 再讨论</h1><h2 id="8-1-Hadoop的优化与发展"><a href="#8-1-Hadoop的优化与发展" class="headerlink" title="8.1 Hadoop的优化与发展"></a>8.1 Hadoop的优化与发展</h2><p>Hadoop1.0的核心组件（仅指MapReduce和HDFS，不包括Hadoop生态系统内的Pig、Hive、HBase等其他组件）主要存在以下不足。</p>
<p>在后续发展过程中，Hadoop对MapReduce和HDFS的许多方面做了有针对性的改进提升（见表8-1），同时在Hadoop生态系统中也融入了更多的新成员，使得Hadoop功能更加完善，比较有代表性的产品包括Pig、Oozie、Tez、Kafka等（见表8-2）。</p>
<p><img src="/images/2019/picture/Big-data/19.jpeg" alt="HDFS进化"><br><img src="/images/2019/picture/Big-data/20.jpeg" alt="HDFS进化"> </p>
<h2 id="8-2-HDFS2-0的新特性"><a href="#8-2-HDFS2-0的新特性" class="headerlink" title="8.2 HDFS2.0的新特性"></a>8.2 HDFS2.0的新特性</h2><p>在HDFS1.0中，只存在一个名称节点，这就是常说的“单点故障问题”</p>
<p>为了解决单点故障问题，HDFS2.0采用了HA（High Availability）架构。</p>
<p>在一个典型的HA集群中，一般设置两个名称节点，其中一个名称节点处于“活跃（Active）”状态，另一个处于“待命（Standby）”状态，如图8-1所示。</p>
<p><img src="/images/2019/picture/Big-data/21.jpeg" alt="名称节点"> </p>
<p>由于待命名称节点是活跃名称节点的“热备份”，因此活跃名称节点的状态信息必须实时同步到待命名称节点。</p>
<p>两种名称节点的状态同步，可以借助于一个共享存储系统来实现，比如 NFS （Network File System）、QJM（Quorum Journal Manager）或者Zookeeper。</p>
<p>Zookeeper可以确保任意时刻只有一个名称节点提供对外服务</p>
<p>HDFS1.0采用单名称节点的设计，不仅会带来单点故障问题，还存在可扩展性、性能和隔离性等问题</p>
<p>HDFS1.0中只有一个名称节点，不可以水平扩展，而单个名称节点的内存空间是有上限的，这限制了系统中数据块、文件和目录的数目。是否可以通过纵向扩展的方式（即为单个名称节点增加更多的CPU、内存等资源）解决这个问题呢？答案是否定的。</p>
<p>HDFS HA在本质上还是单名称节点，只是通过“热备份”设计方式解决了单点故障问题，并没有解决可扩展性、系统性能和隔离性三个方面的问题。</p>
<p>HDFS 联邦可以很好地解决上述三个方面的问题。</p>
<p>HDFS 联邦并不是真正的分布式设计，但是采用这种简单的“联合”设计方式，在实现和管理复杂性方面，都要远低于真正的分布式设计，而且可以快速满足需求。</p>
<p>在 HDFS 联邦中，所有名称节点会共享底层的数据节点存储资源，如图 8-2 所示。每个数据节点要向集群中所有的名称节点注册，并周期性地向名称节点发送“心跳”和块信息，报告自己的状态，同时也会处理来自名称节点的指令。</p>
<p>HDFS1.0不同的是，HDFS 联邦拥有多个独立的命名空间，其中，每一个命名空间管理属于自己的一组块，这些属于同一个命名空间的块构成一个“块池”（Block Pool）。每个数据节点会为多个块池提供块的存储。可以看出，数据节点是一个物理概念，而块池则属于逻辑概念，一个块池是一组块的逻辑集合，块池中的各个块实际上是存储在各个不同的数据节点中的。因此，HDFS 联邦中的一个名称节点失效，也不会影响到与它相关的数据节点继续为其他名称节点提供服务。</p>
<p>对于HDFS联邦中的多个命名空间，可以采用客户端挂载表（Client Side Mount Table）方式进行数据共享和访问。</p>
<p>客户可以访问不同的挂载点来访问不同的子命名空间。这就是 HDFS 联邦中命名空间管理的基本原理，即把各个命名空间挂载到全局“挂载表”（Mount-table）中，实现数据全局共享；</p>
<h2 id="8-3-新一代资源管理调度框架YARN"><a href="#8-3-新一代资源管理调度框架YARN" class="headerlink" title="8.3 新一代资源管理调度框架YARN"></a>8.3 新一代资源管理调度框架YARN</h2><p>MapReduce1.0 采用 Master/Slave 架构设计（见图 8-4），包括一个 JobTracker 和若干个TaskTracker，前者负责作业的调度和资源的管理，后者负责执行JobTracker指派的具体任务</p>
<p><img src="/images/2019/picture/Big-data/22.jpeg" alt="架构设计"> </p>
<ul>
<li>（1）存在单点故障。</li>
<li>（2）JobTracker“大包大揽”导致任务过重。</li>
<li>（3）容易出现内存溢出。</li>
<li>（4）资源划分不合理。</li>
</ul>
<p>资源（CPU、内存）被强制等量划分成多个“槽”（Slot），槽又被进一步划分为Map槽和Reduce槽两种，分别供Map任务和Reduce任务使用，彼此之间不能使用分配给对方的槽，也就是说，当Map任务已经用完Map槽时，即使系统中还有大量剩余的Reduce槽，也不能拿来运行Map任务，反之亦然</p>
<p>为了克服 MapReduce1.0 版本的缺陷，Hadoop2.0 以后的版本对其核心子项目 MapReduce1.0的体系结构进行了重新设计，生成了MapReduce2.0和YARN（Yet Another Resource Negotiator）。</p>
<p>YARN架构设计思路如图8-5所示，基本思路就是“放权”，即不让JobTracker这一个组件承担过多的功能，把原JobTracker三大功能（资源管理、任务调度和任务监控）进行拆分，分别交给不同的新组件去处理。重新设计后得到的 YARN 包括 ResourceManager、ApplicationMaster 和NodeManager，其中，由ResourceManager负责资源管理，由ApplicationMaster负责任务调度和监控，由 NodeManager 负责执行原 TaskTracker 的任务。通过这种“放权”的设计，大大降低了JobTracker的负担，提升了系统运行的效率和稳定性。</p>
<p><img src="/images/2019/picture/Big-data/23.jpeg" alt="架构设计"> </p>
<p>在Hadoop1.0中，其核心子项目MapReduce1.0既是一个计算框架，也是一个资源管理调度框架。到了Hadoop2.0以后，MapReduce1.0中的资源管理调度功能被单独分离出来形成了YARN，它是一个纯粹的资源管理调度框架，而不是一个计算框架；而被剥离了资源管理调度功能的MapReduce 框架就变成了MapReduce2.0，它是运行在YARN之上的一个纯粹的计算框架，不再自己负责资源调度管理服务，而是由YARN为其提供资源管理调度服务。</p>
<p>如图8-6所示，YARN体系结构中包含了三个组件：ResourceManager、ApplicationMaster和NodeManager。YARN各个组件的功能见表8-3。</p>
<p><img src="/images/2019/picture/Big-data/24.jpeg" alt="架构设计"> </p>
<p>ResourceManager（RM）是一个全局的资源管理器，负责整个系统的资源管理和分配，主要包括两个组件，即调度器（Scheduler）和应用程序管理器（Applications Manager）</p>
<p>调度器主要负责资源管理和分配，不再负责跟踪和监控应用程序的执行状态，也不负责执行失败恢复，因为这些任务都已经交给ApplicationMaster组件来负责。</p>
<p>调度器接收来自ApplicationMaster的应用程序资源请求，并根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），把集群中的资源以“容器”的形式分配给提出申请的应用程序，容器的选择通常会考虑应用程序所要处理的数据的位置，进行就近选择，从而实现“计算向数据靠拢”。</p>
<p>而在 YARN 中是以容器（Container）作为动态资源分配单位，每个容器中都封装了一定数量的CPU、内存、磁盘等资源，从而限定每个应用程序可以使用的资源量</p>
<p><img src="/images/2019/picture/Big-data/25.jpeg" alt="架构设计"> </p>
<p>在Hadoop平台上，用户的应用程序是以作业（Job）的形式提交的，然后一个作业会被分解成多个任务（包括Map任务和Reduce任务）进行分布式执行。</p>
<p>ApplicationMaster。ApplicationMaster 的主要功能是：（1）当用户作业提交时，ApplicationMaster 与 ResourceManager 协商获取资源，ResourceManager 会以容器的形式为ApplicationMaster分配资源；（2）把获得的资源进一步分配给内部的各个任务（Map任务或Reduce任务），实现资源的“二次分配”；（3）与NodeManager保持交互通信进行应用程序的启动、运行、监控和停止，监控申请到的资源的使用情况，对所有任务的执行进度和状态进行监控，并在任务发生失败时执行失败恢复（即重新申请资源重启任务）；（4）定时向ResourceManager发送“心跳”消息，报告资源的使用情况和应用的进度信息；（5）当作业完成时，ApplicationMaster 向ResourceManager注销容器，执行周期完成。</p>
<p><img src="/images/2019/picture/Big-data/26.jpeg" alt="架构设计"> </p>
<p>NodeManager是驻留在一个YARN集群中的每个节点上的代理，主要负责容器生命周期管理，监控每个容器的资源（CPU、内存等）使用情况，跟踪节点健康状况，并以“心跳”的方式与ResourceManager保持通信，向ResourceManager汇报作业的资源使用情况和每个容器的运行状态，同时，它还要接收来自 ApplicationMaster 的启动/停止容器的各种请求</p>
<p>在集群部署方面，YARN 的各个组件是和 Hadoop 集群中的其他组件进行统一部署的</p>
<p>YARN的ResourceManager组件和HDFS的名称节点（NameNode）部署在一个节点上，YARN的ApplicationMaster及NodeManager是和HDFS的数据节点（DataNode）部署在一起的。YARN中的容器代表了CPU、内存、网络等计算资源，它也是和HDFS的数据节点一起的。</p>
<h3 id="8-3-4-YARN工作流程"><a href="#8-3-4-YARN工作流程" class="headerlink" title="8.3.4 YARN工作流程"></a>8.3.4 YARN工作流程</h3><p>YARN的工作流程如图8-8所示，在YARN框架中执行一个MapReduce程序时，从提交到完成需要经历如下8个步骤。① 用户编写客户端应用程序，向 YARN 提交应用程序，提交的内容包括 ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。② YARN中的ResourceManager负责接收和处理来自客户端的请求。接到客户端应用程序请求后，ResourceManager里面的调度器会为应用程序分配一个容器。同时，ResourceManager的应用程序管理器会与该容器所在的 NodeManager 通信，为该应用程序在该容器中启动一个ApplicationMaster（即图8-8中的“MR App Mstr”）。③ ApplicationMaster 被创建后会首先向 ResourceManager 注册，从而使得用户可以通过ResourceManager 来直接查看应用程序的运行状态。接下来的步骤 4～7 是具体的应用程序执行步骤。④ ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请资源。⑤ ResourceManager 以“容器”的形式向提出申请的 ApplicationMaster 分配资源，一旦ApplicationMaster 申请到资源后，就会与该容器所在的 NodeManager 进行通信，要求它启动任务。⑥ 当ApplicationMaster要求容器启动任务时，它会为任务设置好运行环境（包括环境变量、JAR 包、二进制程序等），然后将任务启动命令写到一个脚本中，最后通过在容器中运行该脚本来启动任务。⑦ 各个任务通过某个 RPC 协议向 ApplicationMaster 汇报自己的状态和进度，让ApplicationMaster可以随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。⑧ 应用程序运行完成后，ApplicationMaster向ResourceManager的应用程序管理器注销并关闭自己。若 ApplicationMaster 因故失败，ResourceManager 中的应用程序管理器会监测到失败的情形，然后将其重新启动，直到所有的任务执行完毕。</p>
<p><img src="/images/2019/picture/Big-data/27.jpeg" alt="架构设计">  </p>
<h3 id="8-3-5-YARN框架与MapReduce1-0框架的对比分析"><a href="#8-3-5-YARN框架与MapReduce1-0框架的对比分析" class="headerlink" title="8.3.5 YARN框架与MapReduce1.0框架的对比分析"></a>8.3.5 YARN框架与MapReduce1.0框架的对比分析</h3><p>而 YARN 则是一个纯粹的资源调度管理框架，在它上面可以运行包括 MapReduce 在内的不同类型的计算框架，默认类型是MapReduce</p>
<p>YARN 有着更加“宏伟”的发展构想，即发展成为集群中统一的资源管理调度框架，在一个集群中为上层的各种计算框架提供统一的资源管理调度服务。</p>
<h2 id="8-4-Hadoop生态系统中具有代表性的功能组件"><a href="#8-4-Hadoop生态系统中具有代表性的功能组件" class="headerlink" title="8.4 Hadoop生态系统中具有代表性的功能组件"></a>8.4 Hadoop生态系统中具有代表性的功能组件</h2><p>Pig通常用于 ETL（Extraction、Transformation、Loading）过程，即来自各个不同数据源的数据被收集过来以后，采用Pig进行统一加工处理，然后加载到数据仓库Hive中，由Hive实现对海量数据的分析。</p>
<p>图8-11中，group by和join操作都“跨越”了Map和Reduce两个阶段，这是因为，group by和join操作都涉及到Shuffle过程，根据“第7章MapReduce”可以知道，Shuffle过程包含了Map端和Reduce端，所以图中表示group by和join操作的矩形框与Map和Reduce两个阶段都存在重叠区域。</p>
<p><img src="/images/2019/picture/Big-data/28.jpeg" alt="架构设计">  </p>
<p>因此Tez框架可以发挥重要的作用。可以让 Tez 框架运行在 YARN 框架之上，如图 8-13所示，然后让MapReduce、Pig和Hive等计算框架运行在 Tez 框架之上，从而借助于 Tez 框架实现对MapReduce、Pig和Hive等的性能优化，更好地解决现有MapReduce框架在迭代计算（如PageRank计算）和交互式计算方面存在的问题。</p>
<p><img src="/images/2019/picture/Big-data/29.jpeg" alt="架构设计"></p>
<p>Kafka是由LinkedIn公司开发的一种高吞吐量的分布式发布订阅消息系统，用户通过Kafka系统可以发布大量的消息，同时也能实时订阅消费消息。Kafka 设计的初衷是构建一个可以处理海量日志、用户行为和网站运营统计等的数据处理框架。为了满足上述应用需求，就需要同时提供实时在线处理的低延迟和批量离线处理的高吞吐量。</p>
<p>其他工具加入大数据生态系统后，只需要开发和这款通用工具的数据交换方案，就可以通过这个交换枢纽轻松实现和其他Hadoop组件的数据交换。Kafka就是一款可以实现这种功能的产品。</p>
<p>在公司的大数据生态系统中，可以把Kafka作为数据交换枢纽，不同类型的分布式系统（如关系数据库、NoSQL数据库、流处理系统、批处理系统等）可以统一接入Kafka，如图8-14所示，实现和Hadoop各个组件之间的不同类型数据的实时高效交换，较好地满足各种企业的应用需求</p>
<p><img src="/images/2019/picture/Big-data/31.jpeg" alt="架构设计"></p>
<h1 id="第9章-Spark"><a href="#第9章-Spark" class="headerlink" title="第9章 Spark"></a>第9章 Spark</h1><p>而为了使编写程序更为容易，Spark使用简练、优雅的Scala语言编写，基于Scala提供了交互式的编程体验。</p>
<h2 id="9-1-概述"><a href="#9-1-概述" class="headerlink" title="9.1 概述"></a>9.1 概述</h2><p>Spark具有如下4个主要特点。</p>
<ul>
<li>① 运行速度快。Spark使用先进的DAG（Directed Acyclic Graph，有向无环图）执行引擎，以支持循环数据流与内存计算，基于内存的执行速度可比Hadoop MapReduce快上百倍，基于磁盘的执行速度也能快十倍。</li>
</ul>
<p><img src="/images/2019/picture/Big-data/32.jpeg" alt="架构设计"></p>
<ul>
<li>② 容易使用。Spark支持使用Scala、Java、Python和R语言进行编程，简洁的API设计有助于用户轻松构建并行程序，并且可以通过Spark Shell进行交互式编程。</li>
<li>③ 通用性。Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件，这些组件可以无缝整合在同一个应用中，足以应对复杂的计算。</li>
<li>④ 运行模式多样。Spark可运行于独立的集群模式中，或者运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源。</li>
</ul>
<p>scala 优势：</p>
<ul>
<li>① Scala具备强大的并发性</li>
<li>② Scala语法简洁，</li>
<li>③ Scala兼容Java，</li>
</ul>
<p>spark 优势： </p>
<ul>
<li>① Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比MapReduce更灵活。</li>
<li>② Spark提供了内存计算，中间结果直接放到内存中，带来了更高的迭代运算效率。</li>
<li>③ Spark基于DAG的任务调度执行机制，要优于MapReduce的迭代执行机制。</li>
</ul>
<h2 id="9-2-Spark生态系统"><a href="#9-2-Spark生态系统" class="headerlink" title="9.2 Spark生态系统"></a>9.2 Spark生态系统</h2><p>Spark 的设计遵循“一个软件栈满足不同应用场景”的理念，逐渐形成了一套完整的生态系统，既能够提供内存计算框架，也可以支持SQL即席查询、实时流式计算、机器学习和图计算等。Spark可以部署在资源管理器YARN之上，提供一站式的大数据解决方案。</p>
<p><img src="/images/2019/picture/Big-data/33.jpeg" alt="架构设计"></p>
<h2 id="9-3-Spark运行架构"><a href="#9-3-Spark运行架构" class="headerlink" title="9.3 Spark运行架构"></a>9.3 Spark运行架构</h2><p>本节首先介绍Spark的基本概念和架构设计方法，然后介绍Spark运行基本流程，最后介绍RDD的运行原理。</p>
<h3 id="9-3-1-基本概念"><a href="#9-3-1-基本概念" class="headerlink" title="9.3.1 基本概念"></a>9.3.1 基本概念</h3><p>在具体讲解Spark运行架构之前，需要先了解以下7个重要的概念。① RDD：是弹性分布式数据集（Resilient Distributed Dataset）的英文缩写，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型。② DAG：是Directed Acyclic Graph（有向无环图）的英文缩写，反映RDD之间的依赖关系。③ Executor：是运行在工作节点（Worker Node）上的一个进程，负责运行任务，并为应用程序存储数据。④ 应用：用户编写的Spark应用程序。⑤ 任务：运行在Executor上的工作单元。⑥ 作业：一个作业包含多个RDD及作用于相应RDD上的各种操作。⑦ 阶段：是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为“阶段”，或者也被称为“任务集”。</p>
<p>Spark运行架构如图9-5所示，包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor）。</p>
<p><img src="/images/2019/picture/Big-data/34.jpeg" alt="架构设计"> </p>
<p>与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点：一是利用多线程来执行具体的任务（Hadoop MapReduce采用的是进程模型），减少任务的启动开销；二是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，当需要多轮迭代计算时，可以将中间结果存储到这个存储模块里，下次需要时就可以直接读该存储模块里的数据，而不需要读写到HDFS等文件系统里，因而有效减少了IO开销；或者在交互式查询场景下，预先将表缓存到该存储系统上，从而可以提高读写IO性能。</p>
<p><img src="/images/2019/picture/Big-data/35.jpeg" alt="架构设计"> </p>
<p>Spark运行基本流程如图9-7 所示，流程如下。（1）当一个Spark应用被提交时，首先需要为这个应用构建起基本的运行环境，即由任务控制节点（Driver）创建一个SparkContext，由SparkContext负责和资源管理器（Cluster Manager）的通信以及进行资源的申请、任务的分配和监控等。SparkContext 会向资源管理器注册并申请运行Executor的资源。（2）资源管理器为Executor分配资源，并启动Executor进程，Executor运行情况将随着“心跳”发送到资源管理器上。（3）SparkContext 根据 RDD 的依赖关系构建 DAG 图，DAG 图提交给 DAG 调度器（DAGScheduler）进行解析，将DAG图分解成多个“阶段”（每个阶段都是一个任务集），并且计算出各个阶段之间的依赖关系，然后把一个个“任务集”提交给底层的任务调度器（TaskScheduler）进行处理；Executor 向 SparkContext 申请任务，任务调度器将任务分发给 Executor 运行，同时SparkContext将应用程序代码发放给Executor。（4）任务在Executor上运行，把执行结果反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。</p>
<p><img src="/images/2019/picture/Big-data/36.jpeg" alt="架构设计">  </p>
<p>RDD 的设计理念源自 AMP 实验室发表的论文《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》</p>
<p>一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可以分成多个分区，每个分区就是一个数据集片段，并且一个 RDD 的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算</p>
<p>总体而言，Spark采用RDD以后能够实现高效计算的主要原因如下。</p>
<ul>
<li>（1）高效的容错性</li>
<li>（2）中间结果持久化到内存</li>
<li>（3）存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化开销。</li>
</ul>
<p>4.RDD之间的依赖关系</p>
<p>RDD中不同的操作会使得不同RDD中的分区产生不同的依赖。RDD中的依赖关系分为窄依赖（Narrow Dependency）与宽依赖（Wide Dependency），两种依赖之间的区别如图9-10所示。</p>
<p><img src="/images/2019/picture/Big-data/37.jpeg" alt="架构设计">  </p>
<p>窄依赖表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区。</p>
<p>宽依赖则表现为存在一个父 RDD的一个分区对应一个子 RDD的多个分区。</p>
<p>窄依赖典型的操作包括map、filter、union等，宽依赖典型的操作包括groupByKey、sortByKey等。对于连接（Join）操作，可以分为两种情况。</p>
<ul>
<li>（1）对输入进行协同划分，属于窄依赖</li>
<li>（2）对输入做非协同划分，属于宽依赖，</li>
</ul>
<p>对于窄依赖的RDD，可以以流水线的方式计算所有父分区，不会造成网络之间的数据混合。对于宽依赖的RDD，则通常伴随着Shuffle操作，即首先需要计算好所有父分区数据，然后在节点之间进行Shuffle。</p>
<p>5.阶段的划分</p>
<p>Spark通过分析各个RDD的依赖关系生成了DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分阶段，具体划分方法是：在DAG中进行反向解析，遇到宽依赖就断开，遇到窄依赖就把当前的RDD加入到当前的阶段中；将窄依赖尽量划分在同一个阶段中，可以实现流水线计算（具体的阶段划分算法请参见 AMP 实验室发表的论文《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》）</p>
<p>由上述论述可知，把一个 DAG 图划分成多个阶段以后，每个阶段都代表了一组关联的、相互之间没有 Shuffle 依赖关系的任务组成的任务集合。每个任务集合会被提交给任务调度器（TaskScheduler）进行处理，由任务调度器将任务分发给Executor运行</p>
<p><img src="/images/2019/picture/Big-data/38.jpeg" alt="架构设计">  </p>
<p>6.RDD运行过程</p>
<p>（1）创建RDD对象。（2）SparkContext负责计算RDD之间的依赖关系，构建DAG。（3）DAGScheduler负责把DAG图分解成多个阶段，每个阶段中包含了多个任务，每个任务会被任务调度器分发给各个工作节点（Worker Node）上的Executor去执行。</p>
<p><img src="/images/2019/picture/Big-data/39.jpeg" alt="架构设计"> </p>
<h2 id="9-4-Spark的部署和应用方式"><a href="#9-4-Spark的部署和应用方式" class="headerlink" title="9.4 Spark的部署和应用方式"></a>9.4 Spark的部署和应用方式</h2><p>本节首先介绍Spark支持的三种典型部署方式，即standalone、Spark on Mesos和Spark on YARN</p>
<h2 id="9-5-Spark编程实践"><a href="#9-5-Spark编程实践" class="headerlink" title="9.5 Spark编程实践"></a>9.5 Spark编程实践</h2><p>表9-2 常用的几个Action API介绍</p>
<p><img src="/images/2019/picture/Big-data/40.jpeg" alt="架构设计"> </p>
<p>表9-3 常用的几个Transformation API介绍</p>
<p><img src="/images/2019/picture/Big-data/41.jpeg" alt="架构设计"> </p>
<p>Spark属于MapReduce计算模型，因此也可以实现MapReduce的计算流程，如实现单词统计，可以首先使用 flatMap()将每一行的文本内容通过空格进行划分为单词；然后使用 map()将单词映射为(K,V)的键值对，其中K为单词，V为1；最后使用reduceByKey()将相同单词的计数进行相加，最终得到该单词总的出现次数。具体实现命令如下：scala &gt; val wordCounts = textFile.flatMap(line =&gt; line.split(“ “)).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b)scala &gt; wordCounts.collect() // 输出单词统计结果// Array[(String, Int)]= Array((package,1), (For,2), (Programs,1), (processing.,1), (Because,1), (The,1)…)在上面的代码中，flatMap()、map()和reduceByKey()都是属于“转换”操作，由于Spark采用了惰性机制，这些转换操作只是记录了 RDD 之间的依赖关系，并不会真正计算出结果。最后，运行collect()，它属于“行动”类型的操作，这时才会执行真正的计算，Spark会把计算打散成多个任务分发到不同的机器上并行执行。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/05/prepare-of-time-series/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/05/prepare-of-time-series/" itemprop="url">2.时间序列准备</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-05T22:50:51+08:00">
                2019-09-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/time-series/" itemprop="url" rel="index">
                    <span itemprop="name">time series</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="预测者的工具集"><a href="#预测者的工具集" class="headerlink" title="预测者的工具集"></a>预测者的工具集</h1><p>本章我们会讨论一些在不同预测场景中应用的常用工具。我们将介绍一些基准预测方法，并探讨如何通过数据变换与调整简化预测，如何判断预测是否充分利用了现有信息以及如何计算预测区间。</p>
<h2 id="一些简单的预测方法"><a href="#一些简单的预测方法" class="headerlink" title="一些简单的预测方法"></a>一些简单的预测方法</h2><h3 id="均值法"><a href="#均值法" class="headerlink" title="均值法"></a>均值法</h3><h3 id="Naive-方法"><a href="#Naive-方法" class="headerlink" title="Naïve 方法"></a>Naïve 方法</h3><h3 id="季节性-Naive-方法"><a href="#季节性-Naive-方法" class="headerlink" title="季节性 Naïve 方法"></a>季节性 Naïve 方法</h3><h3 id="漂移法"><a href="#漂移法" class="headerlink" title="漂移法"></a>漂移法</h3><p><strong>例子</strong></p>
<h2 id="变换和调整"><a href="#变换和调整" class="headerlink" title="变换和调整"></a>变换和调整</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/rudiments-of-time-series/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/rudiments-of-time-series/" itemprop="url">1. 时间序列入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T16:49:11+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/time-series/" itemprop="url" rel="index">
                    <span itemprop="name">time series</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h1><p>开始时，作者讲了一些小典故，非常有意思:</p>
<blockquote>
<p>数千年来，预测一直吸引着人们。古巴比伦的预言家们可以基于蛆在腐烂的绵羊肝脏中的分布预测未来。在公元前300年，想要预知未来的人们会前往希腊的德尔菲祈求神谕，神谕会在被乙醚蒸汽陶醉的情况下给出她的预言。预言家们在康斯坦丁大帝的统治下经历了一段艰难的时期，康斯坦丁大帝在公元357年颁布了一项法令禁止任何人“去咨询占卜者、数学家或预言家，对预言未来的好奇将被永远禁止。”1763年英国颁布了一项相似的禁令：通过预测骗取钱财将被判作犯罪，其刑罚是判处三个月的狱中的苦役。</p>
</blockquote>
<h2 id="什么是可以被预测的"><a href="#什么是可以被预测的" class="headerlink" title="什么是可以被预测的"></a>什么是可以被预测的</h2><p>事件（或数量）的可预测性取决于以下几个因素：</p>
<ul>
<li>我们对它的影响因素的了解程度;</li>
<li>有多少数据是可用的;</li>
<li>预测是否会影响我们试图预测的事物。</li>
</ul>
<h2 id="预测、计划和目标"><a href="#预测、计划和目标" class="headerlink" title="预测、计划和目标"></a>预测、计划和目标</h2><p>该小节讲述了预测、目标以及计划之间的关系。</p>
<p>预测：它是指在考虑到所有可用信息的前提下，包括历史数据和可以影响预测的任何未来事件的知识，尽可能准确地预言。</p>
<p>目标：它是指你想要发生的事情。目标理应与预测和计划联系在一起，但是这并不经常发生。很多时候，设定目标时没有任何如何去实现这些目标的计划，也没有目标是否切合实际的预测。</p>
<p>计划：它是对预测和目标的回应。计划包括制定使得你的预测符合你的目标的适当行动。</p>
<p>而预测又可以分为短期预测、中期预测、长期预测等，具体预测取决于特定的应用场景。</p>
<h2 id="决定预测什么"><a href="#决定预测什么" class="headerlink" title="决定预测什么"></a>决定预测什么</h2><ul>
<li>决定预测什么：<ul>
<li>用于每条产品线或一组产品？</li>
<li>用于每条产品线或一组产品？</li>
<li>每周数据、月度数据或年度数据？</li>
</ul>
</li>
<li>考虑预测的前景时段：<ul>
<li>需要提前1个月，提前6个月还是提前10年预测？</li>
<li>预测需要多频繁？需要经常进行的预测, 最好是使用自动化系统, 而不是需要仔细人工操作。</li>
</ul>
</li>
</ul>
<p>在制定合适的预测方法之前, 预测者大一部分的时间将用于寻找和整理可用数据。</p>
<h2 id="预测数据和方法"><a href="#预测数据和方法" class="headerlink" title="预测数据和方法"></a>预测数据和方法</h2><p>在大程度上，什么数据是可用的决定了适合什么合适的预测方法。</p>
<ul>
<li><p>定性预测：</p>
<ul>
<li>如果没有可用的数据，或者如果可用的数据与预测无关，那么应该使用定性预测方法。</li>
</ul>
</li>
<li><p>在满足以下两个条件的时候可以使用定量预测 ：</p>
<ul>
<li>关于过去的数字化信息是可以用的；</li>
<li>有理由假设过去的一些模式会在未来延续下去。</li>
</ul>
</li>
</ul>
<p>大多数定量预测问题都使用时间序列数据 (按时间间隔定期收集) 或横截面数据 (在一个时间点收集)。在本书中,我们关注预测未来的数据,并且我们主要专注于时间序列领域。</p>
<p>时间序列数据样例包括:</p>
<ul>
<li>IBM每日股票价格</li>
<li>每月降水量</li>
<li>亚马逊季度销售结果</li>
<li>谷歌年度利润</li>
</ul>
<p>最简单的时间序列预测方法只用了预测变量的信息，而不去寻找影响预测变量的因素。因此，这些方法可以推断趋势部分和季节性部分，但是它们会忽略掉所有其他的信息，如营销计划，竞争对手活动，经济状况变动等。</p>
<p>用于预测的时间序列模型包括分解模型，指数平滑模型，ARIMA 模型。这些模型分别在章节 6，7 和 8 中进行了分析探讨。</p>
<h3 id="预测变量与时间序列预测"><a href="#预测变量与时间序列预测" class="headerlink" title="预测变量与时间序列预测"></a>预测变量与时间序列预测</h3><p>以预测夏季每小时用电需求量为例：</p>
<ul>
<li>解释模型</li>
</ul>
<p>ED = f(当前气温，经济实力，人口当日时间，星期几，误差)</p>
<p>这种关系并不确切–总会有不能由预测变量决定的电力需求变化。右侧的“误差”项表示随机波动和没有被包括在模型中的相关变量的影响。我们将它称之为“解释模型”，因为它帮助解释电力需求变化的原因。</p>
<ul>
<li>时间序列模型</li>
</ul>
<center>$$ ED _{t+1} = f(ED _{t}, ED _{t-1}, ED _{t-2}, ED _{t-3}, …, error) $$</center>

<p>t 表示当前的时间， t+1 表示下一个小时，t−1 表示前一个小时，t−2 表示前两个小时，以此类推。此处，对未来的预测是基于变量的过去值，而不是基于可能影响系统的外部变量。同样，右侧的“误差”项允许随机波动和不包含在模型中的相关变量的影响。</p>
<ul>
<li>混合模型：结合了上述两种模型的特点</li>
</ul>
<center>$$ ED _{t+1} = f(ED _{t}, 当前气温，当日时间，星期几，误差) $$</center>

<p>解释模型非常有用,因为它包含了有关其他变量的信息,而不仅仅是要预测的变量的历史值。但是, 预测者可能选择时间序列模型而不是解释性或混合模型的原因有多种：</p>
<ul>
<li>这一系统可能不被理解,即使被理解,也很难衡量被认为应该管理行为的关系。</li>
<li>其次,有必要知道或预测各种预测因子的未来价值, 以便能够预测有意义的变量, 但是这可能太难了。</li>
<li>第三, 可能主要只是关注预测会发生什么,而不知道为什么会发生。</li>
<li>最后,时间序列模型可以提供比解释或混合模型更准确的预测。</li>
</ul>
<p>在预测中使用的模型取决于可用的资源和数据、模型的准确性以及预测模型的使用方式。</p>
<h2 id="案例学习"><a href="#案例学习" class="headerlink" title="案例学习"></a>案例学习</h2><p>该小节，作者讲了4个案例，有以下几个特点：</p>
<ul>
<li>时间序列数据显示出一系列模式，其中一些带有长期趋势，一些带有季节变动，还有一些两者都不具备。</li>
<li>几乎每种药品的销售量数据都包含长期趋势和季节变动模式。很多药品的销量会因药品补贴政策的变化而突然上升或下降，对很多药品的津贴支出也会因出现低价可替代药品而突然发生变化。（因此，我们需要寻找到一种能够对包含长期趋势和季节变动因素的数据进行预测的方法，使得该方法不仅可以对潜在模式下的突然变动进行稳健预测，同时能够处理大样本的时间序列数据。）</li>
<li>一群专家正在预测汽车转售价格。他们认为统计模型的建立会对他们的生计造成威胁，因而在提供信息方面不合作。尽管如此，该公司还是给我们提供了大量的车辆和汽车转售价格的历史数据。</li>
<li>航空乘客人数会受到学校假期、重大体育赛事、广告活动、竞争行为等影响。一般情况下，澳大利亚不同城市的学校假期不会同时出现，体育赛事有时也会从一个城市转移到另一个城市。在历史数据相应期间发生过一场关键飞行员的罢工运动，其间几个月都没有相关航线运行，一条新的低价航线推出后也惨遭失败。在历史数据期间的末尾，航空公司将一些经济舱座位重新改造为商务舱和头等舱座位，然而几个月后，座位安排重新恢复到原来的状态。</li>
</ul>
<h2 id="预测过程的主要步骤"><a href="#预测过程的主要步骤" class="headerlink" title="预测过程的主要步骤"></a>预测过程的主要步骤</h2><p>一个预测过程通常包括五个基本步骤。</p>
<h3 id="定义问题"><a href="#定义问题" class="headerlink" title="定义问题"></a>定义问题</h3><p>通常这是预测中最困难的步骤。要准确定义这个问题，需要了解怎样运用预测方法，谁需要这个预测，以及预测效果如何满足需要这个预测的机构。预测人员需要花费一定时间与所有参与收集数据、维护数据库和使用这个预测对未来进行规划的人沟通。</p>
<h3 id="收集信息"><a href="#收集信息" class="headerlink" title="收集信息"></a>收集信息</h3><p>一般至少需要两种信息收集方式：(a) 统计数据，(b) 收集数据和进行预测方面专家的积累经验。通常情况下，要获得足够多的历史数据以构建良好的统计模型是很困难的。在这种情况下，可以使用 第4节 中的判断预测方法。有时候，陈旧数据会因相应数据发生结构变化而失效，因而我们一般只选择使用较新的数据。然而，一个好的统计模型可以处理系统中的结构变化，因此不要轻易丢弃好的数据。</p>
<h3 id="初步（探索性）分析"><a href="#初步（探索性）分析" class="headerlink" title="初步（探索性）分析"></a>初步（探索性）分析</h3><p>总是以图形开头，观察思考以下几个问题：</p>
<ul>
<li>有一致的模式吗？</li>
<li>有明显的长期趋势吗？</li>
<li>季节性重要吗？</li>
<li>是否有证据表明商业周期存在？</li>
<li>数据中是否包含需要专业知识解释的异常值？</li>
<li>用于分析的变量之间的相关性有多强？</li>
</ul>
<p>目前已经开发了各种工具来帮助进行这种分析。这些将在章节 2 和 章节6中讨论。、</p>
<h3 id="选择及拟合模型"><a href="#选择及拟合模型" class="headerlink" title="选择及拟合模型"></a>选择及拟合模型</h3><p>最佳模型的选择取决于历史数据的可用性、预测变量与各解释变量之间的相关性，以及预测的使用方式。比较两个或三个潜在的模型是很常见的。每个模型本身都基于人为提出的一组假设(显式和隐式)而建立，通常包含一个或多个参数，这些参数必须使用已知的历史数据进行估计。我们将讨论回归模型(章节 5)、指数平滑方法(章节 7)、Box-Jenkins ARIMA模型(章节 8)、动态回归模型(章节 9)、分层预测(章节 10)，以及其他各种方法，包括计数时间序列、神经网络和章节 11中的向量自回归。</p>
<h3 id="使用及评估预测模型"><a href="#使用及评估预测模型" class="headerlink" title="使用及评估预测模型"></a>使用及评估预测模型</h3><p>一旦模型及其参数确定后，该模型就可以用来进行预测。模型的预测效果只有用于预测的数据得到之后才能得到正确的评价。目前已经开发了许多方法来评估预测的准确性。在使用和进行预测时会存在很多组织结构问题。对其中一些问题的简要讨论将在章节 3 中给出。</p>
<h2 id="统计预测观点"><a href="#统计预测观点" class="headerlink" title="统计预测观点"></a>统计预测观点</h2><p>我们试图预测的东西是未知的(或者我们不能预测它)，所以我们可以把它想象成一个随机变量。</p>
<p>例如，下个月的总销售额可能会有一系列的可能值，直到月底我们把实际销售额加起来，我们才知道这个值会是多少。所以在我们知道下个月的销售情况之前，这是一个随机的变量。</p>
<p>因为下个月时间节点比较近，我们通常清楚销售量大概是多少。如果我们预测明年同一个月的销售情况，可能的销售量变动就会较大。在大多数预测情况下，随着事件的临近，预测对象的相关变动较小。换句话说，预测的越早，预测结果越不稳定。</p>
<p>我们可以想象许多可能性，每一个都为我们将要预测的事物带来不同的影响。下图是1980年到2015年澳大利亚的国际游客总数以及2016至2025年的10个可能的预测值。</p>
<p><img src="/images/2019/picture/time-forecast/1-2.png" alt></p>
<blockquote>
<p>用上面10个预测来加权，应该效果不错。</p>
</blockquote>
<p>我们进行预测的过程实际是寻找随机变量可能取值范围内的中间值。通常情况下，预测会伴随着一个预测区间，给出一个随机变量具有较高概率的范围值。例如，95%的预测区间包含一系列的值，这个预测区间包含实际未来值的概率为95%。</p>
<p>我们通常会给出这些预测区间，而不是图 1.2 中显示的单个可能的预测值。下面的图表显示了未来澳大利亚国际游客的80%和95%的预测区间。蓝线是可能的预测值的平均值，我们称之为“点预测”。</p>
<p><img src="/images/2019/picture/time-forecast/1-3.png" alt></p>
<p>使用下标 t 作为时间。例如， $ y _{t} $ 表示时间 t 对应的观察值。假设将观察到的所有信息表示为 T，目标是预测 $ y _{t} $ .此时，我们将 $ y _{t} | T $ 表示为“给定已知 T 情况下的随机变量 $ y _{t} $ ”。这个随机变量取值的概率测度称为 $ y _{t} | T $ 的 “概率分布”。在预测中，我们称之为“预测分布”。</p>
<p>每当我们谈到“预测”时，通常指的是预测分布的平均值，用 $ \hat{y _{t}} $ 来表示 $ y _{t} $ 的预测值，这意味着 $ y _{t} $ 所有可能取值的均值包含了我们所有已知的信息。有时我们将使用 $\hat{y _{t}} $ 来表示预测分布的‘中位数’(或中间值)。</p>
<p>明确指出我们在进行预测时使用的信息是很必要的。例如，我们使用 $ \hat{y _{t}} $ 表示在已知观测值 $ (y _{1},…,y _{t-1}) $ 的情况下 $ y _{t} $ 的预测值。类似地，我们使用 $ \hat{y _{T+h|T}} $ 表示在已知观测值 $ (y _{1},…,y _{T}) $ 的情况下 $ y _{T+h} $ 的预测值（即考虑时间 T 之前所有观测值的h步预测）.</p>
<h2 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><ul>
<li>Armstrong (2001) 涵盖了整个预测领域的内容，每个章节都由不同的专家撰写。文章部分观点非常武断(我们并不同意其中的观点)，但在处理预测问题上有很多优秀的一般性建议。</li>
<li>Ord, Fildes, and Kourentzes (2017) 是一本预测教材，涵盖了与本书相同的部分，但重点不同，它不关注任何特定的软件环境。这是由三位有着几十年经验的权威预测专家撰写的。</li>
</ul>
<h1 id="时间序列图形"><a href="#时间序列图形" class="headerlink" title="时间序列图形"></a>时间序列图形</h1><p>对于任何数据分析工作而言，其首要任务是数据可视化。图示化数据可以清晰地展现数据的特征，包括数据的形态、异常值、随时间变化情况以及变量间的相互关系。我们在预测时应尽可能地将图中显示的特征纳入考虑。正如数据类型决定使用什么预测方法一样，数据类型也决定了使用什么图形来展示数据。</p>
<p>在画图之前，首先我们应该在R中设置我们的时间序列数据。</p>
<h2 id="ts-对象"><a href="#ts-对象" class="headerlink" title="ts 对象"></a>ts 对象</h2><p>R语言中的一种记录时序的对象。</p>
<pre><code>y &lt;- ts(c(123,39,78,52,110), start=2012)
</code></pre><h2 id="时间图"><a href="#时间图" class="headerlink" title="时间图"></a>时间图</h2><p>对于时间序列数据而言，我们从最简单的时间图开始。时间图是用将观测值与观测时间点作图，散点之间用直线连接。例如图2.1表示在澳大利亚两个最大的城市之间，Ansett航空公司的每周客流量。</p>
<p><img src="/images/2019/picture/time-forecast/2-1.png" alt></p>
<p>该时间图直观地展现出数据具有的一些特征：</p>
<ul>
<li>由于1989年当地的工业纠纷，当年的客流量为0.</li>
<li>在1992年中，由于一部分经济舱被商务舱取代，导致客流量大幅减少。</li>
<li>1991年下半年客流量大幅上升。</li>
<li>由于假日效应，在每年年初，客流量都会有一定幅度的下降。</li>
<li>这是序列存在长期波动，在1987年向上波动，在1988年向下波动，而在1990年和1991年又再次向上波动。</li>
<li>在某些时期存在缺失值。</li>
</ul>
<p><img src="/images/2019/picture/time-forecast/2-2.png" alt></p>
<ul>
<li>显然，图示的时间序列具有明显增长的趋势。</li>
<li>同时，在上升趋势中伴随着明显的季节性。</li>
<li>在每年年底，由于政府补贴计划，使得降糖药品更便宜，所以人们倾向于在年底囤积药物，从而导致年初的销售额大幅下降。</li>
<li>因此，当我们对降糖药物的销量进行预测时，需同时考虑其趋势和季节性因素。</li>
</ul>
<h2 id="时间序列形态"><a href="#时间序列形态" class="headerlink" title="时间序列形态"></a>时间序列形态</h2><p>我们通常使用例如“趋势”、“季节性”等词语描述时间序列。在深入研究时间序列形态时，应该更精确的定义这些词语。</p>
<h3 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h3><p>当一个时间序列数据长期增长或者长期下降时，表示该序列有 <code>趋势</code> 。在某些场合，趋势代表着“转换方向”。例如从增长的趋势转换为下降趋势。在图 2.2 中，明显存在一个增长的趋势。</p>
<h3 id="季节性"><a href="#季节性" class="headerlink" title="季节性"></a>季节性</h3><p>当时间序列中的数据受到季节性因素（例如一年的时间或者一周的时间）的影响时，表示该序列具有 <code>季节性</code> 。季节性总是一个已知并且固定的频率。由于抗糖尿病药物的成本在年底时会有变化，导致上述抗糖尿药物的月销售额存在季节性。</p>
<h3 id="周期性"><a href="#周期性" class="headerlink" title="周期性"></a>周期性</h3><p>当时间序列数据存在不固定频率的上升和下降时，表示该序列有 <code>周期性</code> 。这些波动经常由经济活动引起，并且与“商业周期”有关。周期波动通常至少持续两年。</p>
<blockquote>
<p>许多初学者都不能很好的区分季节性和周期，然而这两个概念是完全不同的。当数据的波动是无规律时，表示序列存在周期性；如果波动的频率不变并且与固定长度的时间段有关，表示序列存在季节性。一般而言，周期的长度较长，并且周期的波动幅度也更大。</p>
</blockquote>
<p>许多时间序列同时包含趋势、季节性以及周期性。当我们选择预测方法时，首先应该分析时间序列数据所具备的特征，然后再选择合适的预测方法抓取特征。</p>
<p>以下四个示例分别是上述三个特征的不同组合。</p>
<p><img src="/images/2019/picture/time-forecast/2-3.png" alt></p>
<ul>
<li>美国新建房屋销售额（左上）表现出强烈的年度季节性，以及周期为6~10年的周期性。但是数据并没有表现出明显的趋势。</li>
<li>美国国债价格（右上）表示1981年美国国债在芝加哥市场连续100个交易日的价格。可以看出，该序列并没有季节性，但是有明显下降的趋势。假如我们拥有该序列更多的观测数据，我们可以看到这个下降的趋势是一个长期循环的一部分。但是现在我们只有连续100天的数据，它表现出下降的趋势。</li>
<li>澳大利亚月度电力产值数据（左下）明显表现出向上增长的趋势，以及强季节性。但是并不存在周期性。</li>
<li>Google收盘股价格（右下）的价格波动没有趋势，季节性和周期性。随机波动没有良好的形态特性，不能很好地预测。</li>
</ul>
<h2 id="季节图"><a href="#季节图" class="headerlink" title="季节图"></a>季节图</h2><p>季节图和时间序列图很相似，不同之处是季节图是针对观察数据的“季节性”绘制的。下面的例子是降糖药物的销售情况。</p>
<p><img src="/images/2019/picture/time-forecast/2-4.png" alt></p>
<p>在早些年，数据形态基本相同，但是近些年数据存在相互堆叠的情况。季节图可以很清晰的显示季节形态，这对识别数据形态是否发生变化非常有效。</p>
<p>在本例中，在每年一月份降糖药物的销量都会大幅下降。实际上，患者会在12月下旬大量购买降糖药物，但是这部分销量会在一两周后才向政府登记。从上图还可以看出，2008年3月销量大幅下降（其他年份2月份至3月份的销量增加）。2008年6月份销量较少可能是由于销量数据收集不完整导致。</p>
<p>季节图中可以将直角坐标转换为极坐标。</p>
<p><img src="/images/2019/picture/time-forecast/2-5.png" alt></p>
<h2 id="子系列季节图"><a href="#子系列季节图" class="headerlink" title="子系列季节图"></a>子系列季节图</h2><p><img src="/images/2019/picture/time-forecast/2-6.png" alt></p>
<p>图中的水平线表示每月的平均销量。子系列季节图可以清晰的描绘出数据的潜在季节性形态，并且显示了季节性随时间的变化情况。这类图可以很好地查看各时期内数据的变化情况。在本例中，子系列季节图并没有明显地体现数据特性，但是这是观察季节性变化最有用的方式。</p>
<h2 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h2><p>在此之前，我们所讨论的内容都是单个时间序列的可视化。此外，多个时间序列的可视化也是非常有用的。 图 2.7 分别展示了两个时间序列：2014年澳大利亚维多利亚州每半小时的用电量（以千兆瓦为单位）和温度（以摄氏度为单位）</p>
<p><img src="/images/2019/picture/time-forecast/2-7.png" alt></p>
<p><img src="/images/2019/picture/time-forecast/2-8.png" alt></p>
<p>这个散点图可以很好的帮助我们理解变量之间的相互关系。从图中我们可以看出，当温度很高时，人们会大量的使用空调进行降温，进而导致用电量随之增加；当温度很低时，人们会使用空调取暖，也会使得用电量一定程度上增加。</p>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><p>我们经常会用 相关系数 衡量两个两个变量之间的相关强度。假如已知两个变量 x 和 y ，那么它们之间的相关系数为：</p>
<center> $$ r = \frac{\sum (x _{t} - \bar{x})(y _{t} - \bar{y}))}{\sqrt{\sum (x _{t}- \bar{x}) ^{2}}\sqrt{\sum (y _{t}-\bar{y}) ^{2}}} $$ </center>

<p>r 的值始终在-1到1之间。当两个变量完全负相关时，r 值为-1；当两个变量完全正相关时，r 为1.图 2.9 分别展示了不同相关强度的例子。</p>
<p><img src="/images/2019/picture/time-forecast/2-9.png" alt></p>
<p>需要注意的是，相关系数仅仅衡量了变量之间的线性关系，并且有时会导致错误的结果。例如，在图 2.10中，所有例子的相关系数均为0.82，但是它们有着完全不同的形态。 这表明，在分析变量之间关系时，不仅要看相关系数值，而且要关注生成的图形。</p>
<p><img src="/images/2019/picture/time-forecast/2-10.png" alt></p>
<p>在图 2.8中，用电量和温度之间的相关系数仅为0.2798，但并不代表用电量和温度之间存在很强的非线性关系。</p>
<h3 id="散点图矩阵"><a href="#散点图矩阵" class="headerlink" title="散点图矩阵"></a>散点图矩阵</h3><p>当所分析的数据有多个变量时，将每个变量与其他变量进行比较也很有意义。如图2.11所示，表示澳大利亚新南威尔士五个地区的季度游客人数。</p>
<p><img src="/images/2019/picture/time-forecast/2-11.png" alt></p>
<p>如图 2.12所示，我们可以绘制出它们的散点图矩阵。</p>
<p><img src="/images/2019/picture/time-forecast/2-12.png" alt></p>
<p>我们可以通过散点图矩阵快速查看所有变量之间的相关关系。在本例中，由图中第二列数据可知，新南威尔士州北部海岸游客与新南威尔士南部海岸游客之间存在强烈的正关系，而新南威尔士州北部海岸的游客与新南威尔士内陆游客之间几乎没有相关关系。同时，我们可以通过散点图矩阵检测到异常值。由于2000年悉尼奥运会，新南威尔士大都会地区存在异常大的客流量。</p>
<h2 id="滞后图"><a href="#滞后图" class="headerlink" title="滞后图"></a>滞后图</h2><p>图 2.13是澳大利亚每季度啤酒产量的散点图，横轴表示时间序列的滞后阶数。各图分别显示了不同 k 值下 $ y _{t} $ 和 $ y _{t-k} $ 的散点图。</p>
<p><img src="/images/2019/picture/time-forecast/2-13.png" alt></p>
<p>2-13 澳大利亚每季度啤酒产量不同滞后阶数散点图</p>
<p>图中不同颜色代表不同季节，每条线都按时间顺序连接。从图中可以看出，滞后四阶和滞后八阶有正相关关系，说明数据具有很强的季节性。二阶滞后图和六阶滞后图显示，第四季度的峰值对应第二季度的最低点。</p>
<h2 id="自相关"><a href="#自相关" class="headerlink" title="自相关"></a>自相关</h2><p>正如相关系数可以衡量两个变量之间的线性相关关系一样，自相关系数可以测量时间序列 滞后值 之间的线性关系。</p>
<p>以下几个不同的自相关系数，对应于滞后图中的不同情况。例如， $ r _{1} $ 衡量 $ y _{t} $ 和 $ y _{t-1} $ 之间的关系， $ r _{2} $ 衡量 $ y _{t} $ 和 $ y _{t-2} $ 之间的关系.</p>
<p>其中， $ r _{k} $ 的定义如下：</p>
<center> $$ r <em>{k}=\frac{\sum</em>{t=k+1}^{T}(y _{t}-\bar{y})(y <em>{t-k}-\bar{y})}{\sum</em>{t=1}^{T}(y _{t}-\bar{y})^{2}} $$ </center>

<p>其中，T 是时间序列的长度。</p>
<p>澳大利亚啤酒产量数据的前九个自相关系数如下表所示。（略）</p>
<p>各值分别对应于图 2.13 中的九个散点图。通过绘制自相关系数图可以描绘 自相关函数 或者是ACF。因此也被称为相关图。</p>
<p><img src="/images/2019/picture/time-forecast/2-14.png" alt></p>
<p>在该图中：</p>
<ul>
<li>$ r _{4} $ 值最大。这是由于数据的季节性形态：顶峰往往出现在第四季度，谷底往往出现在第二季度。</li>
<li>$ r _{2} $ 值最小。这是由于谷底往往在高峰之后的两个季度出现。</li>
<li>蓝色虚线之内的区域自相关性可近似看做0。这将会在下节详细阐述。</li>
</ul>
<h3 id="ACF-图中的趋势性和季节性"><a href="#ACF-图中的趋势性和季节性" class="headerlink" title="ACF 图中的趋势性和季节性"></a>ACF 图中的趋势性和季节性</h3><p>当数据具有趋势性时，短期滞后的自相关值较大，因为观测点附近的值波动不会很大。时间序列的ACF一般是正值，随着滞后阶数的增加而缓慢下降。</p>
<p>当数据具有季节性时，自相关值在滞后阶数与季节周期相同时（或者在季节周期的倍数）较大。</p>
<p>当数据同时具有趋势和季节性时，我们会观察到组合效应。如图 2.15 是澳大利亚用电量，该序列同时具有趋势和季节性。它的ACF值如图 2.16 所示。</p>
<p><img src="/images/2019/picture/time-forecast/2-15.png" alt></p>
<p><img src="/images/2019/picture/time-forecast/2-16.png" alt></p>
<p>自相关系数值随着滞后阶数增加而缓慢降低，是因为原时间序列中具有趋势变化，而图中的“圆齿状”形状是来源于原时间序列中的季节性变化。</p>
<h2 id="白噪声"><a href="#白噪声" class="headerlink" title="白噪声"></a>白噪声</h2><p>“白噪声”是一个对所有时间其自相关系数为零的随机过程。 图 2.17是一个白噪声的例子。</p>
<p><img src="/images/2019/picture/time-forecast/2-17.png" alt></p>
<p>白噪声函数的自相关函数如下图所示：</p>
<p><img src="/images/2019/picture/time-forecast/2-18.png" alt></p>
<p>对于白噪声而言，我们期望它的自相关值接近0。但是由于随机扰动的存在，自相关值并不会精确地等于0。对于一个长度为 T 的白噪声序列而言，我们期望在0.95的置信度下，它的自相关值处于 $ \pm 2/\sqrt{T} $ 之间。我们可以很容易的画出ACF的边界值（图中蓝色虚线）。如果一个序列中有较多的自相关值处于边界之外，那么该序列很可能不是白噪声序列。</p>
<p>在上例中，序列长度 T = 50，边界为 $ \pm 2/\sqrt{50} = \pm 0.28 $ 。所有的自相关值均落在边界之内，证明序列是白噪声.</p>
<h2 id="拓展阅读-1"><a href="#拓展阅读-1" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><ul>
<li>Cleveland (1993) 是关于数据分析可视化原理的经典著作。虽然已有20多年历史，但它的思想永不过时。</li>
<li>Unwin (2015) 是一本关于使用R进行图形数据分析的前沿著作。它并没有着重介绍如何绘制时间序列图形，更多的是关于使用图形进行数据分析。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/fbprophet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/fbprophet/" itemprop="url">5. fbprophet原理及实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T16:13:28+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="fbprophet简介"><a href="#fbprophet简介" class="headerlink" title="fbprophet简介"></a>fbprophet简介</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/xgboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/xgboost/" itemprop="url">4. xgboost原理及实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T16:11:58+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="xgboost-简介"><a href="#xgboost-简介" class="headerlink" title="xgboost 简介"></a>xgboost 简介</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/07/GBDT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/GBDT/" itemprop="url">3. GBDT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-07T22:38:27+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="提升树模型"><a href="#提升树模型" class="headerlink" title="提升树模型"></a>提升树模型</h1><p>提升方法实际采用加法模型(即基函数的线性组合)与前向分布算法。以决策树为基函数的提升方法称为提升树(boosting tree)。<font color="orange">对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。还有一种实现版本是分类问题、回归问题都使用的是CART回归树。具体情况，还需要查看其对应的实现方法。TODO：sklearn里面的GBDT实现方式。</font> 提升树模型可以表示为决策树的加法模型：</p>
<p><center>$$ f <em>{M}(x)=\sum</em>{m=1}^{M}T(x;\Theta _{m}) $$</center><br>其中 $ T(x;\Theta _{m}) $ 表示决策树， $ \Theta _{m} $ 为决策树的参数， M 为树的个数。</p>
<h1 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h1><p>提升树算法采用前向分布算法，首先确定初始提升树 $ f _{0}=0 $ ,第m步的模型是： $ f _{m}=f _{m-1}(x)+T(x; \Theta _{m}) $ 。</p>
<p>其中， $ f _{m-1}(x) $ 为当前模型，通过经验风险极小化确定下一颗决策树参数 $ \Theta _{m} $ ：</p>
<p><center>$$ \Theta_{m}^{*}=argmin_{\Theta_{m}}\sum_{i=1}^{N}L(y_{i},f_{m-1}(x_{i})+T(x_{i};\Theta_{m})) $$</center><br>树的线性组合可以很好的拟合训练数据，即使数据中的输入与输出关系复杂也是，所以提升树一般是高功能的学习算法。</p>
<font color="orange">下面讨论针对不同问题的提升树学习算法，主要区别在于损失函数的不同。包括用平方误差损失的回归问题，用指数损失函数的分类问题，以及用一般损失函数的决策问题。</font>

<h2 id="负梯度拟合"><a href="#负梯度拟合" class="headerlink" title="负梯度拟合"></a>负梯度拟合</h2><p>GBDT损失函数拟合方法采用的是 Freidman 提出的损失函数负梯度来拟合本轮损失近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为：</p>
<p><center>r _{ti}=-\left [ \frac{\partial L(y _{i},f(x _{i})))}{\partial f(x _{i})} \right ] _{f(x)=f _{t-1}(x)} </center><br>利用 $ (x _{i}, r _{ti})(i=1,2,…,m) $ ，可以拟合一颗CART回归树，其对应的叶子点区域 $ R _{tj}, j=1,2,…,J $ .其中J为叶子结点个数。</p>
<p>针对每一个叶子结点的样本，我们求出使损失函数最小，也就是拟合叶子结点最好的输出值 $ c _{tj} $ 如下：</p>
<p><center>$$ c _{tj}=\underbrace{argmin} <em>{c}\sum</em>{x _{i}\epsilon R _{tj}}L(y _{i},f _{t-1}(x _{i})+c) $$</center><br>这样就可以得到本轮决策树拟合函数如下：</p>
<p><center>$$ h <em>{t}(x)=\sum</em>{j=1}^{J}c _{tj}I(x\epsilon R _{tj}) $$</center><br>从而本轮最终得到的强学习器的表达式为：</p>
<p><center>$$ f _{t}(x)=f <em>{t-1}(x) + \sum</em>{j=1}^{J}c _{tj}I(x\epsilon R _{tj}) $$</center><br>通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。</p>
<h2 id="二类分类问题"><a href="#二类分类问题" class="headerlink" title="二类分类问题"></a>二类分类问题</h2><p>分类问题，目前有两种实现方式：CART分类树或者CART回归树。大多讲义上都只介绍一种，导致很多同学对GBDT分类问题使用的是二叉分类树还是二叉回归树说法不一。</p>
<p>如果使用二叉分类树，提升树算法只需将AdaBoost算法的基本分类器限制为二类分类树即可，可以说是这时的提升树算法是 AdaBoost 算法的特例。</p>
<p>如果使用二叉回归树，</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/03/AdaBoost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/03/AdaBoost/" itemprop="url">2. AdaBoost</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-03T10:10:23+08:00">
                2019-08-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="提升方法的基本思路"><a href="#提升方法的基本思路" class="headerlink" title="提升方法的基本思路"></a>提升方法的基本思路</h1><p>理论基础：</p>
<p>强可学习(strongly learning)：在概率近似正确(probably approximately correct, PAC)学习框架下，一个概念(一个类)如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么称他为强可学习的。</p>
<p>弱可学习(weakly learnable): 一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测好，那么称这个概念为弱可学习的。</p>
<p>Schapire证明强可学习与弱可学习是等价的。即在PAC学习框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。这样如果发现了“弱学习算法”能否将他提升为“强可学习算法”。<font color="orange">大多数的提升方法都是改变训练数据的概率分布(训练数据的权值分布)，针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</font>下面介绍最典型的AdaBoost算法。</p>
<p>还有两个问题是：</p>
<p>1) 每一轮如何改变训练数据的权值或概率分布？</p>
<p>AdaBoost的做法是提高被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。</p>
<p>2) 如何将弱分类器组合为一个强分类器？</p>
<p>AdaBoost 采取加权多数表决方法，加大分类误差率小的弱分类器的权重，使其在表决中起较大作用，减小分类误差率大的弱分类器的权值，使其表决中起较小的作用。</p>
<h1 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h1><h2 id="AdaBoost二分类算法"><a href="#AdaBoost二分类算法" class="headerlink" title="AdaBoost二分类算法"></a>AdaBoost二分类算法</h2><p>假设给定一个二分类训练数据集：</p>
<center>$$ T=\left{(x_{1},y_{1}),(x_{2},y_{2}),…,(x_{N},y_{N})\right} $$</center>

<p>其中，每个样本点由实例与标记组成，实例 $ x_{i}\epsilon \chi \subseteq \mathbb{R}<em>{n} $ ,标记 $ y</em>{i}\epsilon Y = \left{ -1,+1 \right} $ , X 是实例空间，Y是标记集合。</p>
<p>算法：AdaBoost </p>
<p>输入：训练数据集 $ T= \left { (x_{1}, y_{1}),(x_{2},y_{2}),…,(x_{N},y_{N}) \right } $ ,其中 $ x_{i}\epsilon \chi \subseteq \mathbb{R}<em>{n} $ , $ y</em>{i}\epsilon Y = \left { -1,+1 \right } $ ;弱学习算法。</p>
<p>输出：最终分类器G(x).</p>
<ul>
<li>(1) 初始化训练数据的权值分布：<center>$$ D _{1}=(w _{11},…,w _{1i},…,w _{1N}), w _{1i}=\frac{1}{N}, i=1,2,…,N $$</center></li>
<li>(2) 对 m=1,2,…,M<br>– (a) 使用具有权值分布 $ D _{m} $ 的训练数据集学习，得到基本分类器：<br><center>$$ G _{m}(x):\chi \rightarrow \left { -1,+1 \right }. $$</center><br>– (b) 计算 $ G <em>{m}(x) $ 在训练数据集上的分类误差率：<br><center>$$ e</center></em>{m}=P(G_{m}(x_{i})\neq y {i})=\sum_{i=1}^{N}w_{mi}I(G_{m}(x_{i})\neq y_{i}) $$<br>– (c) 计算 $ G _{m}(x) $ 的系数：<br><center>$$ \alpha _{m}=\frac{1}{2}log\frac{1-e _{m}}{e _{m}} $$</center><br>– (d) 更新训练数据集的权值分布：<br><center>$$ D _{m+1}=(w _{m+1,1},…,w _{m+1,i},…,w _{m+1,N}) $$</center><br><center>$$ w _{m+1,i}=\frac{w _{mi}}{Z _{m}}exp(-\alpha _{m}y _{i}G _{m}(x _{i})),i=1,2,…,N $$</center><br>这里， $ Z _{m} $ 是规范因子<br><center>$$ Z <em>{m}=\sum</em>{i=1}^{N}w _{mi}exp(-\alpha _{m}y _{i}G _{m}(x _{i})) $$</center><br>它使 $ D _{m+1} $ 成为一个概率分布.</li>
<li>(3) 构建基本分类器的线性组合<center>$$ f(x)=\sum_{m=1}^{M}\alpha _{m}G <em>{m}(x) $$</em></center><br>得到最终分类器：<br><center>$$ G(x)=sign(f(x))=sign(\sum{m=1}^{M}\alpha _{m}G _{m}(x)) $$</center>

</li>
</ul>
<h2 id="AdaBoost回归问题算法流程"><a href="#AdaBoost回归问题算法流程" class="headerlink" title="AdaBoost回归问题算法流程"></a>AdaBoost回归问题算法流程</h2><p>来源于：<a href="https://www.cnblogs.com/pinard/p/6133937.html" target="_blank" rel="noopener">集成学习之Adaboost算法原理小结</a></p>
<p>输入：样本集 $ T=\left { (x _{1},y _{1}),(x _{2},y _{2}),…,(x _{m},y _{m}) \right } $ , 弱学习器算法，弱学习器迭代器K.</p>
<p>输出：强学习器 $ f(x) $</p>
<ul>
<li>(1) 初始化样本权重：<center>$$ D _{1}=(w _{11},…,w _{1i},…,w _{1N}), w _{1i}=\frac{1}{N}, i=1,2,…,N $$</center></li>
<li>(2) 对于k=1,2,…,k:<br>– a) 使用具有权重 $ D _{k} $ 的样本集来训练数据，得到弱学习器 $ G _{k}(x) $<br>– b) 计算训练集上的最大误差： $ E _{k}=max|y _{i} - G _{k}(x _{i})|, i=1,2,…,m $<br>– c) 计算每个样本相对误差：<br>— 如果是线性误差，则 $ e _{ki} = \frac{|y _{i}-G _{k}(x _{i})|}{E _{k}} $<br>— 如果是平方误差，则 $ e _{ki} = \frac{(y _{i}-G _{k}(x _{i}))^{2}}{E _{k}^{2}} $<br>— 如果是指数误差，则 $ e _{ki} =  1-exp(\frac{-|y _{i}-G _{k}(x _{i})|}{E _{k}}) $<br>– d) 计算回归误差率： $ e <em>{k}=\sum</em>{i=1}^{m}w _{ki}e _{ki} $<br>– e) 计算弱学习器系数： $ \alpha _{k} = \frac{e _{k}}{1-e _{k}} $<br>– f) 更新样本集的权重分布： $ w _{k+1,i}=\frac{w _{ki}}{Z _{k}}\alpha _{k}^{1-e _{ki}} $ ,其中 $ Z _{k} $ 是规范化因子： $ Z <em>{k}=\sum</em>{i=1}^{m}w _{ki}\alpha _{k}^{1-e _{ki}} $ .</li>
<li>(3) 构建最终强学习器为： $ f(x)=G _{k*}(x) $ .</li>
</ul>
<p>其中， $ G_ {k∗}(x) $ 是所有 $ ln\frac{1}{\alpha _{k}}, k=1,2,…,K $的中位数值对应序号k∗对应的弱学习器。</p>
<h1 id="AdaBoost的解释：前向分布算法"><a href="#AdaBoost的解释：前向分布算法" class="headerlink" title="AdaBoost的解释：前向分布算法"></a>AdaBoost的解释：前向分布算法</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/27/Decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/27/Decision-tree/" itemprop="url">1. Decision-tree</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-27T08:16:21+08:00">
                2019-07-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>目前，工作中接触最多的就是树模型，为了加深对树模型的理解，是时候将树模型总结总结了。</p>
<p>决策树：可以认为是 if-then 规则集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</p>
<p>学习时，利用训练数据，根据损失函数最小化建立决策树模型；预测时，利用决策树模型对新数据分类。</p>
<p>决策树学习通常包括3个步骤：</p>
<font color="red">特征选择、决策树生成和决策树修剪。</font>

<p>决策树主要思想来自于：Quinlan 在1986提出的 ID3 和 1993年提出的C4.5，以及由 Breiman 等人在1984年提出的CART算法。</p>
<h1 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h1><p>决策树由结点(node)和有向边(directed edge)组成。结点有两类：内部结点(internal node, 表示一个特征或属性)和叶结点(leaf node, 表示一个类)。</p>
<p>可以将决策树看做是一个 if-then 规则：即决策树的根节点到叶结点的每一条路径为一条规则：路径上内部结点特征对应着规则的条件，而叶结点的类对应则规则的结论。</p>
<p>也可以将决策树表示给定特征条件下类的条件概率分布：这一条件概率分布定义在特征空间的一个划分上，将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布，即构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。</p>
<p>李航博士将统计学习的三要素为：方法=模型+策略+算法</p>
<ul>
<li><font color="red">模型：</font>决策树。</li>
<li><font color="red">策略：</font>以损失函数(通常是正则化的极大似然函数)为目标函数的最小化。</li>
<li><font color="red">算法：</font>通常采用启发式方法，近似求解最优化问题。</li>
</ul>
<p>决策树学习的过程通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类过程，这一过程对应着对特征空间的划分，也对应着决策树的构建。</p>
<p>决策树学习算法包含<font color="Tomato">特征选择、决策树生成、决策树剪枝过程</font>。其中决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。</p>
<p>常用的算法包含有 ID3、C4.5 与 CART，下面分别介绍三种算法的决策树学习的特征选择、决策树的生成和剪枝过程。</p>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>直观上，如果一个特征具有更好的分类能力，或者按照这个特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就应该选择这个特征，信息增益(information gain)就能够很好地表示这一直观准则。</p>
<p>下面依次介绍与信息增益有关的几个概念：</p>
<font color="red">熵(entropy): </font>用来表示随机变量的不确定性的度量。设X是一个取值有限的离散变量，概率分布为:<br><center>$$\mathit{P(X = x_{i})=p_{i}, i=1,2,…,n}$$</center><br>则随机变量 X 的熵定义为：<br><center>$$\mathit{H(X)=-\sum_{i=1}^{n}p_{i}logp_{i}}$$</center><br>如果 p<sub>i</sub>=0, 则定义 0log0=0. 由上面可以看出，熵只依赖于 <em>X</em> 的分布，而与 <em>X</em> 的取值无关，所以也可以将 X 的熵记为 <em>H(p)</em>, 即：<br><center>$$\mathit{H(p)=-\sum_{i=1}^{n}p_{i}logp_{i}}$$</center><br>熵越大，随机变量的不确定性就越大，从定义可以看出：<br><center>$$\mathit{0\leqslant H(p)\leqslant logn}$$</center><br>当随机变量只有两个取值时，则 <em>X</em> 的分布为：<br><center>$$\mathit{P(X=1)=p,P(X=0)=1-p,0\leqslant p\leqslant 1}$$</center><br>熵为：<br><center>$$\mathit{H(p)=-plog_{2}p-(1-p)log_{2}(1-p)}$$</center><br>其中 <em>H(p)</em> 随概率 <em>p</em> 变化的曲线如图所示:<br><br><img src="/images/2019/picture/Decision-tree/1.png" alt><br><br><font color="red">联合概率分布: </font>设有随机变量 <em>(X, Y)</em>，其联合概率分布为：<br><center>$$\mathit{P(X=x_{i},Y=y_{i})=p_{ij},i=1,2,…,n;j=1,2,…,m}$$</center><br><font color="red">条件熵: </font> <em>H(Y|X)</em>表示在已知随机变量 <em>X</em> 的条件下随机变量 <em>Y</em> 的不确定性. 随机变量 <em>X</em> 给定条件下的随机变量 <em>Y</em> 的条件熵(conditional entropy) <em>H(Y|X)</em>, 定义为 <em>X</em> 给定条件下 <em>Y</em> 的条件概率分布的熵对 X 的数学期望：<br><center>$$\mathit{H(Y|X)=\sum_{i=1}^{n}p_{i}H(Y|X=x_{i})}$$</center><br>其中，$$\mathit{p_{i}=P(X=x_{i}),i=1,2,…,n.}$$<br><br>当熵和条件熵中的概率由 <strong>数据估计(特别是极大似然估计)</strong> 得到时，所对应的的熵分别为经验熵(empirical entropy) 和经验条件熵(empirical conditional entropy).<br><br><font color="red">信息增益: </font>特征<em>A</em>对训练数据集<em>D</em>的信息增益<em>g(D,A)</em>, 定义为集合D的经验熵<em>H(D)</em>与特征<em>A</em>给定条件下<em>D</em>的经验熵<em>H(D/A)</em>之差，即：<br><center>$$\mathit{g(D,A)=H(D)-H(D|A)}$$</center><br>信息增益表示了得知特征<em>X</em>的信息，使得类<em>Y</em>的信息不确定性减少的程度。<br><br>一般地，熵<em>H(Y)</em>与条件熵<em>H(Y|X)</em>之差称为 <strong>互信息(mutual information)</strong>. 决策树学习中的信息增益等价于训练数据集中类与特征的互信息。<br><br><font color="orange"><strong>决策树学习应用信息增益准则选择特征。</strong> 给定训练数据集D和特征A，<code>经验熵 H(D)</code> 表示对数据集D进行分类的不确定性，而 <code>经验条件熵 H(D|A)</code> 表示在特征A给定条件下对数据集D进行分类的不确定性。那么他们的差，即 <strong>信息增益，表示由于特征A而使得数据集D的分类不确定性减少的程度。</strong></font>

<p>设训练数据集为D，|D|表示其样本容量，即样本个数。设有 <em>K</em> 个类 $ C_{k},k=1,2,…,K, |C_{k}| $ 为属于类 $ C_{k} $ 的样本个数，$ \mathit{\sum_{k=1}^{K}|C_{k}|=|D|.} $ 设特征 <em>A</em> 有n个不同的取值 $ \mathit{(a_{1},a_{2},…,a_{n}.)}, $ 根据特征 A 的取值将D划分为 n 个子集  $ \mathit{(D_{1},D_{2},…,D_{n}), |D_{i}|} $ 为 $ D_{i} $ 的样本个数， $ \mathit{\sum_{i=1}^{n}|D_{i}|=|D|}. $ 记子集 $ D_{i} $ 中属于类 $ C_{k} $ 的样本集合为 $ D_{ik}, $ 即 $ D_{ik}=D_{i}\bigcap C_{k} $ , $ |D_{ik}| $ 为 $ D_{ik} $ 的样本个数。于是信息增益的算法为:</p>
<font color="gray"><br>输入：训练数据集D和特征A;<br><br>输出：特征A对训练数据集D的信息增益 g(D,A).<br><br>(1) 计算数据集 D的经验熵 H(D)<br><center>$$ \mathit{H(D)=-\sum_{i=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}} $$</center><br>(2) 计算特征A对数据集D的 <code>经验条件熵H(D|A)</code><br><center>$$\mathit{H(D|A)=\sum_{i=1}^{n}p_{i}H(D|A=a_{i})=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D|A=a_{i})}$$</center><br>即：<br><center>$$\mathit{H(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D_{i})=- \sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_{i}|}log_{2}\frac{|D_{ik}|}{|D_{i}|}}$$</center><br>(3) 计算信息增益<br><center>$$\mathit{g(D,A)=H(D)-H(D,A)}$$</center><br></font>

<h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><font color="red">信息增益比: 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比(information gain ratio)可以对这一问题进行纠正。</font> 

<p>特征A对训练数据集的信息增益比 $ g_{R}(D,A) $ 定义为信息增益 $ g(D,A) $ 与训练数据集D关于特征A的值的的经验熵 $ H_{A}(D) $ 之比：</p>
<center>$$\mathit{g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}}$$</center>

<p>其中：</p>
<center>$$\mathit{H_{A}(D)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}}$$</center>


<h1 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h1><h2 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h2><p>输入：训练数据集D，特征集A, 阈值 $ \varepsilon $ ;</p>
<p>输出：决策树T.</p>
<ul>
<li>(1) 若D中所有实例属于同一类 $ C_{k} $ ,则T为单结点树，并将类 $ C_{k} $ 作为结点的类标记，返回T;</li>
<li>(2) 若 $ A=\Phi $ ,则T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(3) 否则，计算A中各个特征对D的<font color="red">信息增益</font>，选择信息增益最大的特征 $ A_{g} $ ;</li>
<li>(4) 如果 $ A_{g} $ 的信息增益小于阈值 $ \Phi $ , 则置T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(5) 否则，对 $ A_{g} $ 的每一可能值 $ a_{i} $ , 依 $ A_{g}=a_{i} $ 将D分割为若干非空子集 $ D_{i} $ , 将 $ D_{i} $ 中实例最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T;</li>
<li>(6) 对第i个子结点，以 $ D_{i} $ 为训练集，以 $ A-{A_{g} } $ 为特征集，递归地调用步(1)~(5), 得到子树 $ T_{i} $ , 返回 $ T_{i} $ .</li>
</ul>
<h2 id="ID3算法的不足"><a href="#ID3算法的不足" class="headerlink" title="ID3算法的不足"></a>ID3算法的不足</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a></p>
<ul>
<li>1) 无法处理连续特征。</li>
<li>2) 采用信息增益的特征缺点：取值比较多的特征比取值较少的特征的信息增益大。</li>
<li>3) 无法处理缺失值。</li>
<li>4) 没有考虑过拟合问题。</li>
</ul>
<h2 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h2><p>输入：训练数据集D，特征集A, 阈值 $ \varepsilon $ ;</p>
<p>输出：决策树T.</p>
<ul>
<li>(1) 若D中所有实例属于同一类 $ C_{k} $ ,则T为单结点树，并将类 $ C_{k} $ 作为结点的类标记，返回T;</li>
<li>(2) 若 $ A=\Phi $ ,则T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(3) 否则，计算A中各个特征对D的<font color="red">信息增益比</font>，选择信息增益最大的特征 $ A_{g} $ ;</li>
<li>(4) 如果 $ A_{g} $ 的信息增益小于阈值 $ \Phi $ , 则置T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(5) 否则，对 $ A_{g} $ 的每一可能值 $ a_{i} $ , 依 $ A_{g}=a_{i} $ 将D分割为若干非空子集 $ D_{i} $ , 将 $ D_{i} $ 中实例最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T;</li>
<li>(6) 对第i个子结点，以 $ D_{i} $ 为训练集，以 $ A-{A_{g} } $ 为特征集，递归地调用步(1)~(5), 得到子树 $ T_{i} $ , 返回 $ T_{i} $ .</li>
</ul>
<h2 id="C4-5对ID3的改进"><a href="#C4-5对ID3的改进" class="headerlink" title="C4.5对ID3的改进"></a>C4.5对ID3的改进</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a></p>
<ul>
<li>1) 不能处理连续特征的解决方法：C4.5将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为 $ a_{1},a_{2},…,a_{m}, $ 则C4.5取相邻两样本的平均值，一共取得m-1个划分点，其中第i个划分点 $ T_{i} $ 表示为： $ \mathit{T_{i}=\frac{a_{i}+a_{i+1}}{2}}. $ 对于这 m-1 个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的<font color="orange">二元离线分类点</font>。比如取到的增益最大的点为 $ a_{t}, $ 则小于 $ a_{t} $ 的值为类别1，大于 $ a_{t} $ 的值为类别2，这样我们就做到了<font color="orange">连续特征的离散化</font>。要注意的是，与离散属性不同的是，<font color="orange">如果当前节点为连续属性，则该属性后面还可以参与子结点的产生选择过程。</font> <strong>这里应该也采用的是信息增益比吧？</strong></li>
<li>2) 采用信息增益比来解决。</li>
<li>3) 对于确实值的处理，主要解决是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。</li>
<li><ul>
<li>3.1）对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。</li>
</ul>
</li>
<li><ul>
<li>3.2) 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。</li>
</ul>
</li>
<li>4) C4.5引入了正则化系数进行初步的剪枝。</li>
</ul>
<h2 id="C4-5的不足"><a href="#C4-5的不足" class="headerlink" title="C4.5的不足"></a>C4.5的不足</h2><ul>
<li>1) 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝.思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝.后面在下篇讲CART树的时候我们会专门讲决策树的减枝思路，主要采用的是后剪枝加上交叉验证选择最合适的决策树。</li>
<li>2) C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。</li>
<li>3) C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</li>
<li>4) C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。</li>
</ul>
<h1 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h1><p>决策树生成算法递归地产生决策树，容易出现过拟合，其原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构造出过于复杂的决策树。</p>
<p>在决策树学习中将已生成的树进行简化的过程称为剪枝。</p>
<font color="orange">决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)来实现。</font>

<p>设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点有 $ N_{t} $ 个样本点，其中k类的样本点有 $ N_{tk} $ 个，k=1,2,…,K，$ H_{t}(T) $ 为叶结点t上的经验熵，$ \alpha \geq 0 $ 为参数，则决策树学习的损失函数可以定义为：<br>$$ \mathit{C_{\alpha}(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)+\alpha|T|} $$<br>其中经验熵为：<br>$$ \mathit{H_{t}(T)=-\sum_{k}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}} $$<br>令：<br>$$ \mathit{C(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}} $$<br>这时有：<br>$$ \mathit{C_{\alpha}(T)=C(T)+\alpha|T|} $$</p>
<p>上式中，C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数 $ \alpha \geq 0 $ 控制两者之间的影响。较大的 $ \alpha $ 促使选择较简单的模型，较小的 $ \alpha $ 促使选择复杂的模型， $ \alpha =0 $ 意味只考虑与训练数据的拟合程度。</p>
<font color="orange">剪枝，就是当 $ \alpha $ 确定时，选择损失函数最小的模型。上式定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</font>

<p>树的剪枝算法：</p>
<p>输入：生成算法产生的整个树T，参数 $ \alpha $ ;</p>
<p>输出：剪枝后的子树 $ T_{\alpha}. $</p>
<ul>
<li>(1) 计算每个结点的经验熵</li>
<li>(2) 递归地从树的叶结点向上回缩：设一组叶结点回缩到父节点之前与之后的整体树分别为 $ T_{B} $ 与 $ T_{A} $ , 其对应的损失函数分别是 $ C_{\alpha}(T_{B}) $ 与 $ C_{\alpha}(T_{A}) $ , 如果 $ C_{\alpha}(T_{A}) \geq C_{\alpha}(T_{B}) $ 则进行剪枝，即将父节点变为新的叶结点.</li>
<li>(3) 返回(2), 直到不能继续为止，得到损失函数最小的子树 $ T_{\alpha}. $</li>
</ul>
<h1 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h1><p>CART 假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。</p>
<h2 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>为何CART要采用基尼系数：</p>
<p>我们知道，在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。</p>
<p>能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！<font color="orange">CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。</font></p>
<p>具体的，在分类问题中，假设有K个类别，第k个类别的概率为 $ p_{k} $ , 则基尼系数的表达式为：</p>
<center>$$ \mathit{Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}} $$</center><br>如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：<br><center>$$ \mathit{Gini(p)=2p(1-p)} $$</center><br>对于给定的样本D,假设有K个类别, 第k个类别的数量为 $ C_{k} $ ,则样本D的基尼系数表达式为：<br><center>$$ \mathit{Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_{k}|}{|D|})^{2}} $$</center><br>特别的，对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：<br><center>$$ \mathit{Gini(D, A)=\frac{|D_{1}|}{|D|}Gini(D_{1})+\frac{|D_{2}|}{|D|}Gini(D_{2})} $$</center>


<h2 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h2><p>决策树的生成就是递归地构建二叉决策树的过程。特征选择的标准为：</p>
<ul>
<li>对回归树用平方误差最小化准则；</li>
<li>对分类树用基尼指数最小化准则；</li>
</ul>
<h3 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h3><p>回归树的特征处理，连续特征和离线特征同样采用下面的启发式的方法，即选择第j个变量 $ x^{j} $ 和它的取值 s,作为切分变量(splitting variable) 和切分点(splitting point), 并定义两个区域：</p>
<center>$$ \mathit{R_{1}(j,s)=[x|x^{j}\leqslant s], R_{2}(j,s)=[x|x^{j}&gt;s]} $$</center><br>然后寻找最优切分变量j和最优切分点s，具体地，求解：<br><center>$$ \mathit{\min_{j,s}[\min_{c_{1}}\sum_{x_{i}\epsilon R_{1}(j,s)}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\epsilon R_{2}(j,s)}(y_{i}-c_{2})^{2}]} $$</center><br>对于固定输入变量j可以找到最优切分点s.<br><center>$$ c_{1}^{<em>}=avg(y_{i}|x_{i}\epsilon R_{1}(j,s)),c_{2}^{</em>}=avg(y_{i}|x_{i}\epsilon R_{2}(j,s)) $$</center>

<p>输入：训练数据集D</p>
<p>输出：回归树f(x)</p>
<font color="orange">在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</font>

<ul>
<li>(1) 寻找最优切分变量 j 与切分点 s, 求解：<center>$$ \mathit{\min_{j,s}[\min_{c_{1}}\sum_{x_{i}\epsilon R_{1}(j,s)}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\epsilon R_{2}(j,s)}(y_{i}-c_{2})^{2}]} $$</center></li>
<li>(2) 用选定的对(j,s)划分区域并决定相应的输出值：<center>$$ \mathit{R_{1}(j,s)=[x|x^{j}\leqslant s], R_{2}(j,s)=[x|x^{j} &gt; s]} $$</center><br><center>$$ \mathit{c_{m}^{*}=\frac{1}{N_{m}}\sum_{x_{i}\epsilon R_{m}(j,s)}y_{i},x\epsilon R_{m},m=1,2} $$</center></li>
<li>(3) 继续对两个子区域调用步骤(1),(2),直到满足停止条件.</li>
<li>(4) 将输入空间划分为M个区域 $ R_{1},R_{2},…,R_{M}, $ 并生成决策树：<center>$$ \mathit{f(x)=\sum_{m=1}^{M}c_{m}^{*}I(x\epsilon R_{m})} $$</center>

</li>
</ul>
<font color="orange">还有个疑问是：回归树对于离线的特征是如何处理的？1.计算基尼系数，这样如何和连续特征进行比较(排除);2.也采用均方差最小的方式，这也有两种情况:1)按是否大于某一值划分为两部分，2)按是否等于某一值划分为两部分。还有个问题是：在使用时模型如何区别连续特征和离散特征(看 sklearn 库中并没有传入标记离散特征或者连续特征的参数。)</font>

<h3 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h3><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>连续值的处理：</p>
<p>具体的思路如下，比如m个样本的连续特征A有m个，从小到大排列为 $ a_{1},a_{2},…,a_{m}, $ 则CART算法取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$ Ti=\frac{a_{i}+a_{i+1}}{2} $ 。对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为 $ a_{t} $ ,则小于 $ a_{t} $ 的值为类别1，大于 $ a_{t} $ 的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与ID3或者C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。</p>
<p>离散值的处理：</p>
<p>回忆下ID3或者C4.5，如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点。这样导致决策树是多叉树。但是CART分类树使用的方法不同，他采用的是不停的二分，还是这个例子，CART分类树会考虑把A分成{A1}和{A2,A3}, {A2}和{A1,A3}, {A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3},然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。</p>
<p>CART 生成算法：</p>
<p>输入：训练数据集D，停止计算的条件(样本个数小于阈值，或者没有特征，或者基尼系数小于阈值。)；</p>
<p>输出：CART决策树。</p>
<ul>
<li>(1) 设结点的训练数据集为D，计算现有特征对该数据集的基尼指数，此时，对每个特征A，将数据集划分为两部分，根据上面介绍的连续值、离散值计算基尼指数。</li>
<li>(2) 选择基尼指数最小的特征A及其对应的切分点a，依最优特征与最优切分点，从现结点生成两个子结点，将训练集依特征分配到两个子结点中去。</li>
<li>(3) 对两个子结点递归调用(1),(2)直到满足停止条件。</li>
<li>(4) 生成CART树。 </li>
</ul>
<font color="orange">还有个疑问是：分类树如何确定离线特征或者连续特征。</font>

<h3 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a>CART 剪枝</h3><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数，算法基本完全一样，这里我们一起来讲。</p>
<p>由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。但是，有很多的剪枝方法，我们应该这么选择呢？CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。</p>
<p>也就是说，CART树的剪枝算法可以概括为两步，第一步是从原始决策树生成各种剪枝效果的决策树，第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。</p>
<p>首先我们看看剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树T,其损失函数为：</p>
<p><center>$$ \mathit{C_{\alpha }(T_{t})=C(T_{t})+\alpha |T_{t}|} $$</center><br>其中，α为正则化参数，这和线性回归的正则化一样。 $ C(T_{t}) $ 为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。 $ |T_{t}| $ 是子树T的叶子节点的数量。</p>
<p>当α=0时，即没有正则化，原始的生成的CART树即为最优子树。当α=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的α，一定存在使损失函数 $ C_{α}(T) $ 最小的唯一子树。</p>
<p>看过剪枝的损失函数度量后，我们再来看看剪枝的思路，对于位于节点t的任意一颗子树Tt，如果没有剪枝，它的损失是:</p>
<p><center>$$ \mathit{C_{\alpha }(T_{t})=C(T_{t})+\alpha |T_{t}|} $$</center><br>如果将其剪掉，仅仅保留根节点，则损失是:</p>
<p><center>$$ \mathit{C_{\alpha }(T)=C(T)+\alpha} $$</center><br>当α=0或者α很小时， $ C_{α}(T_{t}) &lt; C_{α}(T) $ , 当α增大到一定的程度时: $ C_{α}(T_{t}) = C_{α}(T) $ .</p>
<p>当α继续增大时不等式反向，也就是说，如果满足下式：</p>
<p><center>$$ \mathit{\alpha =\frac{C(T)-C(T_{t})}{|T_{t}|-1}} $$</center><br>$ T_{t} $ 和T有相同的损失函数，但是T节点更少，因此可以对子树 $ T_{t} $ 进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点T。</p>
<p>最后我们看看CART树的交叉验证策略。上面我们讲到，可以计算出每个子树是否剪枝的阈值α，如果我们把所有的节点是否剪枝的值α都计算出来，然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。</p>
<p>好了，有了上面的思路，我们现在来看看CART树的剪枝算法。</p>
<p>输入是CART树建立算法得到的原始决策树T。</p>
<p>输出是最优决策子树 $ T_{α} $ 。</p>
<p>算法过程如下：</p>
<ul>
<li>(1) 初始化 $ \alpha _{min}=\infty  $， 最优子树集合 w={T}。</li>
<li>(2) 从叶子节点开始自下而上计算各内部节点t的训练误差损失函数 $ C_{\alpha}(T_{t}) $ (回归树为均方差，分类树为基尼系数)，叶子结点树 $ |T_{t}| $ ,以及正则化阈值 $ \alpha=min(\frac{C(T)-C(T_{t})}{|T_{t}|-1},\alpha _{min}) $ ,更新 $ \alpha _{min} = \alpha $ .</li>
<li>(3) 得到所有节点的α值的集合M。</li>
<li>(4) 从M中选择最大的值 $ \alpha _{k} $, 自上而下的访问子树t的内部结点，如果 $ \frac{C(T)-C(T _{t})}{|T _{t}|-1} \leq \alpha _{k} $ 时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到 $ \alpha _{k} $ 对应的最优子树 $ T _{k} $ .</li>
<li>(5) 最优子树集合 $ \omega = \omega \cup T _{k}, M=M-\alpha _{k} $ .</li>
<li>(6) 如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合 $ \omega $ .</li>
<li>(7) 采用交叉验证在 $ \omega $ 选择最优子树 $ T _{\alpha} $ .</li>
</ul>
<h1 id="对3种不同的树进行总结"><a href="#对3种不同的树进行总结" class="headerlink" title="对3种不同的树进行总结"></a>对3种不同的树进行总结</h1><table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值处理</th>
<th>剪枝</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类，回归</td>
<td>二叉树</td>
<td>基尼系数，均方差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><ul>
<li>总结《统计学习方法》-李航博士笔记</li>
<li>之前看过刘建平大佬写过的系列博客，在此强烈推荐 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树原理</a>。总结过程中，会参考大佬的文章。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/06/Programming-Hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/06/Programming-Hive/" itemprop="url">Hive 编程指南</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-06T15:31:43+08:00">
                2019-07-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index">
                    <span itemprop="name">big data</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>前言</strong></p>
<p>Hive 是 Hadoop 生态系统中必不可少的一个工具，它提供了一种 SQL 语言，可以查询存储在 Hadoop 分布式文件系统(HDFS)中的数据或其他和 Hadoop 集成的文件系统， 如 MapR-FS，Amazon的S3和像 HBase(Hadoop数据库) 和 Cassandra 这样的数据库中的数据。</p>
<h1 id="第1章-基础知识"><a href="#第1章-基础知识" class="headerlink" title="第1章 基础知识"></a>第1章 基础知识</h1><p>搜索引擎公司、电子商务公司、社交网络等许多组织意识到他们所收集的数据是让他们了解他们的用户，提高业务在市场上的表现以及提高基础架构效率的一个宝贵的资源。</p>
<p>Hive提供了一个被称为 Hive 查询语言(HiveQL 或 HQL)的SQL语言，来查询存储在 Hadoop 集群中的数据。Hive 可以将大多数的查询转换为MapReduce任务(job)，进而在介绍一个令人熟悉的 SQL 抽象的同时，拓宽 Hadoop 的课扩展性。</p>
<p>Hive 的劣势：</p>
<ul>
<li>Hive 不支持记录级别的更新，插入或者删除操作。</li>
<li>因为Hadoop是一个面向批处理的系统，而MapReduce任务(job)的启动过程需要消耗较长时间，所以 Hive 查询延时比较严重。</li>
<li>Hive 不支持事务。因此，Hive 不支持OLTP(联机事务处理)所需的关键功能。</li>
</ul>
<p>如果用户需要对大规模数据使用OLTP功能的话，那么应该选择使用一个NoSQL数据库，例如，和 Hadoop 结合使用的 HBase 及 Cassandra。</p>
<h2 id="Hadoop-和-MapReduce-综述"><a href="#Hadoop-和-MapReduce-综述" class="headerlink" title="Hadoop 和 MapReduce 综述"></a>Hadoop 和 MapReduce 综述</h2><p>参考Tom White 《Hadoop权威指南》一书。</p>
<p>MapReduce</p>
<p>MapReduce是一种计算模型，该模型可以将大型数据处理任务分解为很多单个的，可以在服务器集群中并行执行的任务，这些任务的计算结果可以合并在一起来计算最终的结果。</p>
<p>MapReduce编程模型由谷歌开发，两篇经典的论文为：《MapReduce: 大数据之上的简化数据处理》，以及《Google 文件系统》。这两篇论文启发了道-卡丁开发了 Hadoop。</p>
<p>MapReduce这个术语来自于两个基本的数据转换操作: Map过程和reduce过程。MapReduce 计算框架中的输入和输出的基本数据结构是键-值对。</p>
<p>下图介绍了一种 Word Count程序，左边的每个 Input(输入) 框都表示一个单独的文件，默认情况下，每个文档都会触发一个 Mapper 进程进行处理。而在实际场景中，大文件可能会划分为多个部分，每个部分都会被发送给一个 Mapper 进行处理。同时，也有将多个小文件合并为一个部分供某个 Mapper进行处理。</p>
<p><img src="/images/2019/picture/Programming-Hive/1-1.png" alt></p>
<p>Hadoop 神奇的地方一部分在于后面要进行的Sort和Shuffle过程，Hadoop会按照键来对键-值进行排序，然后Shuffle，将所有具有相同键的键-值对分发到同一个Reducer中。这里有很多方式可以决定哪个Reducer获取哪个范围内的键对应的数据。</p>
<h2 id="Hadoop生态系统中的Hive"><a href="#Hadoop生态系统中的Hive" class="headerlink" title="Hadoop生态系统中的Hive"></a>Hadoop生态系统中的Hive</h2><p>下图显示了Hive的主要模块，以及Hive是如何与Hadoop交互工作的。有好几种方式可以与Hive进行交互，例如命令行。或者图形界面：Karmasphere发布的一个商业产品，Cloudera提供的开源 Hue 项目，以及 Qubole 提供的 “Hive即服务”。</p>
<p><img src="/images/2019/picture/Programming-Hive/1-2.png" alt></p>
<p>Hive 发行版中附带的模块有CLI(命令行)，一个称为 Hive 网页界面(HWI)的简单网页界面，以及可通过 JDBC、ODBC 和一个 Thrift 服务器进行编程访问的几个模块。</p>
<p>所有的命令和查询都会进入到Driver模块，通过该模块对输入进行解析编译，对需求的计算进行优化，然后按照指定的步骤执行(通常是启动多个MapReduce任务(job)来执行)。当需要启动 MapReduce job 时，Hive本身是不会生成 Java MapReduce 算法程序的。相反，Hive 通过一个表示 “job执行计划”的XML 文件驱动执行内置的、原生的 Mapper 和 Reducer 模块。</p>
<p>Hive通过和 JobTracker通信来初始化 MapReduce任务(job)，而不必部署在 JobTracker 所在的管理节点上执行。</p>
<p>Metastore(元数据存储)是一个独立的关系型数据库(通常是一个MySQL实例)，Hive 会在其中保存表模式和其他系统元数据。</p>
<h3 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a>Pig</h3><p>Hive的替代工具中最有名的就是 Pig，假设用户的输入数据具有一个或者多个源，而用户需要进行一组复杂的转换来生成一个或者多个输出数据集。如果使用 Hive, 用户可能会使用嵌套查询，但是在某些时刻会需要重新保存临时表来控制复杂度。</p>
<p>Pig被描述为一种数据流语言，而不是一种查询语言，因此，Pig常用于ETL(数据抽取，数据转换，和数据装载)过程的一部分.</p>
<p>参考 Alan Gates 《Pig 编程指南》</p>
<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>如果用户需要 Hive 无法提供的数据特性(如行级别的更新，快速查询响应时间，以及支持事务)的话，那么该怎么办呢？ HBase 是一个分布式的、可伸缩的数据存储，其支持行级别的数据更新，快速查询和行级事务(但不支持多行事务)。</p>
<p>HBase支持的一个重要特性就是列存储，可以像键-值存储一样来使用HBase。HBase使用HDFS(或其他某种分布式文件系统)来持久化存储数据。Hive 现在已经可以和 HBase 结合使用了。</p>
<h3 id="Cascading、Crunch-及其他"><a href="#Cascading、Crunch-及其他" class="headerlink" title="Cascading、Crunch 及其他"></a>Cascading、Crunch 及其他</h3><h2 id="Java-和-Hive-词频统计算法"><a href="#Java-和-Hive-词频统计算法" class="headerlink" title="Java 和 Hive: 词频统计算法"></a>Java 和 Hive: 词频统计算法</h2><p>统计词频：</p>
<pre><code>CREATE TABLE docs (line STRING);
LOAD DATA INPATH &apos;docs&apos; OVERWRITE INTO TABLE docs;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
    (SELECT explode(split(line, &apos;\s&apos;)) AS word FROM docs) w
GROUP BY word
ORDER BY word;
</code></pre><h1 id="第2章-基础操作"><a href="#第2章-基础操作" class="headerlink" title="第2章 基础操作"></a>第2章 基础操作</h1><h2 id="安装方式"><a href="#安装方式" class="headerlink" title="安装方式"></a>安装方式</h2><p><a href="https://xiaolongc929.github.io/2019/03/12/hadoop-spark-install/" target="_blank" rel="noopener">macOS hadoop-spark安装方式</a></p>
<h3 id="本地模式、伪分布式模式、分布式模式"><a href="#本地模式、伪分布式模式、分布式模式" class="headerlink" title="本地模式、伪分布式模式、分布式模式"></a>本地模式、伪分布式模式、分布式模式</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="chenxiaolong">
            
              <p class="site-author-name" itemprop="name">chenxiaolong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xiaolongc929" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:xiaolongc929@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://plus.google.com/xiaolongc929" target="_blank" title="Google">
                      
                        <i class="fa fa-fw fa-google"></i>Google</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenxiaolong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
