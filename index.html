<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="xiaolongc">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="xiaolongc">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="xiaolongc">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>xiaolongc</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">xiaolongc</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2099/01/01/Contents/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2099/01/01/Contents/" itemprop="url">博客目录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2099-01-01T00:00:00+08:00">
                2099-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/目录/" itemprop="url" rel="index">
                    <span itemprop="name">目录</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="机器学习基石-amp-技巧"><a href="#机器学习基石-amp-技巧" class="headerlink" title="机器学习基石&amp;技巧"></a>机器学习基石&amp;技巧</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/03/16/The-Learning-Problem/" target="_blank" rel="noopener">The-Learning-Problem</a></li>
<li>2 <a href="https://xiaolongc929.github.io/2019/03/17/Learning-to-answer-yes-no/" target="_blank" rel="noopener">Learning-to-answer-yes-no</a></li>
<li>3 <a href="https://xiaolongc929.github.io/2019/03/31/Types-of-learning/" target="_blank" rel="noopener">Types-of-learning</a></li>
<li>4 <a href="https://xiaolongc929.github.io/2019/04/05/Feasibility-of-learning/" target="_blank" rel="noopener">Feasibility-of-learning</a></li>
<li>5 <a href="https://xiaolongc929.github.io/2019/05/26/Training-versus-Testing/" target="_blank" rel="noopener">Training-versus-Testing</a></li>
<li>6 <a href="https://xiaolongc929.github.io/2019/06/23/Theory-of-Generalization/" target="_blank" rel="noopener">Theory-of-Generalization</a></li>
</ul>
<h1 id="机器学习原理"><a href="#机器学习原理" class="headerlink" title="机器学习原理"></a>机器学习原理</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/07/27/Decision-tree/" target="_blank" rel="noopener">Decision-tree</a></li>
<li>2 <a href="https://xiaolongc929.github.io/2019/08/03/AdaBoost/" target="_blank" rel="noopener">AdaBoost</a></li>
<li>3 <a href="https://xiaolongc929.github.io/2019/08/07/GBDT/" target="_blank" rel="noopener">GBDT</a></li>
<li>4 <a href="https://xiaolongc929.github.io/2019/09/01/xgboost/" target="_blank" rel="noopener">xgboost</a></li>
<li>5 <a href="https://xiaolongc929.github.io/2019/09/01/fbprophet/" target="_blank" rel="noopener">fbprophet</a></li>
</ul>
<h1 id="时间序列"><a href="#时间序列" class="headerlink" title="时间序列"></a>时间序列</h1><h1 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h1><ul>
<li><a href="https://xiaolongc929.github.io/2019/03/12/hadoop-spark-install/" target="_blank" rel="noopener">macoc 下 hadoop spark 安装和配置</a></li>
<li><a href="https://xiaolongc929.github.io/2019/07/06/Programming-Hive/" target="_blank" rel="noopener">Hive 编程指南</a></li>
</ul>
<h1 id="运筹规划"><a href="#运筹规划" class="headerlink" title="运筹规划"></a>运筹规划</h1><h1 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a>云计算</h1><h1 id="Java-语言"><a href="#Java-语言" class="headerlink" title="Java 语言"></a>Java 语言</h1><h1 id="Python-语言"><a href="#Python-语言" class="headerlink" title="Python 语言"></a>Python 语言</h1><h1 id="C-语言"><a href="#C-语言" class="headerlink" title="C++ 语言"></a>C++ 语言</h1><h1 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h1><h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/02/17/Hexo-HelloWorld/" target="_blank" rel="noopener">Hexo-HelloWorld</a></li>
<li>2 <a href="https://xiaolongc929.github.io/2019/03/10/hexo-action/" target="_blank" rel="noopener">hexo-实战</a></li>
<li>3 <a href="https://xiaolongc929.github.io/2019/03/12/hadoop-spark-install/" target="_blank" rel="noopener">macoc 下 hadoop spark 安装和配置</a></li>
</ul>
<h1 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/03/08/maven-action/" target="_blank" rel="noopener">maven实战</a></li>
</ul>
<h1 id="阅读感想"><a href="#阅读感想" class="headerlink" title="阅读感想"></a>阅读感想</h1><h1 id="个人随笔"><a href="#个人随笔" class="headerlink" title="个人随笔"></a>个人随笔</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/03/10/Join-the-Internet-industry/" target="_blank" rel="noopener">加入互联网行业</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/05/prepare-of-time-series/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/05/prepare-of-time-series/" itemprop="url">2.时间序列准备</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-05T22:50:51+08:00">
                2019-09-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/time-series/" itemprop="url" rel="index">
                    <span itemprop="name">time series</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="预测者的工具集"><a href="#预测者的工具集" class="headerlink" title="预测者的工具集"></a>预测者的工具集</h1><p>本章我们会讨论一些在不同预测场景中应用的常用工具。我们将介绍一些基准预测方法，并探讨如何通过数据变换与调整简化预测，如何判断预测是否充分利用了现有信息以及如何计算预测区间。</p>
<h2 id="一些简单的预测方法"><a href="#一些简单的预测方法" class="headerlink" title="一些简单的预测方法"></a>一些简单的预测方法</h2><h3 id="均值法"><a href="#均值法" class="headerlink" title="均值法"></a>均值法</h3><h3 id="Naive-方法"><a href="#Naive-方法" class="headerlink" title="Naïve 方法"></a>Naïve 方法</h3><h3 id="季节性-Naive-方法"><a href="#季节性-Naive-方法" class="headerlink" title="季节性 Naïve 方法"></a>季节性 Naïve 方法</h3><h3 id="漂移法"><a href="#漂移法" class="headerlink" title="漂移法"></a>漂移法</h3><p><strong>例子</strong></p>
<h2 id="变换和调整"><a href="#变换和调整" class="headerlink" title="变换和调整"></a>变换和调整</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/rudiments-of-time-series/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/rudiments-of-time-series/" itemprop="url">1. 时间序列入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T16:49:11+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/time-series/" itemprop="url" rel="index">
                    <span itemprop="name">time series</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h1><p>开始时，作者讲了一些小典故，非常有意思:</p>
<blockquote>
<p>数千年来，预测一直吸引着人们。古巴比伦的预言家们可以基于蛆在腐烂的绵羊肝脏中的分布预测未来。在公元前300年，想要预知未来的人们会前往希腊的德尔菲祈求神谕，神谕会在被乙醚蒸汽陶醉的情况下给出她的预言。预言家们在康斯坦丁大帝的统治下经历了一段艰难的时期，康斯坦丁大帝在公元357年颁布了一项法令禁止任何人“去咨询占卜者、数学家或预言家，对预言未来的好奇将被永远禁止。”1763年英国颁布了一项相似的禁令：通过预测骗取钱财将被判作犯罪，其刑罚是判处三个月的狱中的苦役。</p>
</blockquote>
<h2 id="什么是可以被预测的"><a href="#什么是可以被预测的" class="headerlink" title="什么是可以被预测的"></a>什么是可以被预测的</h2><p>事件（或数量）的可预测性取决于以下几个因素：</p>
<ul>
<li>我们对它的影响因素的了解程度;</li>
<li>有多少数据是可用的;</li>
<li>预测是否会影响我们试图预测的事物。</li>
</ul>
<h2 id="预测、计划和目标"><a href="#预测、计划和目标" class="headerlink" title="预测、计划和目标"></a>预测、计划和目标</h2><p>该小节讲述了预测、目标以及计划之间的关系。</p>
<p>预测：它是指在考虑到所有可用信息的前提下，包括历史数据和可以影响预测的任何未来事件的知识，尽可能准确地预言。</p>
<p>目标：它是指你想要发生的事情。目标理应与预测和计划联系在一起，但是这并不经常发生。很多时候，设定目标时没有任何如何去实现这些目标的计划，也没有目标是否切合实际的预测。</p>
<p>计划：它是对预测和目标的回应。计划包括制定使得你的预测符合你的目标的适当行动。</p>
<p>而预测又可以分为短期预测、中期预测、长期预测等，具体预测取决于特定的应用场景。</p>
<h2 id="决定预测什么"><a href="#决定预测什么" class="headerlink" title="决定预测什么"></a>决定预测什么</h2><ul>
<li>决定预测什么：<ul>
<li>用于每条产品线或一组产品？</li>
<li>用于每条产品线或一组产品？</li>
<li>每周数据、月度数据或年度数据？</li>
</ul>
</li>
<li>考虑预测的前景时段：<ul>
<li>需要提前1个月，提前6个月还是提前10年预测？</li>
<li>预测需要多频繁？需要经常进行的预测, 最好是使用自动化系统, 而不是需要仔细人工操作。</li>
</ul>
</li>
</ul>
<p>在制定合适的预测方法之前, 预测者大一部分的时间将用于寻找和整理可用数据。</p>
<h2 id="预测数据和方法"><a href="#预测数据和方法" class="headerlink" title="预测数据和方法"></a>预测数据和方法</h2><p>在大程度上，什么数据是可用的决定了适合什么合适的预测方法。</p>
<ul>
<li><p>定性预测：</p>
<ul>
<li>如果没有可用的数据，或者如果可用的数据与预测无关，那么应该使用定性预测方法。</li>
</ul>
</li>
<li><p>在满足以下两个条件的时候可以使用定量预测 ：</p>
<ul>
<li>关于过去的数字化信息是可以用的；</li>
<li>有理由假设过去的一些模式会在未来延续下去。</li>
</ul>
</li>
</ul>
<p>大多数定量预测问题都使用时间序列数据 (按时间间隔定期收集) 或横截面数据 (在一个时间点收集)。在本书中,我们关注预测未来的数据,并且我们主要专注于时间序列领域。</p>
<p>时间序列数据样例包括:</p>
<ul>
<li>IBM每日股票价格</li>
<li>每月降水量</li>
<li>亚马逊季度销售结果</li>
<li>谷歌年度利润</li>
</ul>
<p>最简单的时间序列预测方法只用了预测变量的信息，而不去寻找影响预测变量的因素。因此，这些方法可以推断趋势部分和季节性部分，但是它们会忽略掉所有其他的信息，如营销计划，竞争对手活动，经济状况变动等。</p>
<p>用于预测的时间序列模型包括分解模型，指数平滑模型，ARIMA 模型。这些模型分别在章节 6，7 和 8 中进行了分析探讨。</p>
<h3 id="预测变量与时间序列预测"><a href="#预测变量与时间序列预测" class="headerlink" title="预测变量与时间序列预测"></a>预测变量与时间序列预测</h3><p>以预测夏季每小时用电需求量为例：</p>
<ul>
<li>解释模型</li>
</ul>
<p>ED = f(当前气温，经济实力，人口当日时间，星期几，误差)</p>
<p>这种关系并不确切–总会有不能由预测变量决定的电力需求变化。右侧的“误差”项表示随机波动和没有被包括在模型中的相关变量的影响。我们将它称之为“解释模型”，因为它帮助解释电力需求变化的原因。</p>
<ul>
<li>时间序列模型</li>
</ul>
<center>$$ ED _{t+1} = f(ED _{t}, ED _{t-1}, ED _{t-2}, ED _{t-3}, …, error) $$</center>

<p>t 表示当前的时间， t+1 表示下一个小时，t−1 表示前一个小时，t−2 表示前两个小时，以此类推。此处，对未来的预测是基于变量的过去值，而不是基于可能影响系统的外部变量。同样，右侧的“误差”项允许随机波动和不包含在模型中的相关变量的影响。</p>
<ul>
<li>混合模型：结合了上述两种模型的特点</li>
</ul>
<center>$$ ED _{t+1} = f(ED _{t}, 当前气温，当日时间，星期几，误差) $$</center>

<p>解释模型非常有用,因为它包含了有关其他变量的信息,而不仅仅是要预测的变量的历史值。但是, 预测者可能选择时间序列模型而不是解释性或混合模型的原因有多种：</p>
<ul>
<li>这一系统可能不被理解,即使被理解,也很难衡量被认为应该管理行为的关系。</li>
<li>其次,有必要知道或预测各种预测因子的未来价值, 以便能够预测有意义的变量, 但是这可能太难了。</li>
<li>第三, 可能主要只是关注预测会发生什么,而不知道为什么会发生。</li>
<li>最后,时间序列模型可以提供比解释或混合模型更准确的预测。</li>
</ul>
<p>在预测中使用的模型取决于可用的资源和数据、模型的准确性以及预测模型的使用方式。</p>
<h2 id="案例学习"><a href="#案例学习" class="headerlink" title="案例学习"></a>案例学习</h2><p>该小节，作者讲了4个案例，有以下几个特点：</p>
<ul>
<li>时间序列数据显示出一系列模式，其中一些带有长期趋势，一些带有季节变动，还有一些两者都不具备。</li>
<li>几乎每种药品的销售量数据都包含长期趋势和季节变动模式。很多药品的销量会因药品补贴政策的变化而突然上升或下降，对很多药品的津贴支出也会因出现低价可替代药品而突然发生变化。（因此，我们需要寻找到一种能够对包含长期趋势和季节变动因素的数据进行预测的方法，使得该方法不仅可以对潜在模式下的突然变动进行稳健预测，同时能够处理大样本的时间序列数据。）</li>
<li>一群专家正在预测汽车转售价格。他们认为统计模型的建立会对他们的生计造成威胁，因而在提供信息方面不合作。尽管如此，该公司还是给我们提供了大量的车辆和汽车转售价格的历史数据。</li>
<li>航空乘客人数会受到学校假期、重大体育赛事、广告活动、竞争行为等影响。一般情况下，澳大利亚不同城市的学校假期不会同时出现，体育赛事有时也会从一个城市转移到另一个城市。在历史数据相应期间发生过一场关键飞行员的罢工运动，其间几个月都没有相关航线运行，一条新的低价航线推出后也惨遭失败。在历史数据期间的末尾，航空公司将一些经济舱座位重新改造为商务舱和头等舱座位，然而几个月后，座位安排重新恢复到原来的状态。</li>
</ul>
<h2 id="预测过程的主要步骤"><a href="#预测过程的主要步骤" class="headerlink" title="预测过程的主要步骤"></a>预测过程的主要步骤</h2><p>一个预测过程通常包括五个基本步骤。</p>
<h3 id="定义问题"><a href="#定义问题" class="headerlink" title="定义问题"></a>定义问题</h3><p>通常这是预测中最困难的步骤。要准确定义这个问题，需要了解怎样运用预测方法，谁需要这个预测，以及预测效果如何满足需要这个预测的机构。预测人员需要花费一定时间与所有参与收集数据、维护数据库和使用这个预测对未来进行规划的人沟通。</p>
<h3 id="收集信息"><a href="#收集信息" class="headerlink" title="收集信息"></a>收集信息</h3><p>一般至少需要两种信息收集方式：(a) 统计数据，(b) 收集数据和进行预测方面专家的积累经验。通常情况下，要获得足够多的历史数据以构建良好的统计模型是很困难的。在这种情况下，可以使用 第4节 中的判断预测方法。有时候，陈旧数据会因相应数据发生结构变化而失效，因而我们一般只选择使用较新的数据。然而，一个好的统计模型可以处理系统中的结构变化，因此不要轻易丢弃好的数据。</p>
<h3 id="初步（探索性）分析"><a href="#初步（探索性）分析" class="headerlink" title="初步（探索性）分析"></a>初步（探索性）分析</h3><p>总是以图形开头，观察思考以下几个问题：</p>
<ul>
<li>有一致的模式吗？</li>
<li>有明显的长期趋势吗？</li>
<li>季节性重要吗？</li>
<li>是否有证据表明商业周期存在？</li>
<li>数据中是否包含需要专业知识解释的异常值？</li>
<li>用于分析的变量之间的相关性有多强？</li>
</ul>
<p>目前已经开发了各种工具来帮助进行这种分析。这些将在章节 2 和 章节6中讨论。、</p>
<h3 id="选择及拟合模型"><a href="#选择及拟合模型" class="headerlink" title="选择及拟合模型"></a>选择及拟合模型</h3><p>最佳模型的选择取决于历史数据的可用性、预测变量与各解释变量之间的相关性，以及预测的使用方式。比较两个或三个潜在的模型是很常见的。每个模型本身都基于人为提出的一组假设(显式和隐式)而建立，通常包含一个或多个参数，这些参数必须使用已知的历史数据进行估计。我们将讨论回归模型(章节 5)、指数平滑方法(章节 7)、Box-Jenkins ARIMA模型(章节 8)、动态回归模型(章节 9)、分层预测(章节 10)，以及其他各种方法，包括计数时间序列、神经网络和章节 11中的向量自回归。</p>
<h3 id="使用及评估预测模型"><a href="#使用及评估预测模型" class="headerlink" title="使用及评估预测模型"></a>使用及评估预测模型</h3><p>一旦模型及其参数确定后，该模型就可以用来进行预测。模型的预测效果只有用于预测的数据得到之后才能得到正确的评价。目前已经开发了许多方法来评估预测的准确性。在使用和进行预测时会存在很多组织结构问题。对其中一些问题的简要讨论将在章节 3 中给出。</p>
<h2 id="统计预测观点"><a href="#统计预测观点" class="headerlink" title="统计预测观点"></a>统计预测观点</h2><p>我们试图预测的东西是未知的(或者我们不能预测它)，所以我们可以把它想象成一个随机变量。</p>
<p>例如，下个月的总销售额可能会有一系列的可能值，直到月底我们把实际销售额加起来，我们才知道这个值会是多少。所以在我们知道下个月的销售情况之前，这是一个随机的变量。</p>
<p>因为下个月时间节点比较近，我们通常清楚销售量大概是多少。如果我们预测明年同一个月的销售情况，可能的销售量变动就会较大。在大多数预测情况下，随着事件的临近，预测对象的相关变动较小。换句话说，预测的越早，预测结果越不稳定。</p>
<p>我们可以想象许多可能性，每一个都为我们将要预测的事物带来不同的影响。下图是1980年到2015年澳大利亚的国际游客总数以及2016至2025年的10个可能的预测值。</p>
<p><img src="/images/2019/picture/time-forecast/1-2.png" alt></p>
<blockquote>
<p>用上面10个预测来加权，应该效果不错。</p>
</blockquote>
<p>我们进行预测的过程实际是寻找随机变量可能取值范围内的中间值。通常情况下，预测会伴随着一个预测区间，给出一个随机变量具有较高概率的范围值。例如，95%的预测区间包含一系列的值，这个预测区间包含实际未来值的概率为95%。</p>
<p>我们通常会给出这些预测区间，而不是图 1.2 中显示的单个可能的预测值。下面的图表显示了未来澳大利亚国际游客的80%和95%的预测区间。蓝线是可能的预测值的平均值，我们称之为“点预测”。</p>
<p><img src="/images/2019/picture/time-forecast/1-3.png" alt></p>
<p>使用下标 t 作为时间。例如， $ y _{t} $ 表示时间 t 对应的观察值。假设将观察到的所有信息表示为 T，目标是预测 $ y _{t} $ .此时，我们将 $ y _{t} | T $ 表示为“给定已知 T 情况下的随机变量 $ y _{t} $ ”。这个随机变量取值的概率测度称为 $ y _{t} | T $ 的 “概率分布”。在预测中，我们称之为“预测分布”。</p>
<p>每当我们谈到“预测”时，通常指的是预测分布的平均值，用 $ \hat{y _{t}} $ 来表示 $ y _{t} $ 的预测值，这意味着 $ y _{t} $ 所有可能取值的均值包含了我们所有已知的信息。有时我们将使用 $\hat{y _{t}} $ 来表示预测分布的‘中位数’(或中间值)。</p>
<p>明确指出我们在进行预测时使用的信息是很必要的。例如，我们使用 $ \hat{y _{t}} $ 表示在已知观测值 $ (y _{1},…,y _{t-1}) $ 的情况下 $ y _{t} $ 的预测值。类似地，我们使用 $ \hat{y _{T+h|T}} $ 表示在已知观测值 $ (y _{1},…,y _{T}) $ 的情况下 $ y _{T+h} $ 的预测值（即考虑时间 T 之前所有观测值的h步预测）.</p>
<h2 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><ul>
<li>Armstrong (2001) 涵盖了整个预测领域的内容，每个章节都由不同的专家撰写。文章部分观点非常武断(我们并不同意其中的观点)，但在处理预测问题上有很多优秀的一般性建议。</li>
<li>Ord, Fildes, and Kourentzes (2017) 是一本预测教材，涵盖了与本书相同的部分，但重点不同，它不关注任何特定的软件环境。这是由三位有着几十年经验的权威预测专家撰写的。</li>
</ul>
<h1 id="时间序列图形"><a href="#时间序列图形" class="headerlink" title="时间序列图形"></a>时间序列图形</h1><p>对于任何数据分析工作而言，其首要任务是数据可视化。图示化数据可以清晰地展现数据的特征，包括数据的形态、异常值、随时间变化情况以及变量间的相互关系。我们在预测时应尽可能地将图中显示的特征纳入考虑。正如数据类型决定使用什么预测方法一样，数据类型也决定了使用什么图形来展示数据。</p>
<p>在画图之前，首先我们应该在R中设置我们的时间序列数据。</p>
<h2 id="ts-对象"><a href="#ts-对象" class="headerlink" title="ts 对象"></a>ts 对象</h2><p>R语言中的一种记录时序的对象。</p>
<pre><code>y &lt;- ts(c(123,39,78,52,110), start=2012)
</code></pre><h2 id="时间图"><a href="#时间图" class="headerlink" title="时间图"></a>时间图</h2><p>对于时间序列数据而言，我们从最简单的时间图开始。时间图是用将观测值与观测时间点作图，散点之间用直线连接。例如图2.1表示在澳大利亚两个最大的城市之间，Ansett航空公司的每周客流量。</p>
<p><img src="/images/2019/picture/time-forecast/2-1.png" alt></p>
<p>该时间图直观地展现出数据具有的一些特征：</p>
<ul>
<li>由于1989年当地的工业纠纷，当年的客流量为0.</li>
<li>在1992年中，由于一部分经济舱被商务舱取代，导致客流量大幅减少。</li>
<li>1991年下半年客流量大幅上升。</li>
<li>由于假日效应，在每年年初，客流量都会有一定幅度的下降。</li>
<li>这是序列存在长期波动，在1987年向上波动，在1988年向下波动，而在1990年和1991年又再次向上波动。</li>
<li>在某些时期存在缺失值。</li>
</ul>
<p><img src="/images/2019/picture/time-forecast/2-2.png" alt></p>
<ul>
<li>显然，图示的时间序列具有明显增长的趋势。</li>
<li>同时，在上升趋势中伴随着明显的季节性。</li>
<li>在每年年底，由于政府补贴计划，使得降糖药品更便宜，所以人们倾向于在年底囤积药物，从而导致年初的销售额大幅下降。</li>
<li>因此，当我们对降糖药物的销量进行预测时，需同时考虑其趋势和季节性因素。</li>
</ul>
<h2 id="时间序列形态"><a href="#时间序列形态" class="headerlink" title="时间序列形态"></a>时间序列形态</h2><p>我们通常使用例如“趋势”、“季节性”等词语描述时间序列。在深入研究时间序列形态时，应该更精确的定义这些词语。</p>
<h3 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h3><p>当一个时间序列数据长期增长或者长期下降时，表示该序列有 <code>趋势</code> 。在某些场合，趋势代表着“转换方向”。例如从增长的趋势转换为下降趋势。在图 2.2 中，明显存在一个增长的趋势。</p>
<h3 id="季节性"><a href="#季节性" class="headerlink" title="季节性"></a>季节性</h3><p>当时间序列中的数据受到季节性因素（例如一年的时间或者一周的时间）的影响时，表示该序列具有 <code>季节性</code> 。季节性总是一个已知并且固定的频率。由于抗糖尿病药物的成本在年底时会有变化，导致上述抗糖尿药物的月销售额存在季节性。</p>
<h3 id="周期性"><a href="#周期性" class="headerlink" title="周期性"></a>周期性</h3><p>当时间序列数据存在不固定频率的上升和下降时，表示该序列有 <code>周期性</code> 。这些波动经常由经济活动引起，并且与“商业周期”有关。周期波动通常至少持续两年。</p>
<blockquote>
<p>许多初学者都不能很好的区分季节性和周期，然而这两个概念是完全不同的。当数据的波动是无规律时，表示序列存在周期性；如果波动的频率不变并且与固定长度的时间段有关，表示序列存在季节性。一般而言，周期的长度较长，并且周期的波动幅度也更大。</p>
</blockquote>
<p>许多时间序列同时包含趋势、季节性以及周期性。当我们选择预测方法时，首先应该分析时间序列数据所具备的特征，然后再选择合适的预测方法抓取特征。</p>
<p>以下四个示例分别是上述三个特征的不同组合。</p>
<p><img src="/images/2019/picture/time-forecast/2-3.png" alt></p>
<ul>
<li>美国新建房屋销售额（左上）表现出强烈的年度季节性，以及周期为6~10年的周期性。但是数据并没有表现出明显的趋势。</li>
<li>美国国债价格（右上）表示1981年美国国债在芝加哥市场连续100个交易日的价格。可以看出，该序列并没有季节性，但是有明显下降的趋势。假如我们拥有该序列更多的观测数据，我们可以看到这个下降的趋势是一个长期循环的一部分。但是现在我们只有连续100天的数据，它表现出下降的趋势。</li>
<li>澳大利亚月度电力产值数据（左下）明显表现出向上增长的趋势，以及强季节性。但是并不存在周期性。</li>
<li>Google收盘股价格（右下）的价格波动没有趋势，季节性和周期性。随机波动没有良好的形态特性，不能很好地预测。</li>
</ul>
<h2 id="季节图"><a href="#季节图" class="headerlink" title="季节图"></a>季节图</h2><p>季节图和时间序列图很相似，不同之处是季节图是针对观察数据的“季节性”绘制的。下面的例子是降糖药物的销售情况。</p>
<p><img src="/images/2019/picture/time-forecast/2-4.png" alt></p>
<p>在早些年，数据形态基本相同，但是近些年数据存在相互堆叠的情况。季节图可以很清晰的显示季节形态，这对识别数据形态是否发生变化非常有效。</p>
<p>在本例中，在每年一月份降糖药物的销量都会大幅下降。实际上，患者会在12月下旬大量购买降糖药物，但是这部分销量会在一两周后才向政府登记。从上图还可以看出，2008年3月销量大幅下降（其他年份2月份至3月份的销量增加）。2008年6月份销量较少可能是由于销量数据收集不完整导致。</p>
<p>季节图中可以将直角坐标转换为极坐标。</p>
<p><img src="/images/2019/picture/time-forecast/2-5.png" alt></p>
<h2 id="子系列季节图"><a href="#子系列季节图" class="headerlink" title="子系列季节图"></a>子系列季节图</h2><p><img src="/images/2019/picture/time-forecast/2-6.png" alt></p>
<p>图中的水平线表示每月的平均销量。子系列季节图可以清晰的描绘出数据的潜在季节性形态，并且显示了季节性随时间的变化情况。这类图可以很好地查看各时期内数据的变化情况。在本例中，子系列季节图并没有明显地体现数据特性，但是这是观察季节性变化最有用的方式。</p>
<h2 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h2><p>在此之前，我们所讨论的内容都是单个时间序列的可视化。此外，多个时间序列的可视化也是非常有用的。 图 2.7 分别展示了两个时间序列：2014年澳大利亚维多利亚州每半小时的用电量（以千兆瓦为单位）和温度（以摄氏度为单位）</p>
<p><img src="/images/2019/picture/time-forecast/2-7.png" alt></p>
<p><img src="/images/2019/picture/time-forecast/2-8.png" alt></p>
<p>这个散点图可以很好的帮助我们理解变量之间的相互关系。从图中我们可以看出，当温度很高时，人们会大量的使用空调进行降温，进而导致用电量随之增加；当温度很低时，人们会使用空调取暖，也会使得用电量一定程度上增加。</p>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><p>我们经常会用 相关系数 衡量两个两个变量之间的相关强度。假如已知两个变量 x 和 y ，那么它们之间的相关系数为：</p>
<center> $$ r = \frac{\sum (x _{t} - \bar{x})(y _{t} - \bar{y}))}{\sqrt{\sum (x _{t}- \bar{x}) ^{2}}\sqrt{\sum (y _{t}-\bar{y}) ^{2}}} $$ </center>

<p>r 的值始终在-1到1之间。当两个变量完全负相关时，r 值为-1；当两个变量完全正相关时，r 为1.图 2.9 分别展示了不同相关强度的例子。</p>
<p><img src="/images/2019/picture/time-forecast/2-9.png" alt></p>
<p>需要注意的是，相关系数仅仅衡量了变量之间的线性关系，并且有时会导致错误的结果。例如，在图 2.10中，所有例子的相关系数均为0.82，但是它们有着完全不同的形态。 这表明，在分析变量之间关系时，不仅要看相关系数值，而且要关注生成的图形。</p>
<p><img src="/images/2019/picture/time-forecast/2-10.png" alt></p>
<p>在图 2.8中，用电量和温度之间的相关系数仅为0.2798，但并不代表用电量和温度之间存在很强的非线性关系。</p>
<h3 id="散点图矩阵"><a href="#散点图矩阵" class="headerlink" title="散点图矩阵"></a>散点图矩阵</h3><p>当所分析的数据有多个变量时，将每个变量与其他变量进行比较也很有意义。如图2.11所示，表示澳大利亚新南威尔士五个地区的季度游客人数。</p>
<p><img src="/images/2019/picture/time-forecast/2-11.png" alt></p>
<p>如图 2.12所示，我们可以绘制出它们的散点图矩阵。</p>
<p><img src="/images/2019/picture/time-forecast/2-12.png" alt></p>
<p>我们可以通过散点图矩阵快速查看所有变量之间的相关关系。在本例中，由图中第二列数据可知，新南威尔士州北部海岸游客与新南威尔士南部海岸游客之间存在强烈的正关系，而新南威尔士州北部海岸的游客与新南威尔士内陆游客之间几乎没有相关关系。同时，我们可以通过散点图矩阵检测到异常值。由于2000年悉尼奥运会，新南威尔士大都会地区存在异常大的客流量。</p>
<h2 id="滞后图"><a href="#滞后图" class="headerlink" title="滞后图"></a>滞后图</h2><p>图 2.13是澳大利亚每季度啤酒产量的散点图，横轴表示时间序列的滞后阶数。各图分别显示了不同 k 值下 $ y _{t} $ 和 $ y _{t-k} $ 的散点图。</p>
<p><img src="/images/2019/picture/time-forecast/2-13.png" alt></p>
<p>2-13 澳大利亚每季度啤酒产量不同滞后阶数散点图</p>
<p>图中不同颜色代表不同季节，每条线都按时间顺序连接。从图中可以看出，滞后四阶和滞后八阶有正相关关系，说明数据具有很强的季节性。二阶滞后图和六阶滞后图显示，第四季度的峰值对应第二季度的最低点。</p>
<h2 id="自相关"><a href="#自相关" class="headerlink" title="自相关"></a>自相关</h2><p>正如相关系数可以衡量两个变量之间的线性相关关系一样，自相关系数可以测量时间序列 滞后值 之间的线性关系。</p>
<p>以下几个不同的自相关系数，对应于滞后图中的不同情况。例如， $ r _{1} $ 衡量 $ y _{t} $ 和 $ y _{t-1} $ 之间的关系， $ r _{2} $ 衡量 $ y _{t} $ 和 $ y _{t-2} $ 之间的关系.</p>
<p>其中， $ r _{k} $ 的定义如下：</p>
<center> $$ r <em>{k}=\frac{\sum</em>{t=k+1}^{T}(y _{t}-\bar{y})(y <em>{t-k}-\bar{y})}{\sum</em>{t=1}^{T}(y _{t}-\bar{y})^{2}} $$ </center>

<p>其中，T 是时间序列的长度。</p>
<p>澳大利亚啤酒产量数据的前九个自相关系数如下表所示。（略）</p>
<p>各值分别对应于图 2.13 中的九个散点图。通过绘制自相关系数图可以描绘 自相关函数 或者是ACF。因此也被称为相关图。</p>
<p><img src="/images/2019/picture/time-forecast/2-14.png" alt></p>
<p>在该图中：</p>
<ul>
<li>$ r _{4} $ 值最大。这是由于数据的季节性形态：顶峰往往出现在第四季度，谷底往往出现在第二季度。</li>
<li>$ r _{2} $ 值最小。这是由于谷底往往在高峰之后的两个季度出现。</li>
<li>蓝色虚线之内的区域自相关性可近似看做0。这将会在下节详细阐述。</li>
</ul>
<h3 id="ACF-图中的趋势性和季节性"><a href="#ACF-图中的趋势性和季节性" class="headerlink" title="ACF 图中的趋势性和季节性"></a>ACF 图中的趋势性和季节性</h3><p>当数据具有趋势性时，短期滞后的自相关值较大，因为观测点附近的值波动不会很大。时间序列的ACF一般是正值，随着滞后阶数的增加而缓慢下降。</p>
<p>当数据具有季节性时，自相关值在滞后阶数与季节周期相同时（或者在季节周期的倍数）较大。</p>
<p>当数据同时具有趋势和季节性时，我们会观察到组合效应。如图 2.15 是澳大利亚用电量，该序列同时具有趋势和季节性。它的ACF值如图 2.16 所示。</p>
<p><img src="/images/2019/picture/time-forecast/2-15.png" alt></p>
<p><img src="/images/2019/picture/time-forecast/2-16.png" alt></p>
<p>自相关系数值随着滞后阶数增加而缓慢降低，是因为原时间序列中具有趋势变化，而图中的“圆齿状”形状是来源于原时间序列中的季节性变化。</p>
<h2 id="白噪声"><a href="#白噪声" class="headerlink" title="白噪声"></a>白噪声</h2><p>“白噪声”是一个对所有时间其自相关系数为零的随机过程。 图 2.17是一个白噪声的例子。</p>
<p><img src="/images/2019/picture/time-forecast/2-17.png" alt></p>
<p>白噪声函数的自相关函数如下图所示：</p>
<p><img src="/images/2019/picture/time-forecast/2-18.png" alt></p>
<p>对于白噪声而言，我们期望它的自相关值接近0。但是由于随机扰动的存在，自相关值并不会精确地等于0。对于一个长度为 T 的白噪声序列而言，我们期望在0.95的置信度下，它的自相关值处于 $ \pm 2/\sqrt{T} $ 之间。我们可以很容易的画出ACF的边界值（图中蓝色虚线）。如果一个序列中有较多的自相关值处于边界之外，那么该序列很可能不是白噪声序列。</p>
<p>在上例中，序列长度 T = 50，边界为 $ \pm 2/\sqrt{50} = \pm 0.28 $ 。所有的自相关值均落在边界之内，证明序列是白噪声.</p>
<h2 id="拓展阅读-1"><a href="#拓展阅读-1" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><ul>
<li>Cleveland (1993) 是关于数据分析可视化原理的经典著作。虽然已有20多年历史，但它的思想永不过时。</li>
<li>Unwin (2015) 是一本关于使用R进行图形数据分析的前沿著作。它并没有着重介绍如何绘制时间序列图形，更多的是关于使用图形进行数据分析。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/fbprophet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/fbprophet/" itemprop="url">5. fbprophet原理及实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T16:13:28+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="fbprophet简介"><a href="#fbprophet简介" class="headerlink" title="fbprophet简介"></a>fbprophet简介</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/xgboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/xgboost/" itemprop="url">4. xgboost原理及实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T16:11:58+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="xgboost-简介"><a href="#xgboost-简介" class="headerlink" title="xgboost 简介"></a>xgboost 简介</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/07/GBDT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/GBDT/" itemprop="url">3. GBDT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-07T22:38:27+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="提升树模型"><a href="#提升树模型" class="headerlink" title="提升树模型"></a>提升树模型</h1><p>提升方法实际采用加法模型(即基函数的线性组合)与前向分布算法。以决策树为基函数的提升方法称为提升树(boosting tree)。<font color="orange">对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。还有一种实现版本是分类问题、回归问题都使用的是CART回归树。具体情况，还需要查看其对应的实现方法。TODO：sklearn里面的GBDT实现方式。</font> 提升树模型可以表示为决策树的加法模型：</p>
<p><center>$$ f <em>{M}(x)=\sum</em>{m=1}^{M}T(x;\Theta _{m}) $$</center><br>其中 $ T(x;\Theta _{m}) $ 表示决策树， $ \Theta _{m} $ 为决策树的参数， M 为树的个数。</p>
<h1 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h1><p>提升树算法采用前向分布算法，首先确定初始提升树 $ f _{0}=0 $ ,第m步的模型是： $ f _{m}=f _{m-1}(x)+T(x; \Theta _{m}) $ 。</p>
<p>其中， $ f _{m-1}(x) $ 为当前模型，通过经验风险极小化确定下一颗决策树参数 $ \Theta _{m} $ ：</p>
<p><center>$$ \Theta <em>{m}^{*}=argmin</em>{\Theta <em>{m}}\sum</em>{i=1}^{N}L(y _{i},f _{m-1}(x _{i})+T(x _{i};\Theta _{m})) $$</center><br>树的线性组合可以很好的拟合训练数据，即使数据中的输入与输出关系复杂也是，所以提升树一般是高功能的学习算法。</p>
<font color="orange">下面讨论针对不同问题的提升树学习算法，主要区别在于损失函数的不同。包括用平方误差损失的回归问题，用指数损失函数的分类问题，以及用一般损失函数的决策问题。</font>

<h2 id="负梯度拟合"><a href="#负梯度拟合" class="headerlink" title="负梯度拟合"></a>负梯度拟合</h2><p>GBDT损失函数拟合方法采用的是 Freidman 提出的损失函数负梯度来拟合本轮损失近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为：</p>
<p><center>r _{ti}=-\left [ \frac{\partial L(y _{i},f(x _{i})))}{\partial f(x _{i})} \right ] _{f(x)=f _{t-1}(x)} </center><br>利用 $ (x _{i}, r _{ti})(i=1,2,…,m) $ ，可以拟合一颗CART回归树，其对应的叶子点区域 $ R _{tj}, j=1,2,…,J $ .其中J为叶子结点个数。</p>
<p>针对每一个叶子结点的样本，我们求出使损失函数最小，也就是拟合叶子结点最好的输出值 $ c _{tj} $ 如下：</p>
<p><center>$$ c _{tj}=\underbrace{argmin} <em>{c}\sum</em>{x _{i}\epsilon R _{tj}}L(y _{i},f _{t-1}(x _{i})+c) $$</center><br>这样就可以得到本轮决策树拟合函数如下：</p>
<p><center>$$ h <em>{t}(x)=\sum</em>{j=1}^{J}c _{tj}I(x\epsilon R _{tj}) $$</center><br>从而本轮最终得到的强学习器的表达式为：</p>
<p><center>$$ f _{t}(x)=f <em>{t-1}(x) + \sum</em>{j=1}^{J}c _{tj}I(x\epsilon R _{tj}) $$</center><br>通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。</p>
<h2 id="二类分类问题"><a href="#二类分类问题" class="headerlink" title="二类分类问题"></a>二类分类问题</h2><p>分类问题，目前有两种实现方式：CART分类树或者CART回归树。大多讲义上都只介绍一种，导致很多同学对GBDT分类问题使用的是二叉分类树还是二叉回归树说法不一。</p>
<p>如果使用二叉分类树，提升树算法只需将AdaBoost算法的基本分类器限制为二类分类树即可，可以说是这时的提升树算法是 AdaBoost 算法的特例。</p>
<p>如果使用二叉回归树，</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/03/AdaBoost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/03/AdaBoost/" itemprop="url">2. AdaBoost</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-03T10:10:23+08:00">
                2019-08-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="提升方法的基本思路"><a href="#提升方法的基本思路" class="headerlink" title="提升方法的基本思路"></a>提升方法的基本思路</h1><p>理论基础：</p>
<p>强可学习(strongly learning)：在概率近似正确(probably approximately correct, PAC)学习框架下，一个概念(一个类)如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么称他为强可学习的。</p>
<p>弱可学习(weakly learnable): 一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测好，那么称这个概念为弱可学习的。</p>
<p>Schapire证明强可学习与弱可学习是等价的。即在PAC学习框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。这样如果发现了“弱学习算法”能否将他提升为“强可学习算法”。<font color="orange">大多数的提升方法都是改变训练数据的概率分布(训练数据的权值分布)，针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</font>下面介绍最典型的AdaBoost算法。</p>
<p>还有两个问题是：</p>
<p>1) 每一轮如何改变训练数据的权值或概率分布？</p>
<p>AdaBoost的做法是提高被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。</p>
<p>2) 如何将弱分类器组合为一个强分类器？</p>
<p>AdaBoost 采取加权多数表决方法，加大分类误差率小的弱分类器的权重，使其在表决中起较大作用，减小分类误差率大的弱分类器的权值，使其表决中起较小的作用。</p>
<h1 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h1><h2 id="AdaBoost二分类算法"><a href="#AdaBoost二分类算法" class="headerlink" title="AdaBoost二分类算法"></a>AdaBoost二分类算法</h2><p>假设给定一个二分类训练数据集：</p>
<center>$$ \mathit{T=\left { (x _{1},y _{1}),(x _{2},y _{2}),…,(x _{N},y _{N}) \right }} $$</center>

<p>其中，每个样本点由实例与标记组成，实例 $ x _{i}\epsilon \chi \subseteq \mathbb{R} _{n} $ ,标记 $ y _{i}\epsilon Y = \left { -1,+1 \right } $ , X 是实例空间，Y是标记集合。</p>
<p>算法：AdaBoost </p>
<p>输入：训练数据集 $ T= \left { (x _{1}, y _{1}),(x _{2},y _{2}),…,(x _{N},y _{N}) \right } $ ,其中 $ x _{i}\epsilon \chi \subseteq \mathbb{R} _{n} $ , $ y _{i}\epsilon Y = \left { -1,+1 \right } $ ;弱学习算法。</p>
<p>输出：最终分类器G(x).</p>
<ul>
<li>(1) 初始化训练数据的权值分布：<center>$$ D _{1}=(w _{11},…,w _{1i},…,w _{1N}), w _{1i}=\frac{1}{N}, i=1,2,…,N $$</center></li>
<li>(2) 对 m=1,2,…,M<br>– (a) 使用具有权值分布 $ D _{m} $ 的训练数据集学习，得到基本分类器：<br><center>$$ G _{m}(x):\chi \rightarrow \left { -1,+1 \right }. $$</center><br>– (b) 计算 $ G _{m}(x) $ 在训练数据集上的分类误差率：<br><center>$$ e _{m}=P(G _{m}(x _{i})\neq y <em>{i})=\sum</em>{i=1}^{N}w _{mi}I(G _{m}(x _{i})\neq y _{i}) $$</center><br>– (c) 计算 $ G _{m}(x) $ 的系数：<br><center>$$ \alpha _{m}=\frac{1}{2}log\frac{1-e _{m}}{e _{m}} $$</center><br>– (d) 更新训练数据集的权值分布：<br><center>$$ D _{m+1}=(w _{m+1,1},…,w _{m+1,i},…,w _{m+1,N}) $$</center><br><center>$$ w _{m+1,i}=\frac{w _{mi}}{Z _{m}}exp(-\alpha _{m}y _{i}G _{m}(x _{i})),i=1,2,…,N $$</center><br>这里， $ Z _{m} $ 是规范因子<br><center>$$ Z <em>{m}=\sum</em>{i=1}^{N}w _{mi}exp(-\alpha _{m}y _{i}G _{m}(x _{i})) $$</center><br>它使 $ D _{m+1} $ 成为一个概率分布.</li>
<li>(3) 构建基本分类器的线性组合<center>$$ f(x)=\sum_{m=1}^{M}\alpha _{m}G <em>{m}(x) $$</em></center><br>得到最终分类器：<br><center>$$ G(x)=sign(f(x))=sign(\sum{m=1}^{M}\alpha _{m}G _{m}(x)) $$</center>

</li>
</ul>
<h2 id="AdaBoost回归问题算法流程"><a href="#AdaBoost回归问题算法流程" class="headerlink" title="AdaBoost回归问题算法流程"></a>AdaBoost回归问题算法流程</h2><p>来源于：<a href="https://www.cnblogs.com/pinard/p/6133937.html" target="_blank" rel="noopener">集成学习之Adaboost算法原理小结</a></p>
<p>输入：样本集 $ T=\left { (x _{1},y _{1}),(x _{2},y _{2}),…,(x _{m},y _{m}) \right } $ , 弱学习器算法，弱学习器迭代器K.</p>
<p>输出：强学习器 $ f(x) $</p>
<ul>
<li>(1) 初始化样本权重：<center>$$ D _{1}=(w _{11},…,w _{1i},…,w _{1N}), w _{1i}=\frac{1}{N}, i=1,2,…,N $$</center></li>
<li>(2) 对于k=1,2,…,k:<br>– a) 使用具有权重 $ D _{k} $ 的样本集来训练数据，得到弱学习器 $ G _{k}(x) $<br>– b) 计算训练集上的最大误差： $ E _{k}=max|y _{i} - G _{k}(x _{i})|, i=1,2,…,m $<br>– c) 计算每个样本相对误差：<br>— 如果是线性误差，则 $ e _{ki} = \frac{|y _{i}-G _{k}(x _{i})|}{E _{k}} $<br>— 如果是平方误差，则 $ e _{ki} = \frac{(y _{i}-G _{k}(x _{i}))^{2}}{E _{k}^{2}} $<br>— 如果是指数误差，则 $ e _{ki} =  1-exp(\frac{-|y _{i}-G _{k}(x _{i})|}{E _{k}}) $<br>– d) 计算回归误差率： $ e <em>{k}=\sum</em>{i=1}^{m}w _{ki}e _{ki} $<br>– e) 计算弱学习器系数： $ \alpha _{k} = \frac{e _{k}}{1-e _{k}} $<br>– f) 更新样本集的权重分布： $ w _{k+1,i}=\frac{w _{ki}}{Z _{k}}\alpha _{k}^{1-e _{ki}} $ ,其中 $ Z _{k} $ 是规范化因子： $ Z <em>{k}=\sum</em>{i=1}^{m}w _{ki}\alpha _{k}^{1-e _{ki}} $ .</li>
<li>(3) 构建最终强学习器为： $ f(x)=G _{k*}(x) $ .</li>
</ul>
<p>其中， $ G_ {k∗}(x) $ 是所有 $ ln\frac{1}{\alpha _{k}}, k=1,2,…,K $的中位数值对应序号k∗对应的弱学习器。</p>
<h1 id="AdaBoost的解释：前向分布算法"><a href="#AdaBoost的解释：前向分布算法" class="headerlink" title="AdaBoost的解释：前向分布算法"></a>AdaBoost的解释：前向分布算法</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/27/Decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/27/Decision-tree/" itemprop="url">1. Decision-tree</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-27T08:16:21+08:00">
                2019-07-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>目前，工作中接触最多的就是树模型，为了加深对树模型的理解，是时候将树模型总结总结了。</p>
<p>决策树：可以认为是 if-then 规则集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</p>
<p>学习时，利用训练数据，根据损失函数最小化建立决策树模型；预测时，利用决策树模型对新数据分类。</p>
<p>决策树学习通常包括3个步骤：</p>
<font color="red">特征选择、决策树生成和决策树修剪。</font>

<p>决策树主要思想来自于：Quinlan 在1986提出的 ID3 和 1993年提出的C4.5，以及由 Breiman 等人在1984年提出的CART算法。</p>
<h1 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h1><p>决策树由结点(node)和有向边(directed edge)组成。结点有两类：内部结点(internal node, 表示一个特征或属性)和叶结点(leaf node, 表示一个类)。</p>
<p>可以将决策树看做是一个 if-then 规则：即决策树的根节点到叶结点的每一条路径为一条规则：路径上内部结点特征对应着规则的条件，而叶结点的类对应则规则的结论。</p>
<p>也可以将决策树表示给定特征条件下类的条件概率分布：这一条件概率分布定义在特征空间的一个划分上，将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布，即构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。</p>
<p>李航博士将统计学习的三要素为：方法=模型+策略+算法</p>
<ul>
<li><font color="red">模型：</font>决策树。</li>
<li><font color="red">策略：</font>以损失函数(通常是正则化的极大似然函数)为目标函数的最小化。</li>
<li><font color="red">算法：</font>通常采用启发式方法，近似求解最优化问题。</li>
</ul>
<p>决策树学习的过程通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类过程，这一过程对应着对特征空间的划分，也对应着决策树的构建。</p>
<p>决策树学习算法包含<font color="Tomato">特征选择、决策树生成、决策树剪枝过程</font>。其中决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。</p>
<p>常用的算法包含有 ID3、C4.5 与 CART，下面分别介绍三种算法的决策树学习的特征选择、决策树的生成和剪枝过程。</p>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>直观上，如果一个特征具有更好的分类能力，或者按照这个特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就应该选择这个特征，信息增益(information gain)就能够很好地表示这一直观准则。</p>
<p>下面依次介绍与信息增益有关的几个概念：</p>
<font color="red">熵(entropy): </font>用来表示随机变量的不确定性的度量。设X是一个取值有限的离散变量，概率分布为:<br><center>$$\mathit{P(X = x_{i})=p_{i}, i=1,2,…,n}$$</center><br>则随机变量 X 的熵定义为：<br><center>$$\mathit{H(X)=-\sum_{i=1}^{n}p_{i}logp_{i}}$$</center><br>如果 p<sub>i</sub>=0, 则定义 0log0=0. 由上面可以看出，熵只依赖于 <em>X</em> 的分布，而与 <em>X</em> 的取值无关，所以也可以将 X 的熵记为 <em>H(p)</em>, 即：<br><center>$$\mathit{H(p)=-\sum_{i=1}^{n}p_{i}logp_{i}}$$</center><br>熵越大，随机变量的不确定性就越大，从定义可以看出：<br><center>$$\mathit{0\leqslant H(p)\leqslant logn}$$</center><br>当随机变量只有两个取值时，则 <em>X</em> 的分布为：<br><center>$$\mathit{P(X=1)=p,P(X=0)=1-p,0\leqslant p\leqslant 1}$$</center><br>熵为：<br><center>$$\mathit{H(p)=-plog_{2}p-(1-p)log_{2}(1-p)}$$</center><br>其中 <em>H(p)</em> 随概率 <em>p</em> 变化的曲线如图所示:<br><br><img src="/images/2019/picture/Decision-tree/1.png" alt><br><br><font color="red">联合概率分布: </font>设有随机变量 <em>(X, Y)</em>，其联合概率分布为：<br><center>$$\mathit{P(X=x_{i},Y=y_{i})=p_{ij},i=1,2,…,n;j=1,2,…,m}$$</center><br><font color="red">条件熵: </font> <em>H(Y|X)</em>表示在已知随机变量 <em>X</em> 的条件下随机变量 <em>Y</em> 的不确定性. 随机变量 <em>X</em> 给定条件下的随机变量 <em>Y</em> 的条件熵(conditional entropy) <em>H(Y|X)</em>, 定义为 <em>X</em> 给定条件下 <em>Y</em> 的条件概率分布的熵对 X 的数学期望：<br><center>$$\mathit{H(Y|X)=\sum_{i=1}^{n}p_{i}H(Y|X=x_{i})}$$</center><br>其中，$$\mathit{p_{i}=P(X=x_{i}),i=1,2,…,n.}$$<br><br>当熵和条件熵中的概率由 <strong>数据估计(特别是极大似然估计)</strong> 得到时，所对应的的熵分别为经验熵(empirical entropy) 和经验条件熵(empirical conditional entropy).<br><br><font color="red">信息增益: </font>特征<em>A</em>对训练数据集<em>D</em>的信息增益<em>g(D,A)</em>, 定义为集合D的经验熵<em>H(D)</em>与特征<em>A</em>给定条件下<em>D</em>的经验熵<em>H(D/A)</em>之差，即：<br><center>$$\mathit{g(D,A)=H(D)-H(D|A)}$$</center><br>信息增益表示了得知特征<em>X</em>的信息，使得类<em>Y</em>的信息不确定性减少的程度。<br><br>一般地，熵<em>H(Y)</em>与条件熵<em>H(Y|X)</em>之差称为 <strong>互信息(mutual information)</strong>. 决策树学习中的信息增益等价于训练数据集中类与特征的互信息。<br><br><font color="orange"><strong>决策树学习应用信息增益准则选择特征。</strong> 给定训练数据集D和特征A，<code>经验熵 H(D)</code> 表示对数据集D进行分类的不确定性，而 <code>经验条件熵 H(D|A)</code> 表示在特征A给定条件下对数据集D进行分类的不确定性。那么他们的差，即 <strong>信息增益，表示由于特征A而使得数据集D的分类不确定性减少的程度。</strong></font>

<p>设训练数据集为D，|D|表示其样本容量，即样本个数。设有 <em>K</em> 个类 $ C_{k},k=1,2,…,K, |C_{k}| $ 为属于类 $ C_{k} $ 的样本个数，$ \mathit{\sum_{k=1}^{K}|C_{k}|=|D|.} $ 设特征 <em>A</em> 有n个不同的取值 $ \mathit{(a_{1},a_{2},…,a_{n}.)}, $ 根据特征 A 的取值将D划分为 n 个子集  $ \mathit{(D_{1},D_{2},…,D_{n}), |D_{i}|} $ 为 $ D_{i} $ 的样本个数， $ \mathit{\sum_{i=1}^{n}|D_{i}|=|D|}. $ 记子集 $ D_{i} $ 中属于类 $ C_{k} $ 的样本集合为 $ D_{ik}, $ 即 $ D_{ik}=D_{i}\bigcap C_{k} $ , $ |D_{ik}| $ 为 $ D_{ik} $ 的样本个数。于是信息增益的算法为:</p>
<font color="gray"><br>输入：训练数据集D和特征A;<br><br>输出：特征A对训练数据集D的信息增益 g(D,A).<br><br>(1) 计算数据集 D的经验熵 H(D)<br><center>$$ \mathit{H(D)=-\sum_{i=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}} $$</center><br>(2) 计算特征A对数据集D的 <code>经验条件熵H(D|A)</code><br><center>$$\mathit{H(D|A)=\sum_{i=1}^{n}p_{i}H(D|A=a_{i})=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D|A=a_{i})}$$</center><br>即：<br><center>$$\mathit{H(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D_{i})=- \sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_{i}|}log_{2}\frac{|D_{ik}|}{|D_{i}|}}$$</center><br>(3) 计算信息增益<br><center>$$\mathit{g(D,A)=H(D)-H(D,A)}$$</center><br></font>

<h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><font color="red">信息增益比: 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比(information gain ratio)可以对这一问题进行纠正。</font> 

<p>特征A对训练数据集的信息增益比 $ g_{R}(D,A) $ 定义为信息增益 $ g(D,A) $ 与训练数据集D关于特征A的值的的经验熵 $ H_{A}(D) $ 之比：</p>
<center>$$\mathit{g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}}$$</center>

<p>其中：</p>
<center>$$\mathit{H_{A}(D)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}}$$</center>


<h1 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h1><h2 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h2><p>输入：训练数据集D，特征集A, 阈值 $ \varepsilon $ ;</p>
<p>输出：决策树T.</p>
<ul>
<li>(1) 若D中所有实例属于同一类 $ C_{k} $ ,则T为单结点树，并将类 $ C_{k} $ 作为结点的类标记，返回T;</li>
<li>(2) 若 $ A=\Phi $ ,则T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(3) 否则，计算A中各个特征对D的<font color="red">信息增益</font>，选择信息增益最大的特征 $ A_{g} $ ;</li>
<li>(4) 如果 $ A_{g} $ 的信息增益小于阈值 $ \Phi $ , 则置T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(5) 否则，对 $ A_{g} $ 的每一可能值 $ a_{i} $ , 依 $ A_{g}=a_{i} $ 将D分割为若干非空子集 $ D_{i} $ , 将 $ D_{i} $ 中实例最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T;</li>
<li>(6) 对第i个子结点，以 $ D_{i} $ 为训练集，以 $ A-{A_{g} } $ 为特征集，递归地调用步(1)~(5), 得到子树 $ T_{i} $ , 返回 $ T_{i} $ .</li>
</ul>
<h2 id="ID3算法的不足"><a href="#ID3算法的不足" class="headerlink" title="ID3算法的不足"></a>ID3算法的不足</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a></p>
<ul>
<li>1) 无法处理连续特征。</li>
<li>2) 采用信息增益的特征缺点：取值比较多的特征比取值较少的特征的信息增益大。</li>
<li>3) 无法处理缺失值。</li>
<li>4) 没有考虑过拟合问题。</li>
</ul>
<h2 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h2><p>输入：训练数据集D，特征集A, 阈值 $ \varepsilon $ ;</p>
<p>输出：决策树T.</p>
<ul>
<li>(1) 若D中所有实例属于同一类 $ C_{k} $ ,则T为单结点树，并将类 $ C_{k} $ 作为结点的类标记，返回T;</li>
<li>(2) 若 $ A=\Phi $ ,则T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(3) 否则，计算A中各个特征对D的<font color="red">信息增益比</font>，选择信息增益最大的特征 $ A_{g} $ ;</li>
<li>(4) 如果 $ A_{g} $ 的信息增益小于阈值 $ \Phi $ , 则置T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(5) 否则，对 $ A_{g} $ 的每一可能值 $ a_{i} $ , 依 $ A_{g}=a_{i} $ 将D分割为若干非空子集 $ D_{i} $ , 将 $ D_{i} $ 中实例最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T;</li>
<li>(6) 对第i个子结点，以 $ D_{i} $ 为训练集，以 $ A-{A_{g} } $ 为特征集，递归地调用步(1)~(5), 得到子树 $ T_{i} $ , 返回 $ T_{i} $ .</li>
</ul>
<h2 id="C4-5对ID3的改进"><a href="#C4-5对ID3的改进" class="headerlink" title="C4.5对ID3的改进"></a>C4.5对ID3的改进</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a></p>
<ul>
<li>1) 不能处理连续特征的解决方法：C4.5将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为 $ a_{1},a_{2},…,a_{m}, $ 则C4.5取相邻两样本的平均值，一共取得m-1个划分点，其中第i个划分点 $ T_{i} $ 表示为： $ \mathit{T_{i}=\frac{a_{i}+a_{i+1}}{2}}. $ 对于这 m-1 个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的<font color="orange">二元离线分类点</font>。比如取到的增益最大的点为 $ a_{t}, $ 则小于 $ a_{t} $ 的值为类别1，大于 $ a_{t} $ 的值为类别2，这样我们就做到了<font color="orange">连续特征的离散化</font>。要注意的是，与离散属性不同的是，<font color="orange">如果当前节点为连续属性，则该属性后面还可以参与子结点的产生选择过程。</font> <strong>这里应该也采用的是信息增益比吧？</strong></li>
<li>2) 采用信息增益比来解决。</li>
<li>3) 对于确实值的处理，主要解决是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。</li>
<li><ul>
<li>3.1）对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。</li>
</ul>
</li>
<li><ul>
<li>3.2) 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。</li>
</ul>
</li>
<li>4) C4.5引入了正则化系数进行初步的剪枝。</li>
</ul>
<h2 id="C4-5的不足"><a href="#C4-5的不足" class="headerlink" title="C4.5的不足"></a>C4.5的不足</h2><ul>
<li>1) 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝.思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝.后面在下篇讲CART树的时候我们会专门讲决策树的减枝思路，主要采用的是后剪枝加上交叉验证选择最合适的决策树。</li>
<li>2) C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。</li>
<li>3) C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</li>
<li>4) C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。</li>
</ul>
<h1 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h1><p>决策树生成算法递归地产生决策树，容易出现过拟合，其原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构造出过于复杂的决策树。</p>
<p>在决策树学习中将已生成的树进行简化的过程称为剪枝。</p>
<font color="orange">决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)来实现。</font>

<p>设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点有 $ N_{t} $ 个样本点，其中k类的样本点有 $ N_{tk} $ 个，k=1,2,…,K，$ H_{t}(T) $ 为叶结点t上的经验熵，$ \alpha \geq 0 $ 为参数，则决策树学习的损失函数可以定义为：<br>$$ \mathit{C_{\alpha}(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)+\alpha|T|} $$<br>其中经验熵为：<br>$$ \mathit{H_{t}(T)=-\sum_{k}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}} $$<br>令：<br>$$ \mathit{C(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}} $$<br>这时有：<br>$$ \mathit{C_{\alpha}(T)=C(T)+\alpha|T|} $$</p>
<p>上式中，C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数 $ \alpha \geq 0 $ 控制两者之间的影响。较大的 $ \alpha $ 促使选择较简单的模型，较小的 $ \alpha $ 促使选择复杂的模型， $ \alpha =0 $ 意味只考虑与训练数据的拟合程度。</p>
<font color="orange">剪枝，就是当 $ \alpha $ 确定时，选择损失函数最小的模型。上式定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</font>

<p>树的剪枝算法：</p>
<p>输入：生成算法产生的整个树T，参数 $ \alpha $ ;</p>
<p>输出：剪枝后的子树 $ T_{\alpha}. $</p>
<ul>
<li>(1) 计算每个结点的经验熵</li>
<li>(2) 递归地从树的叶结点向上回缩：设一组叶结点回缩到父节点之前与之后的整体树分别为 $ T_{B} $ 与 $ T_{A} $ , 其对应的损失函数分别是 $ C_{\alpha}(T_{B}) $ 与 $ C_{\alpha}(T_{A}) $ , 如果 $ C_{\alpha}(T_{A}) \geq C_{\alpha}(T_{B}) $ 则进行剪枝，即将父节点变为新的叶结点.</li>
<li>(3) 返回(2), 直到不能继续为止，得到损失函数最小的子树 $ T_{\alpha}. $</li>
</ul>
<h1 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h1><p>CART 假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。</p>
<h2 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>为何CART要采用基尼系数：</p>
<p>我们知道，在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。</p>
<p>能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！<font color="orange">CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。</font></p>
<p>具体的，在分类问题中，假设有K个类别，第k个类别的概率为 $ p_{k} $ , 则基尼系数的表达式为：</p>
<center>$$ \mathit{Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}} $$</center><br>如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：<br><center>$$ \mathit{Gini(p)=2p(1-p)} $$</center><br>对于给定的样本D,假设有K个类别, 第k个类别的数量为 $ C_{k} $ ,则样本D的基尼系数表达式为：<br><center>$$ \mathit{Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_{k}|}{|D|})^{2}} $$</center><br>特别的，对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：<br><center>$$ \mathit{Gini(D, A)=\frac{|D_{1}|}{|D|}Gini(D_{1})+\frac{|D_{2}|}{|D|}Gini(D_{2})} $$</center>


<h2 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h2><p>决策树的生成就是递归地构建二叉决策树的过程。特征选择的标准为：</p>
<ul>
<li>对回归树用平方误差最小化准则；</li>
<li>对分类树用基尼指数最小化准则；</li>
</ul>
<h3 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h3><p>回归树的特征处理，连续特征和离线特征同样采用下面的启发式的方法，即选择第j个变量 $ x^{j} $ 和它的取值 s,作为切分变量(splitting variable) 和切分点(splitting point), 并定义两个区域：</p>
<center>$$ \mathit{R_{1}(j,s)=[x|x^{j}\leqslant s], R_{2}(j,s)=[x|x^{j}&gt;s]} $$</center><br>然后寻找最优切分变量j和最优切分点s，具体地，求解：<br><center>$$ \mathit{\min_{j,s}[\min_{c_{1}}\sum_{x_{i}\epsilon R_{1}(j,s)}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\epsilon R_{2}(j,s)}(y_{i}-c_{2})^{2}]} $$</center><br>对于固定输入变量j可以找到最优切分点s.<br><center>$$ \mathit{c_{1}^{<em>}=ave(y_{i}| x_{i} \epsilon R_{1}(j,s)),c_{2}^{</em>}=ave(y_{i}| x_{i} \epsilon R_{2}(j,s))} $$</center>

<p>输入：训练数据集D</p>
<p>输出：回归树f(x)</p>
<font color="orange">在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</font>

<ul>
<li>(1) 寻找最优切分变量 j 与切分点 s, 求解：<center>$$ \mathit{\min_{j,s}[\min_{c_{1}}\sum_{x_{i}\epsilon R_{1}(j,s)}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\epsilon R_{2}(j,s)}(y_{i}-c_{2})^{2}]} $$</center></li>
<li>(2) 用选定的对(j,s)划分区域并决定相应的输出值：<center>$$ \mathit{R _{1}(j,s)=[x|x^{j} \leqslant s], R <em>{2}(j,s)=[x|x^{j} &gt; s]} $$</em></center><br><center>$$ \mathit{c{m}^{*}=\frac{1}{N_{m}}\sum_{x_{i}\epsilon R_{m}(j,s)}y_{i},x\epsilon R_{m},m=1,2} $$</center></li>
<li>(3) 继续对两个子区域调用步骤(1),(2),直到满足停止条件.</li>
<li>(4) 将输入空间划分为M个区域 $ R_{1},R_{2},…,R_{M}, $ 并生成决策树：<center>$$ \mathit{f(x)=\sum_{m=1}^{M}c_{m}^{*}I(x\epsilon R_{m})} $$</center>

</li>
</ul>
<font color="orange">还有个疑问是：回归树对于离线的特征是如何处理的？1.计算基尼系数，这样如何和连续特征进行比较(排除);2.也采用均方差最小的方式，这也有两种情况:1)按是否大于某一值划分为两部分，2)按是否等于某一值划分为两部分。还有个问题是：在使用时模型如何区别连续特征和离散特征(看 sklearn 库中并没有传入标记离散特征或者连续特征的参数。)</font>

<h3 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h3><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>连续值的处理：</p>
<p>具体的思路如下，比如m个样本的连续特征A有m个，从小到大排列为 $ a_{1},a_{2},…,a_{m}, $$ 则CART算法取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$ Ti=\frac{a_{i}+a_{i+1}}{2} $ 。对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为 $ a_{t} $ ,则小于 $ a_{t} $ 的值为类别1，大于 $ a_{t} $ 的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与ID3或者C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。</p>
<p>离散值的处理：</p>
<p>回忆下ID3或者C4.5，如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点。这样导致决策树是多叉树。但是CART分类树使用的方法不同，他采用的是不停的二分，还是这个例子，CART分类树会考虑把A分成{A1}和{A2,A3}, {A2}和{A1,A3}, {A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3},然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。</p>
<p>CART 生成算法：</p>
<p>输入：训练数据集D，停止计算的条件(样本个数小于阈值，或者没有特征，或者基尼系数小于阈值。)；</p>
<p>输出：CART决策树。</p>
<ul>
<li>(1) 设结点的训练数据集为D，计算现有特征对该数据集的基尼指数，此时，对每个特征A，将数据集划分为两部分，根据上面介绍的连续值、离散值计算基尼指数。</li>
<li>(2) 选择基尼指数最小的特征A及其对应的切分点a，依最优特征与最优切分点，从现结点生成两个子结点，将训练集依特征分配到两个子结点中去。</li>
<li>(3) 对两个子结点递归调用(1),(2)直到满足停止条件。</li>
<li>(4) 生成CART树。 </li>
</ul>
<font color="orange">还有个疑问是：分类树如何确定离线特征或者连续特征。</font>

<h3 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a>CART 剪枝</h3><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数，算法基本完全一样，这里我们一起来讲。</p>
<p>由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。但是，有很多的剪枝方法，我们应该这么选择呢？CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。</p>
<p>也就是说，CART树的剪枝算法可以概括为两步，第一步是从原始决策树生成各种剪枝效果的决策树，第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。</p>
<p>首先我们看看剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树T,其损失函数为：</p>
<p><center>$$ \mathit{C_{\alpha }(T_{t})=C(T_{t})+\alpha |T_{t}|} $$</center><br>其中，α为正则化参数，这和线性回归的正则化一样。 $ C(T_{t}) $ 为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。 $ |T_{t}| $ 是子树T的叶子节点的数量。</p>
<p>当α=0时，即没有正则化，原始的生成的CART树即为最优子树。当α=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的α，一定存在使损失函数 $ C_{α}(T) $ 最小的唯一子树。</p>
<p>看过剪枝的损失函数度量后，我们再来看看剪枝的思路，对于位于节点t的任意一颗子树Tt，如果没有剪枝，它的损失是:</p>
<p><center>$$ \mathit{C_{\alpha }(T_{t})=C(T_{t})+\alpha |T_{t}|} $$</center><br>如果将其剪掉，仅仅保留根节点，则损失是:</p>
<p><center>$$ \mathit{C_{\alpha }(T)=C(T)+\alpha} $$</center><br>当α=0或者α很小时， $ C_{α}(T_{t}) &lt; C_{α}(T) $ , 当α增大到一定的程度时: $ C_{α}(T_{t}) = C_{α}(T) $ .</p>
<p>当α继续增大时不等式反向，也就是说，如果满足下式：</p>
<p><center>$$ \mathit{\alpha =\frac{C(T)-C(T_{t})}{|T_{t}|-1}} $$</center><br>$ T_{t} $ 和T有相同的损失函数，但是T节点更少，因此可以对子树 $ T_{t} $ 进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点T。</p>
<p>最后我们看看CART树的交叉验证策略。上面我们讲到，可以计算出每个子树是否剪枝的阈值α，如果我们把所有的节点是否剪枝的值α都计算出来，然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。</p>
<p>好了，有了上面的思路，我们现在来看看CART树的剪枝算法。</p>
<p>输入是CART树建立算法得到的原始决策树T。</p>
<p>输出是最优决策子树 $ T_{α} $ 。</p>
<p>算法过程如下：</p>
<ul>
<li>(1) 初始化 $ \alpha _{min}=\infty  $， 最优子树集合 w={T}。</li>
<li>(2) 从叶子节点开始自下而上计算各内部节点t的训练误差损失函数 $ C_{\alpha}(T_{t}) $ (回归树为均方差，分类树为基尼系数)，叶子结点树 $ |T_{t}| $ ,以及正则化阈值 $ \alpha=min(\frac{C(T)-C(T_{t})}{|T_{t}|-1},\alpha _{min}) $ ,更新 $ \alpha _{min} = \alpha $ .</li>
<li>(3) 得到所有节点的α值的集合M。</li>
<li>(4) 从M中选择最大的值 $ \alpha _{k} $, 自上而下的访问子树t的内部结点，如果 $ \frac{C(T)-C(T _{t})}{|T _{t}|-1} \leq \alpha _{k} $ 时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到 $ \alpha _{k} $ 对应的最优子树 $ T _{k} $ .</li>
<li>(5) 最优子树集合 $ \omega = \omega \cup T _{k}, M=M-\alpha _{k} $ .</li>
<li>(6) 如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合 $ \omega $ .</li>
<li>(7) 采用交叉验证在 $ \omega $ 选择最优子树 $ T _{\alpha} $ .</li>
</ul>
<h1 id="对3种不同的树进行总结"><a href="#对3种不同的树进行总结" class="headerlink" title="对3种不同的树进行总结"></a>对3种不同的树进行总结</h1><table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值处理</th>
<th>剪枝</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类，回归</td>
<td>二叉树</td>
<td>基尼系数，均方差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><ul>
<li>总结《统计学习方法》-李航博士笔记</li>
<li>之前看过刘建平大佬写过的系列博客，在此强烈推荐 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树原理</a>。总结过程中，会参考大佬的文章。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/06/Programming-Hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/06/Programming-Hive/" itemprop="url">Hive 编程指南</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-06T15:31:43+08:00">
                2019-07-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index">
                    <span itemprop="name">big data</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>前言</strong></p>
<p>Hive 是 Hadoop 生态系统中必不可少的一个工具，它提供了一种 SQL 语言，可以查询存储在 Hadoop 分布式文件系统(HDFS)中的数据或其他和 Hadoop 集成的文件系统， 如 MapR-FS，Amazon的S3和像 HBase(Hadoop数据库) 和 Cassandra 这样的数据库中的数据。</p>
<h1 id="第1章-基础知识"><a href="#第1章-基础知识" class="headerlink" title="第1章 基础知识"></a>第1章 基础知识</h1><p>搜索引擎公司、电子商务公司、社交网络等许多组织意识到他们所收集的数据是让他们了解他们的用户，提高业务在市场上的表现以及提高基础架构效率的一个宝贵的资源。</p>
<p>Hive提供了一个被称为 Hive 查询语言(HiveQL 或 HQL)的SQL语言，来查询存储在 Hadoop 集群中的数据。Hive 可以将大多数的查询转换为MapReduce任务(job)，进而在介绍一个令人熟悉的 SQL 抽象的同时，拓宽 Hadoop 的课扩展性。</p>
<p>Hive 的劣势：</p>
<ul>
<li>Hive 不支持记录级别的更新，插入或者删除操作。</li>
<li>因为Hadoop是一个面向批处理的系统，而MapReduce任务(job)的启动过程需要消耗较长时间，所以 Hive 查询延时比较严重。</li>
<li>Hive 不支持事务。因此，Hive 不支持OLTP(联机事务处理)所需的关键功能。</li>
</ul>
<p>如果用户需要对大规模数据使用OLTP功能的话，那么应该选择使用一个NoSQL数据库，例如，和 Hadoop 结合使用的 HBase 及 Cassandra。</p>
<h2 id="Hadoop-和-MapReduce-综述"><a href="#Hadoop-和-MapReduce-综述" class="headerlink" title="Hadoop 和 MapReduce 综述"></a>Hadoop 和 MapReduce 综述</h2><p>参考Tom White 《Hadoop权威指南》一书。</p>
<p>MapReduce</p>
<p>MapReduce是一种计算模型，该模型可以将大型数据处理任务分解为很多单个的，可以在服务器集群中并行执行的任务，这些任务的计算结果可以合并在一起来计算最终的结果。</p>
<p>MapReduce编程模型由谷歌开发，两篇经典的论文为：《MapReduce: 大数据之上的简化数据处理》，以及《Google 文件系统》。这两篇论文启发了道-卡丁开发了 Hadoop。</p>
<p>MapReduce这个术语来自于两个基本的数据转换操作: Map过程和reduce过程。MapReduce 计算框架中的输入和输出的基本数据结构是键-值对。</p>
<p>下图介绍了一种 Word Count程序，左边的每个 Input(输入) 框都表示一个单独的文件，默认情况下，每个文档都会触发一个 Mapper 进程进行处理。而在实际场景中，大文件可能会划分为多个部分，每个部分都会被发送给一个 Mapper 进行处理。同时，也有将多个小文件合并为一个部分供某个 Mapper进行处理。</p>
<p><img src="/images/2019/picture/Programming-Hive/1-1.png" alt></p>
<p>Hadoop 神奇的地方一部分在于后面要进行的Sort和Shuffle过程，Hadoop会按照键来对键-值进行排序，然后Shuffle，将所有具有相同键的键-值对分发到同一个Reducer中。这里有很多方式可以决定哪个Reducer获取哪个范围内的键对应的数据。</p>
<h2 id="Hadoop生态系统中的Hive"><a href="#Hadoop生态系统中的Hive" class="headerlink" title="Hadoop生态系统中的Hive"></a>Hadoop生态系统中的Hive</h2><p>下图显示了Hive的主要模块，以及Hive是如何与Hadoop交互工作的。有好几种方式可以与Hive进行交互，例如命令行。或者图形界面：Karmasphere发布的一个商业产品，Cloudera提供的开源 Hue 项目，以及 Qubole 提供的 “Hive即服务”。</p>
<p><img src="/images/2019/picture/Programming-Hive/1-2.png" alt></p>
<p>Hive 发行版中附带的模块有CLI(命令行)，一个称为 Hive 网页界面(HWI)的简单网页界面，以及可通过 JDBC、ODBC 和一个 Thrift 服务器进行编程访问的几个模块。</p>
<p>所有的命令和查询都会进入到Driver模块，通过该模块对输入进行解析编译，对需求的计算进行优化，然后按照指定的步骤执行(通常是启动多个MapReduce任务(job)来执行)。当需要启动 MapReduce job 时，Hive本身是不会生成 Java MapReduce 算法程序的。相反，Hive 通过一个表示 “job执行计划”的XML 文件驱动执行内置的、原生的 Mapper 和 Reducer 模块。</p>
<p>Hive通过和 JobTracker通信来初始化 MapReduce任务(job)，而不必部署在 JobTracker 所在的管理节点上执行。</p>
<p>Metastore(元数据存储)是一个独立的关系型数据库(通常是一个MySQL实例)，Hive 会在其中保存表模式和其他系统元数据。</p>
<h3 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a>Pig</h3><p>Hive的替代工具中最有名的就是 Pig，假设用户的输入数据具有一个或者多个源，而用户需要进行一组复杂的转换来生成一个或者多个输出数据集。如果使用 Hive, 用户可能会使用嵌套查询，但是在某些时刻会需要重新保存临时表来控制复杂度。</p>
<p>Pig被描述为一种数据流语言，而不是一种查询语言，因此，Pig常用于ETL(数据抽取，数据转换，和数据装载)过程的一部分.</p>
<p>参考 Alan Gates 《Pig 编程指南》</p>
<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>如果用户需要 Hive 无法提供的数据特性(如行级别的更新，快速查询响应时间，以及支持事务)的话，那么该怎么办呢？ HBase 是一个分布式的、可伸缩的数据存储，其支持行级别的数据更新，快速查询和行级事务(但不支持多行事务)。</p>
<p>HBase支持的一个重要特性就是列存储，可以像键-值存储一样来使用HBase。HBase使用HDFS(或其他某种分布式文件系统)来持久化存储数据。Hive 现在已经可以和 HBase 结合使用了。</p>
<h3 id="Cascading、Crunch-及其他"><a href="#Cascading、Crunch-及其他" class="headerlink" title="Cascading、Crunch 及其他"></a>Cascading、Crunch 及其他</h3><h2 id="Java-和-Hive-词频统计算法"><a href="#Java-和-Hive-词频统计算法" class="headerlink" title="Java 和 Hive: 词频统计算法"></a>Java 和 Hive: 词频统计算法</h2><p>统计词频：</p>
<pre><code>CREATE TABLE docs (line STRING);
LOAD DATA INPATH &apos;docs&apos; OVERWRITE INTO TABLE docs;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
    (SELECT explode(split(line, &apos;\s&apos;)) AS word FROM docs) w
GROUP BY word
ORDER BY word;
</code></pre><h1 id="第2章-基础操作"><a href="#第2章-基础操作" class="headerlink" title="第2章 基础操作"></a>第2章 基础操作</h1><h2 id="安装方式"><a href="#安装方式" class="headerlink" title="安装方式"></a>安装方式</h2><p><a href="https://xiaolongc929.github.io/2019/03/12/hadoop-spark-install/" target="_blank" rel="noopener">macOS hadoop-spark安装方式</a></p>
<h3 id="本地模式、伪分布式模式、分布式模式"><a href="#本地模式、伪分布式模式、分布式模式" class="headerlink" title="本地模式、伪分布式模式、分布式模式"></a>本地模式、伪分布式模式、分布式模式</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/23/Theory-of-Generalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/23/Theory-of-Generalization/" itemprop="url">6. Theory-of-Generalization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-23T09:55:02+08:00">
                2019-06-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><p>希望成长函数 m<sub>H</sub>(N) 来代替实际的 M ?</p>
<h2 id="Restriction-of-Break-Point"><a href="#Restriction-of-Break-Point" class="headerlink" title="Restriction of Break Point"></a>Restriction of Break Point</h2><p>growth function m<sub>H</sub>(N): max number of dichotomies（即最多能够产生几种不同的 xx 或 oo）。即 hypothesis set 在 N 个点上，最多能够产生多少种 dichotomy. </p>
<p>几种不同类型的成长函数形式：</p>
<p><img src="/images/2019/picture/Theory-of-Generalization/1.png" alt></p>
<p>上面的 2D perceptrons, 虽然不知道成长函数是什么样的类型，但是可以知道 break point 为4.</p>
<p> 下面一种情形为：当最小的 break point k = 2 时，当有 1个点，2个点，3个点的情形：可以证明，当有1个点 N = 1时，可以被成长函数 m<sub>H</sub> shatter(打散),当有2个点的时候，不能够被 shatter ，最大的 dichotomies 小于4，当 N = 3 时，不能被 shatter，最大的 dichotomies 等于4.</p>
<p> <img src="/images/2019/picture/Theory-of-Generalization/2.png" alt></p>
<p>小练习：</p>
<p> <img src="/images/2019/picture/Theory-of-Generalization/3.png" alt></p>
<h2 id="Bounding-Function-Basic-Cases"><a href="#Bounding-Function-Basic-Cases" class="headerlink" title="Bounding Function: Basic Cases"></a>Bounding Function: Basic Cases</h2><p>bounding function B(N, k): maximum possible m<sub>H</sub>(N) when point = k。这个成长函数在 K 那边漏出一线曙光，是它的 Break point, 那这个成长函数最多有多少种 dichotomy 的可能?</p>
<p>限制条件： 最大长度为 N 向量的(o, x)，然而 ‘no shatter’ 任何 长度为k 的子向量。就是不希望看到 2<sup>k</sup> 的组合，即不能出现 shatter，不能将 k 个点，统统都 ko 掉。</p>
<p>例如：上线函数 B(N, 3) 的地方边界有两种：① positive intervals(k=3)；② 1 D perceptrons(k=3)。即不去考虑 positive intervals 以及 1 D perceptrons 的成长函数，而是直接考虑他们的上线：B(N, 3)。</p>
<p>接下来就先证明，上线函数 bounding functions 是像多项式那样的成长。</p>
<p>new goal: B(N, k) ≤ ploy(N) ?</p>
<p>现在的 B function 有两个参数，一个是 N(有几个点), 一个是 K(一线曙光发生的地方).</p>
<p>前面我们知道的情形：</p>
<ul>
<li>B(N, 1) = 1</li>
<li>B(2, 2) = 3(maximum &lt; 4)</li>
<li>B(3, 2) = 4(‘pictorial’ proof previously)</li>
<li>容易知道，当 N &lt; K 时，B(N, k) = 2<sup>N</sup></li>
<li>当 N = K时，B(N, k) = 2<sup>N</sup> - 1</li>
</ul>
<p>B function(实际是成长函数的上线) 查表为：</p>
<p><img src="/images/2019/picture/Theory-of-Generalization/4.png" alt></p>
<h2 id="Bounding-Function-Inductive-Cases"><a href="#Bounding-Function-Inductive-Cases" class="headerlink" title="Bounding Function: Inductive Cases"></a>Bounding Function: Inductive Cases</h2><p>下面开始填上面的 B function 的左下角的部分。</p>
<h2 id="A-Pictorial-Proof"><a href="#A-Pictorial-Proof" class="headerlink" title="A Pictorial Proof"></a>A Pictorial Proof</h2><h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="chenxiaolong">
            
              <p class="site-author-name" itemprop="name">chenxiaolong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xiaolongc929" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:xiaolongc929@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://plus.google.com/xiaolongc929" target="_blank" title="Google">
                      
                        <i class="fa fa-fw fa-google"></i>Google</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenxiaolong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
