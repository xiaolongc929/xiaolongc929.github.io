<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="xiaolongc">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="xiaolongc">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="xiaolongc">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>xiaolongc</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">xiaolongc</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2099/01/01/Contents/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2099/01/01/Contents/" itemprop="url">博客目录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2099-01-01T00:00:00+08:00">
                2099-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/目录/" itemprop="url" rel="index">
                    <span itemprop="name">目录</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/03/16/The-Learning-Problem/" target="_blank" rel="noopener">The-Learning-Problem</a></li>
<li>2 <a href="https://xiaolongc929.github.io/2019/03/17/Learning-to-answer-yes-no/" target="_blank" rel="noopener">Learning-to-answer-yes-no</a></li>
<li>3 <a href="https://xiaolongc929.github.io/2019/03/31/Types-of-learning/" target="_blank" rel="noopener">Types-of-learning</a></li>
<li>4 <a href="https://xiaolongc929.github.io/2019/04/05/Feasibility-of-learning/" target="_blank" rel="noopener">Feasibility-of-learning</a></li>
</ul>
<h1 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h1><h1 id="运筹规划"><a href="#运筹规划" class="headerlink" title="运筹规划"></a>运筹规划</h1><h1 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a>云计算</h1><h1 id="Java-语言"><a href="#Java-语言" class="headerlink" title="Java 语言"></a>Java 语言</h1><h1 id="Python-语言"><a href="#Python-语言" class="headerlink" title="Python 语言"></a>Python 语言</h1><h1 id="C-语言"><a href="#C-语言" class="headerlink" title="C++ 语言"></a>C++ 语言</h1><h1 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h1><h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/02/17/Hexo-HelloWorld/" target="_blank" rel="noopener">Hexo-HelloWorld</a></li>
<li>2 <a href="https://xiaolongc929.github.io/2019/03/10/hexo-action/" target="_blank" rel="noopener">hexo-实战</a></li>
<li>3 <a href="https://xiaolongc929.github.io/2019/03/12/hadoop-spark-install/" target="_blank" rel="noopener">macoc 下 hadoop spark 安装和配置</a></li>
</ul>
<h1 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/03/08/maven-action/" target="_blank" rel="noopener">maven实战</a></li>
</ul>
<h1 id="阅读感想"><a href="#阅读感想" class="headerlink" title="阅读感想"></a>阅读感想</h1><h1 id="个人随笔"><a href="#个人随笔" class="headerlink" title="个人随笔"></a>个人随笔</h1><ul>
<li>1 <a href="https://xiaolongc929.github.io/2019/03/10/Join-the-Internet-industry/" target="_blank" rel="noopener">加入互联网行业</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/fbprophet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/fbprophet/" itemprop="url">5. fbprophet原理及实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T16:13:28+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="fbprophet简介"><a href="#fbprophet简介" class="headerlink" title="fbprophet简介"></a>fbprophet简介</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/xgboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/xgboost/" itemprop="url">4. xgboost原理及实战</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T16:11:58+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="xgboost-简介"><a href="#xgboost-简介" class="headerlink" title="xgboost 简介"></a>xgboost 简介</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/07/GBDT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/GBDT/" itemprop="url">3. GBDT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-07T22:38:27+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="提升树模型"><a href="#提升树模型" class="headerlink" title="提升树模型"></a>提升树模型</h1><p>提升方法实际采用加法模型(即基函数的线性组合)与前向分布算法。以决策树为基函数的提升方法称为提升树(boosting tree)。<font color="orange">对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。还有一种实现版本是分类问题、回归问题都使用的是CART回归树。具体情况，还需要查看其对应的实现方法。TODO：sklearn里面的GBDT实现方式。</font> 提升树模型可以表示为决策树的加法模型：</p>
<p><center>$$ f <em>{M}(x)=\sum</em>{m=1}^{M}T(x;\Theta _{m}) $$</center><br>其中 $ T(x;\Theta _{m}) $ 表示决策树， $ \Theta _{m} $ 为决策树的参数， M 为树的个数。</p>
<h1 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h1><p>提升树算法采用前向分布算法，首先确定初始提升树 $ f _{0}=0 $ ,第m步的模型是： $ f _{m}=f _{m-1}(x)+T(x; \Theta _{m}) $ 。</p>
<p>其中， $ f _{m-1}(x) $ 为当前模型，通过经验风险极小化确定下一颗决策树参数 $ \Theta _{m} $ ：</p>
<p><center>$$ \Theta <em>{m}^{*}=argmin</em>{\Theta <em>{m}}\sum</em>{i=1}^{N}L(y _{i},f _{m-1}(x _{i})+T(x _{i};\Theta _{m})) $$</center><br>树的线性组合可以很好的拟合训练数据，即使数据中的输入与输出关系复杂也是，所以提升树一般是高功能的学习算法。</p>
<font color="orange">下面讨论针对不同问题的提升树学习算法，主要区别在于损失函数的不同。包括用平方误差损失的回归问题，用指数损失函数的分类问题，以及用一般损失函数的决策问题。</font>

<h2 id="负梯度拟合"><a href="#负梯度拟合" class="headerlink" title="负梯度拟合"></a>负梯度拟合</h2><p>GBDT损失函数拟合方法采用的是 Freidman 提出的损失函数负梯度来拟合本轮损失近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为：</p>
<p><center>r _{ti}=-\left [ \frac{\partial L(y _{i},f(x _{i})))}{\partial f(x _{i})} \right ] _{f(x)=f _{t-1}(x)} </center><br>利用 $ (x _{i}, r _{ti})(i=1,2,…,m) $ ，可以拟合一颗CART回归树，其对应的叶子点区域 $ R _{tj}, j=1,2,…,J $ .其中J为叶子结点个数。</p>
<p>针对每一个叶子结点的样本，我们求出使损失函数最小，也就是拟合叶子结点最好的输出值 $ c _{tj} $ 如下：</p>
<p><center>$$ c _{tj}=\underbrace{argmin} <em>{c}\sum</em>{x _{i}\epsilon R _{tj}}L(y _{i},f _{t-1}(x _{i})+c) $$</center><br>这样就可以得到本轮决策树拟合函数如下：</p>
<p><center>$$ h <em>{t}(x)=\sum</em>{j=1}^{J}c _{tj}I(x\epsilon R _{tj}) $$</center><br>从而本轮最终得到的强学习器的表达式为：</p>
<p><center>$$ f _{t}(x)=f <em>{t-1}(x) + \sum</em>{j=1}^{J}c _{tj}I(x\epsilon R _{tj}) $$</center><br>通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。</p>
<h2 id="二类分类问题"><a href="#二类分类问题" class="headerlink" title="二类分类问题"></a>二类分类问题</h2><p>分类问题，目前有两种实现方式：CART分类树或者CART回归树。大多讲义上都只介绍一种，导致很多同学对GBDT分类问题使用的是二叉分类树还是二叉回归树说法不一。</p>
<p>如果使用二叉分类树，提升树算法只需将AdaBoost算法的基本分类器限制为二类分类树即可，可以说是这时的提升树算法是 AdaBoost 算法的特例。</p>
<p>如果使用二叉回归树，</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/03/AdaBoost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/03/AdaBoost/" itemprop="url">2. AdaBoost</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-03T10:10:23+08:00">
                2019-08-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="提升方法的基本思路"><a href="#提升方法的基本思路" class="headerlink" title="提升方法的基本思路"></a>提升方法的基本思路</h1><p>理论基础：</p>
<p>强可学习(strongly learning)：在概率近似正确(probably approximately correct, PAC)学习框架下，一个概念(一个类)如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么称他为强可学习的。</p>
<p>弱可学习(weakly learnable): 一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测好，那么称这个概念为弱可学习的。</p>
<p>Schapire证明强可学习与弱可学习是等价的。即在PAC学习框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。这样如果发现了“弱学习算法”能否将他提升为“强可学习算法”。<font color="orange">大多数的提升方法都是改变训练数据的概率分布(训练数据的权值分布)，针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</font>下面介绍最典型的AdaBoost算法。</p>
<p>还有两个问题是：</p>
<p>1) 每一轮如何改变训练数据的权值或概率分布？</p>
<p>AdaBoost的做法是提高被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。</p>
<p>2) 如何将弱分类器组合为一个强分类器？</p>
<p>AdaBoost 采取加权多数表决方法，加大分类误差率小的弱分类器的权重，使其在表决中起较大作用，减小分类误差率大的弱分类器的权值，使其表决中起较小的作用。</p>
<h1 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h1><h2 id="AdaBoost二分类算法"><a href="#AdaBoost二分类算法" class="headerlink" title="AdaBoost二分类算法"></a>AdaBoost二分类算法</h2><p>假设给定一个二分类训练数据集：</p>
<center>$$ \mathit{T=\left { (x _{1},y _{1}),(x _{2},y _{2}),…,(x _{N},y _{N}) \right }} $$</center>

<p>其中，每个样本点由实例与标记组成，实例 $ x _{i}\epsilon \chi \subseteq \mathbb{R} _{n} $ ,标记 $ y _{i}\epsilon Y = \left { -1,+1 \right } $ , X 是实例空间，Y是标记集合。</p>
<p>算法：AdaBoost </p>
<p>输入：训练数据集 $ T= \left { (x _{1}, y _{1}),(x _{2},y _{2}),…,(x _{N},y _{N}) \right } $ ,其中 $ x _{i}\epsilon \chi \subseteq \mathbb{R} _{n} $ , $ y _{i}\epsilon Y = \left { -1,+1 \right } $ ;弱学习算法。</p>
<p>输出：最终分类器G(x).</p>
<ul>
<li>(1) 初始化训练数据的权值分布：<center>$$ D _{1}=(w _{11},…,w _{1i},…,w _{1N}), w _{1i}=\frac{1}{N}, i=1,2,…,N $$</center></li>
<li>(2) 对 m=1,2,…,M<br>– (a) 使用具有权值分布 $ D _{m} $ 的训练数据集学习，得到基本分类器：<br><center>$$ G _{m}(x):\chi \rightarrow \left { -1,+1 \right }. $$</center><br>– (b) 计算 $ G _{m}(x) $ 在训练数据集上的分类误差率：<br><center>$$ e _{m}=P(G _{m}(x _{i})\neq y <em>{i})=\sum</em>{i=1}^{N}w _{mi}I(G _{m}(x _{i})\neq y _{i}) $$</center><br>– (c) 计算 $ G _{m}(x) $ 的系数：<br><center>$$ \alpha _{m}=\frac{1}{2}log\frac{1-e _{m}}{e _{m}} $$</center><br>– (d) 更新训练数据集的权值分布：<br><center>$$ D _{m+1}=(w _{m+1,1},…,w _{m+1,i},…,w _{m+1,N}) $$</center><br><center>$$ w _{m+1,i}=\frac{w _{mi}}{Z _{m}}exp(-\alpha _{m}y _{i}G _{m}(x _{i})),i=1,2,…,N $$</center><br>这里， $ Z _{m} $ 是规范因子<br><center>$$ Z <em>{m}=\sum</em>{i=1}^{N}w _{mi}exp(-\alpha _{m}y _{i}G _{m}(x _{i})) $$</center><br>它使 $ D _{m+1} $ 成为一个概率分布.</li>
<li>(3) 构建基本分类器的线性组合<center>$$ f(x)=\sum_{m=1}^{M}\alpha _{m}G <em>{m}(x) $$</em></center><br>得到最终分类器：<br><center>$$ G(x)=sign(f(x))=sign(\sum{m=1}^{M}\alpha _{m}G _{m}(x)) $$</center>

</li>
</ul>
<h2 id="AdaBoost回归问题算法流程"><a href="#AdaBoost回归问题算法流程" class="headerlink" title="AdaBoost回归问题算法流程"></a>AdaBoost回归问题算法流程</h2><p>来源于：<a href="https://www.cnblogs.com/pinard/p/6133937.html" target="_blank" rel="noopener">集成学习之Adaboost算法原理小结</a></p>
<p>输入：样本集 $ T=\left { (x _{1},y _{1}),(x _{2},y _{2}),…,(x _{m},y _{m}) \right } $ , 弱学习器算法，弱学习器迭代器K.</p>
<p>输出：强学习器 $ f(x) $</p>
<ul>
<li>(1) 初始化样本权重：<center>$$ D _{1}=(w _{11},…,w _{1i},…,w _{1N}), w _{1i}=\frac{1}{N}, i=1,2,…,N $$</center></li>
<li>(2) 对于k=1,2,…,k:<br>– a) 使用具有权重 $ D _{k} $ 的样本集来训练数据，得到弱学习器 $ G _{k}(x) $<br>– b) 计算训练集上的最大误差： $ E _{k}=max|y _{i} - G _{k}(x _{i})|, i=1,2,…,m $<br>– c) 计算每个样本相对误差：<br>— 如果是线性误差，则 $ e _{ki} = \frac{|y _{i}-G _{k}(x _{i})|}{E _{k}} $<br>— 如果是平方误差，则 $ e _{ki} = \frac{(y _{i}-G _{k}(x _{i}))^{2}}{E _{k}^{2}} $<br>— 如果是指数误差，则 $ e _{ki} =  1-exp(\frac{-|y _{i}-G _{k}(x _{i})|}{E _{k}}) $<br>– d) 计算回归误差率： $ e <em>{k}=\sum</em>{i=1}^{m}w _{ki}e _{ki} $<br>– e) 计算弱学习器系数： $ \alpha _{k} = \frac{e _{k}}{1-e _{k}} $<br>– f) 更新样本集的权重分布： $ w _{k+1,i}=\frac{w _{ki}}{Z _{k}}\alpha _{k}^{1-e _{ki}} $ ,其中 $ Z _{k} $ 是规范化因子： $ Z <em>{k}=\sum</em>{i=1}^{m}w _{ki}\alpha _{k}^{1-e _{ki}} $ .</li>
<li>(3) 构建最终强学习器为： $ f(x)=G _{k*}(x) $ .</li>
</ul>
<p>其中， $ G_ {k∗}(x) $ 是所有 $ ln\frac{1}{\alpha _{k}}, k=1,2,…,K $的中位数值对应序号k∗对应的弱学习器。</p>
<h1 id="AdaBoost的解释：前向分布算法"><a href="#AdaBoost的解释：前向分布算法" class="headerlink" title="AdaBoost的解释：前向分布算法"></a>AdaBoost的解释：前向分布算法</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/27/Decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/27/Decision-tree/" itemprop="url">1. Decision-tree</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-27T08:16:21+08:00">
                2019-07-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>目前，工作中接触最多的就是树模型，为了加深对树模型的理解，是时候将树模型总结总结了。</p>
<p>决策树：可以认为是 if-then 规则集合，也可以认为是定义在特征空间与类空间上的条件概率分布。</p>
<p>学习时，利用训练数据，根据损失函数最小化建立决策树模型；预测时，利用决策树模型对新数据分类。</p>
<p>决策树学习通常包括3个步骤：</p>
<font color="red">特征选择、决策树生成和决策树修剪。</font>

<p>决策树主要思想来自于：Quinlan 在1986提出的 ID3 和 1993年提出的C4.5，以及由 Breiman 等人在1984年提出的CART算法。</p>
<h1 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h1><p>决策树由结点(node)和有向边(directed edge)组成。结点有两类：内部结点(internal node, 表示一个特征或属性)和叶结点(leaf node, 表示一个类)。</p>
<p>可以将决策树看做是一个 if-then 规则：即决策树的根节点到叶结点的每一条路径为一条规则：路径上内部结点特征对应着规则的条件，而叶结点的类对应则规则的结论。</p>
<p>也可以将决策树表示给定特征条件下类的条件概率分布：这一条件概率分布定义在特征空间的一个划分上，将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布，即构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。</p>
<p>李航博士将统计学习的三要素为：方法=模型+策略+算法</p>
<ul>
<li><font color="red">模型：</font>决策树。</li>
<li><font color="red">策略：</font>以损失函数(通常是正则化的极大似然函数)为目标函数的最小化。</li>
<li><font color="red">算法：</font>通常采用启发式方法，近似求解最优化问题。</li>
</ul>
<p>决策树学习的过程通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类过程，这一过程对应着对特征空间的划分，也对应着决策树的构建。</p>
<p>决策树学习算法包含<font color="Tomato">特征选择、决策树生成、决策树剪枝过程</font>。其中决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。</p>
<p>常用的算法包含有 ID3、C4.5 与 CART，下面分别介绍三种算法的决策树学习的特征选择、决策树的生成和剪枝过程。</p>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>直观上，如果一个特征具有更好的分类能力，或者按照这个特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就应该选择这个特征，信息增益(information gain)就能够很好地表示这一直观准则。</p>
<p>下面依次介绍与信息增益有关的几个概念：</p>
<font color="red">熵(entropy): </font>用来表示随机变量的不确定性的度量。设X是一个取值有限的离散变量，概率分布为:<br><center>$$\mathit{P(X = x_{i})=p_{i}, i=1,2,…,n}$$</center><br>则随机变量 X 的熵定义为：<br><center>$$\mathit{H(X)=-\sum_{i=1}^{n}p_{i}logp_{i}}$$</center><br>如果 p<sub>i</sub>=0, 则定义 0log0=0. 由上面可以看出，熵只依赖于 <em>X</em> 的分布，而与 <em>X</em> 的取值无关，所以也可以将 X 的熵记为 <em>H(p)</em>, 即：<br><center>$$\mathit{H(p)=-\sum_{i=1}^{n}p_{i}logp_{i}}$$</center><br>熵越大，随机变量的不确定性就越大，从定义可以看出：<br><center>$$\mathit{0\leqslant H(p)\leqslant logn}$$</center><br>当随机变量只有两个取值时，则 <em>X</em> 的分布为：<br><center>$$\mathit{P(X=1)=p,P(X=0)=1-p,0\leqslant p\leqslant 1}$$</center><br>熵为：<br><center>$$\mathit{H(p)=-plog_{2}p-(1-p)log_{2}(1-p)}$$</center><br>其中 <em>H(p)</em> 随概率 <em>p</em> 变化的曲线如图所示:<br><br><img src="/images/2019/picture/Decision-tree/1.png" alt><br><br><font color="red">联合概率分布: </font>设有随机变量 <em>(X, Y)</em>，其联合概率分布为：<br><center>$$\mathit{P(X=x_{i},Y=y_{i})=p_{ij},i=1,2,…,n;j=1,2,…,m}$$</center><br><font color="red">条件熵: </font> <em>H(Y|X)</em>表示在已知随机变量 <em>X</em> 的条件下随机变量 <em>Y</em> 的不确定性. 随机变量 <em>X</em> 给定条件下的随机变量 <em>Y</em> 的条件熵(conditional entropy) <em>H(Y|X)</em>, 定义为 <em>X</em> 给定条件下 <em>Y</em> 的条件概率分布的熵对 X 的数学期望：<br><center>$$\mathit{H(Y|X)=\sum_{i=1}^{n}p_{i}H(Y|X=x_{i})}$$</center><br>其中，$$\mathit{p_{i}=P(X=x_{i}),i=1,2,…,n.}$$<br><br>当熵和条件熵中的概率由 <strong>数据估计(特别是极大似然估计)</strong> 得到时，所对应的的熵分别为经验熵(empirical entropy) 和经验条件熵(empirical conditional entropy).<br><br><font color="red">信息增益: </font>特征<em>A</em>对训练数据集<em>D</em>的信息增益<em>g(D,A)</em>, 定义为集合D的经验熵<em>H(D)</em>与特征<em>A</em>给定条件下<em>D</em>的经验熵<em>H(D/A)</em>之差，即：<br><center>$$\mathit{g(D,A)=H(D)-H(D|A)}$$</center><br>信息增益表示了得知特征<em>X</em>的信息，使得类<em>Y</em>的信息不确定性减少的程度。<br><br>一般地，熵<em>H(Y)</em>与条件熵<em>H(Y|X)</em>之差称为 <strong>互信息(mutual information)</strong>. 决策树学习中的信息增益等价于训练数据集中类与特征的互信息。<br><br><font color="orange"><strong>决策树学习应用信息增益准则选择特征。</strong> 给定训练数据集D和特征A，<code>经验熵 H(D)</code> 表示对数据集D进行分类的不确定性，而 <code>经验条件熵 H(D|A)</code> 表示在特征A给定条件下对数据集D进行分类的不确定性。那么他们的差，即 <strong>信息增益，表示由于特征A而使得数据集D的分类不确定性减少的程度。</strong></font>

<p>设训练数据集为D，|D|表示其样本容量，即样本个数。设有 <em>K</em> 个类 $ C_{k},k=1,2,…,K, |C_{k}| $ 为属于类 $ C_{k} $ 的样本个数，$ \mathit{\sum_{k=1}^{K}|C_{k}|=|D|.} $ 设特征 <em>A</em> 有n个不同的取值 $ \mathit{(a_{1},a_{2},…,a_{n}.)}, $ 根据特征 A 的取值将D划分为 n 个子集  $ \mathit{(D_{1},D_{2},…,D_{n}), |D_{i}|} $ 为 $ D_{i} $ 的样本个数， $ \mathit{\sum_{i=1}^{n}|D_{i}|=|D|}. $ 记子集 $ D_{i} $ 中属于类 $ C_{k} $ 的样本集合为 $ D_{ik}, $ 即 $ D_{ik}=D_{i}\bigcap C_{k} $ , $ |D_{ik}| $ 为 $ D_{ik} $ 的样本个数。于是信息增益的算法为:</p>
<font color="gray"><br>输入：训练数据集D和特征A;<br><br>输出：特征A对训练数据集D的信息增益 g(D,A).<br><br>(1) 计算数据集 D的经验熵 H(D)<br><center>$$ \mathit{H(D)=-\sum_{i=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}} $$</center><br>(2) 计算特征A对数据集D的 <code>经验条件熵H(D|A)</code><br><center>$$\mathit{H(D|A)=\sum_{i=1}^{n}p_{i}H(D|A=a_{i})=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D|A=a_{i})}$$</center><br>即：<br><center>$$\mathit{H(D|A)=\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}H(D_{i})=- \sum_{i=1}^{n}\frac{|D_{i}|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_{i}|}log_{2}\frac{|D_{ik}|}{|D_{i}|}}$$</center><br>(3) 计算信息增益<br><center>$$\mathit{g(D,A)=H(D)-H(D,A)}$$</center><br></font>

<h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><font color="red">信息增益比: 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比(information gain ratio)可以对这一问题进行纠正。</font> 

<p>特征A对训练数据集的信息增益比 $ g_{R}(D,A) $ 定义为信息增益 $ g(D,A) $ 与训练数据集D关于特征A的值的的经验熵 $ H_{A}(D) $ 之比：</p>
<center>$$\mathit{g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}}$$</center>

<p>其中：</p>
<center>$$\mathit{H_{A}(D)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}}$$</center>


<h1 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h1><h2 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h2><p>输入：训练数据集D，特征集A, 阈值 $ \varepsilon $ ;</p>
<p>输出：决策树T.</p>
<ul>
<li>(1) 若D中所有实例属于同一类 $ C_{k} $ ,则T为单结点树，并将类 $ C_{k} $ 作为结点的类标记，返回T;</li>
<li>(2) 若 $ A=\Phi $ ,则T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(3) 否则，计算A中各个特征对D的<font color="red">信息增益</font>，选择信息增益最大的特征 $ A_{g} $ ;</li>
<li>(4) 如果 $ A_{g} $ 的信息增益小于阈值 $ \Phi $ , 则置T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(5) 否则，对 $ A_{g} $ 的每一可能值 $ a_{i} $ , 依 $ A_{g}=a_{i} $ 将D分割为若干非空子集 $ D_{i} $ , 将 $ D_{i} $ 中实例最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T;</li>
<li>(6) 对第i个子结点，以 $ D_{i} $ 为训练集，以 $ A-{A_{g} } $ 为特征集，递归地调用步(1)~(5), 得到子树 $ T_{i} $ , 返回 $ T_{i} $ .</li>
</ul>
<h2 id="ID3算法的不足"><a href="#ID3算法的不足" class="headerlink" title="ID3算法的不足"></a>ID3算法的不足</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a></p>
<ul>
<li>1) 无法处理连续特征。</li>
<li>2) 采用信息增益的特征缺点：取值比较多的特征比取值较少的特征的信息增益大。</li>
<li>3) 无法处理缺失值。</li>
<li>4) 没有考虑过拟合问题。</li>
</ul>
<h2 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h2><p>输入：训练数据集D，特征集A, 阈值 $ \varepsilon $ ;</p>
<p>输出：决策树T.</p>
<ul>
<li>(1) 若D中所有实例属于同一类 $ C_{k} $ ,则T为单结点树，并将类 $ C_{k} $ 作为结点的类标记，返回T;</li>
<li>(2) 若 $ A=\Phi $ ,则T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(3) 否则，计算A中各个特征对D的<font color="red">信息增益比</font>，选择信息增益最大的特征 $ A_{g} $ ;</li>
<li>(4) 如果 $ A_{g} $ 的信息增益小于阈值 $ \Phi $ , 则置T为单结点树，并将D中实例树最大的类 $ C_{k} $ 作为该结点的类标记，返回T;</li>
<li>(5) 否则，对 $ A_{g} $ 的每一可能值 $ a_{i} $ , 依 $ A_{g}=a_{i} $ 将D分割为若干非空子集 $ D_{i} $ , 将 $ D_{i} $ 中实例最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T;</li>
<li>(6) 对第i个子结点，以 $ D_{i} $ 为训练集，以 $ A-{A_{g} } $ 为特征集，递归地调用步(1)~(5), 得到子树 $ T_{i} $ , 返回 $ T_{i} $ .</li>
</ul>
<h2 id="C4-5对ID3的改进"><a href="#C4-5对ID3的改进" class="headerlink" title="C4.5对ID3的改进"></a>C4.5对ID3的改进</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a></p>
<ul>
<li>1) 不能处理连续特征的解决方法：C4.5将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为 $ a_{1},a_{2},…,a_{m}, $ 则C4.5取相邻两样本的平均值，一共取得m-1个划分点，其中第i个划分点 $ T_{i} $ 表示为： $ \mathit{T_{i}=\frac{a_{i}+a_{i+1}}{2}}. $ 对于这 m-1 个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的<font color="orange">二元离线分类点</font>。比如取到的增益最大的点为 $ a_{t}, $ 则小于 $ a_{t} $ 的值为类别1，大于 $ a_{t} $ 的值为类别2，这样我们就做到了<font color="orange">连续特征的离散化</font>。要注意的是，与离散属性不同的是，<font color="orange">如果当前节点为连续属性，则该属性后面还可以参与子结点的产生选择过程。</font> <strong>这里应该也采用的是信息增益比吧？</strong></li>
<li>2) 采用信息增益比来解决。</li>
<li>3) 对于确实值的处理，主要解决是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。</li>
<li><ul>
<li>3.1）对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。</li>
</ul>
</li>
<li><ul>
<li>3.2) 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。</li>
</ul>
</li>
<li>4) C4.5引入了正则化系数进行初步的剪枝。</li>
</ul>
<h2 id="C4-5的不足"><a href="#C4-5的不足" class="headerlink" title="C4.5的不足"></a>C4.5的不足</h2><ul>
<li>1) 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝.思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝.后面在下篇讲CART树的时候我们会专门讲决策树的减枝思路，主要采用的是后剪枝加上交叉验证选择最合适的决策树。</li>
<li>2) C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。</li>
<li>3) C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</li>
<li>4) C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。</li>
</ul>
<h1 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h1><p>决策树生成算法递归地产生决策树，容易出现过拟合，其原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构造出过于复杂的决策树。</p>
<p>在决策树学习中将已生成的树进行简化的过程称为剪枝。</p>
<font color="orange">决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)来实现。</font>

<p>设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点有 $ N_{t} $ 个样本点，其中k类的样本点有 $ N_{tk} $ 个，k=1,2,…,K，$ H_{t}(T) $ 为叶结点t上的经验熵，$ \alpha \geq 0 $ 为参数，则决策树学习的损失函数可以定义为：<br>$$ \mathit{C_{\alpha}(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)+\alpha|T|} $$<br>其中经验熵为：<br>$$ \mathit{H_{t}(T)=-\sum_{k}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}} $$<br>令：<br>$$ \mathit{C(T)=\sum_{t=1}^{|T|}N_{t}H_{t}(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}\frac{N_{tk}}{N_{t}}log\frac{N_{tk}}{N_{t}}} $$<br>这时有：<br>$$ \mathit{C_{\alpha}(T)=C(T)+\alpha|T|} $$</p>
<p>上式中，C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数 $ \alpha \geq 0 $ 控制两者之间的影响。较大的 $ \alpha $ 促使选择较简单的模型，较小的 $ \alpha $ 促使选择复杂的模型， $ \alpha =0 $ 意味只考虑与训练数据的拟合程度。</p>
<font color="orange">剪枝，就是当 $ \alpha $ 确定时，选择损失函数最小的模型。上式定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</font>

<p>树的剪枝算法：</p>
<p>输入：生成算法产生的整个树T，参数 $ \alpha $ ;</p>
<p>输出：剪枝后的子树 $ T_{\alpha}. $</p>
<ul>
<li>(1) 计算每个结点的经验熵</li>
<li>(2) 递归地从树的叶结点向上回缩：设一组叶结点回缩到父节点之前与之后的整体树分别为 $ T_{B} $ 与 $ T_{A} $ , 其对应的损失函数分别是 $ C_{\alpha}(T_{B}) $ 与 $ C_{\alpha}(T_{A}) $ , 如果 $ C_{\alpha}(T_{A}) \geq C_{\alpha}(T_{B}) $ 则进行剪枝，即将父节点变为新的叶结点.</li>
<li>(3) 返回(2), 直到不能继续为止，得到损失函数最小的子树 $ T_{\alpha}. $</li>
</ul>
<h1 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h1><p>CART 假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。</p>
<h2 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>为何CART要采用基尼系数：</p>
<p>我们知道，在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。</p>
<p>能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！<font color="orange">CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。</font></p>
<p>具体的，在分类问题中，假设有K个类别，第k个类别的概率为 $ p_{k} $ , 则基尼系数的表达式为：</p>
<center>$$ \mathit{Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}} $$</center><br>如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：<br><center>$$ \mathit{Gini(p)=2p(1-p)} $$</center><br>对于给定的样本D,假设有K个类别, 第k个类别的数量为 $ C_{k} $ ,则样本D的基尼系数表达式为：<br><center>$$ \mathit{Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_{k}|}{|D|})^{2}} $$</center><br>特别的，对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：<br><center>$$ \mathit{Gini(D, A)=\frac{|D_{1}|}{|D|}Gini(D_{1})+\frac{|D_{2}|}{|D|}Gini(D_{2})} $$</center>


<h2 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h2><p>决策树的生成就是递归地构建二叉决策树的过程。特征选择的标准为：</p>
<ul>
<li>对回归树用平方误差最小化准则；</li>
<li>对分类树用基尼指数最小化准则；</li>
</ul>
<h3 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h3><p>回归树的特征处理，连续特征和离线特征同样采用下面的启发式的方法，即选择第j个变量 $ x^{j} $ 和它的取值 s,作为切分变量(splitting variable) 和切分点(splitting point), 并定义两个区域：</p>
<center>$$ \mathit{R_{1}(j,s)=[x|x^{j}\leqslant s], R_{2}(j,s)=[x|x^{j}&gt;s]} $$</center><br>然后寻找最优切分变量j和最优切分点s，具体地，求解：<br><center>$$ \mathit{\min_{j,s}[\min_{c_{1}}\sum_{x_{i}\epsilon R_{1}(j,s)}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\epsilon R_{2}(j,s)}(y_{i}-c_{2})^{2}]} $$</center><br>对于固定输入变量j可以找到最优切分点s.<br><center>$$ \mathit{c_{1}^{<em>}=ave(y_{i}| x_{i} \epsilon R_{1}(j,s)),c_{2}^{</em>}=ave(y_{i}| x_{i} \epsilon R_{2}(j,s))} $$</center>

<p>输入：训练数据集D</p>
<p>输出：回归树f(x)</p>
<font color="orange">在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</font>

<ul>
<li>(1) 寻找最优切分变量 j 与切分点 s, 求解：<center>$$ \mathit{\min_{j,s}[\min_{c_{1}}\sum_{x_{i}\epsilon R_{1}(j,s)}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\epsilon R_{2}(j,s)}(y_{i}-c_{2})^{2}]} $$</center></li>
<li>(2) 用选定的对(j,s)划分区域并决定相应的输出值：<center>$$ \mathit{R _{1}(j,s)=[x|x^{j} \leqslant s], R <em>{2}(j,s)=[x|x^{j} &gt; s]} $$</em></center><br><center>$$ \mathit{c{m}^{*}=\frac{1}{N_{m}}\sum_{x_{i}\epsilon R_{m}(j,s)}y_{i},x\epsilon R_{m},m=1,2} $$</center></li>
<li>(3) 继续对两个子区域调用步骤(1),(2),直到满足停止条件.</li>
<li>(4) 将输入空间划分为M个区域 $ R_{1},R_{2},…,R_{M}, $ 并生成决策树：<center>$$ \mathit{f(x)=\sum_{m=1}^{M}c_{m}^{*}I(x\epsilon R_{m})} $$</center>

</li>
</ul>
<font color="orange">还有个疑问是：回归树对于离线的特征是如何处理的？1.计算基尼系数，这样如何和连续特征进行比较(排除);2.也采用均方差最小的方式，这也有两种情况:1)按是否大于某一值划分为两部分，2)按是否等于某一值划分为两部分。还有个问题是：在使用时模型如何区别连续特征和离散特征(看 sklearn 库中并没有传入标记离散特征或者连续特征的参数。)</font>

<h3 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h3><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>连续值的处理：</p>
<p>具体的思路如下，比如m个样本的连续特征A有m个，从小到大排列为 $ a_{1},a_{2},…,a_{m}, $$ 则CART算法取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$ Ti=\frac{a_{i}+a_{i+1}}{2} $ 。对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为 $ a_{t} $ ,则小于 $ a_{t} $ 的值为类别1，大于 $ a_{t} $ 的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与ID3或者C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。</p>
<p>离散值的处理：</p>
<p>回忆下ID3或者C4.5，如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点。这样导致决策树是多叉树。但是CART分类树使用的方法不同，他采用的是不停的二分，还是这个例子，CART分类树会考虑把A分成{A1}和{A2,A3}, {A2}和{A1,A3}, {A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3},然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。</p>
<p>CART 生成算法：</p>
<p>输入：训练数据集D，停止计算的条件(样本个数小于阈值，或者没有特征，或者基尼系数小于阈值。)；</p>
<p>输出：CART决策树。</p>
<ul>
<li>(1) 设结点的训练数据集为D，计算现有特征对该数据集的基尼指数，此时，对每个特征A，将数据集划分为两部分，根据上面介绍的连续值、离散值计算基尼指数。</li>
<li>(2) 选择基尼指数最小的特征A及其对应的切分点a，依最优特征与最优切分点，从现结点生成两个子结点，将训练集依特征分配到两个子结点中去。</li>
<li>(3) 对两个子结点递归调用(1),(2)直到满足停止条件。</li>
<li>(4) 生成CART树。 </li>
</ul>
<font color="orange">还有个疑问是：分类树如何确定离线特征或者连续特征。</font>

<h3 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a>CART 剪枝</h3><p>来自于 <a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)
</a></p>
<p>CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数，算法基本完全一样，这里我们一起来讲。</p>
<p>由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。但是，有很多的剪枝方法，我们应该这么选择呢？CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。</p>
<p>也就是说，CART树的剪枝算法可以概括为两步，第一步是从原始决策树生成各种剪枝效果的决策树，第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。</p>
<p>首先我们看看剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树T,其损失函数为：</p>
<p><center>$$ \mathit{C_{\alpha }(T_{t})=C(T_{t})+\alpha |T_{t}|} $$</center><br>其中，α为正则化参数，这和线性回归的正则化一样。 $ C(T_{t}) $ 为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。 $ |T_{t}| $ 是子树T的叶子节点的数量。</p>
<p>当α=0时，即没有正则化，原始的生成的CART树即为最优子树。当α=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的α，一定存在使损失函数 $ C_{α}(T) $ 最小的唯一子树。</p>
<p>看过剪枝的损失函数度量后，我们再来看看剪枝的思路，对于位于节点t的任意一颗子树Tt，如果没有剪枝，它的损失是:</p>
<p><center>$$ \mathit{C_{\alpha }(T_{t})=C(T_{t})+\alpha |T_{t}|} $$</center><br>如果将其剪掉，仅仅保留根节点，则损失是:</p>
<p><center>$$ \mathit{C_{\alpha }(T)=C(T)+\alpha} $$</center><br>当α=0或者α很小时， $ C_{α}(T_{t}) &lt; C_{α}(T) $ , 当α增大到一定的程度时: $ C_{α}(T_{t}) = C_{α}(T) $ .</p>
<p>当α继续增大时不等式反向，也就是说，如果满足下式：</p>
<p><center>$$ \mathit{\alpha =\frac{C(T)-C(T_{t})}{|T_{t}|-1}} $$</center><br>$ T_{t} $ 和T有相同的损失函数，但是T节点更少，因此可以对子树 $ T_{t} $ 进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点T。</p>
<p>最后我们看看CART树的交叉验证策略。上面我们讲到，可以计算出每个子树是否剪枝的阈值α，如果我们把所有的节点是否剪枝的值α都计算出来，然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。</p>
<p>好了，有了上面的思路，我们现在来看看CART树的剪枝算法。</p>
<p>输入是CART树建立算法得到的原始决策树T。</p>
<p>输出是最优决策子树 $ T_{α} $ 。</p>
<p>算法过程如下：</p>
<ul>
<li>(1) 初始化 $ \alpha _{min}=\infty  $， 最优子树集合 w={T}。</li>
<li>(2) 从叶子节点开始自下而上计算各内部节点t的训练误差损失函数 $ C_{\alpha}(T_{t}) $ (回归树为均方差，分类树为基尼系数)，叶子结点树 $ |T_{t}| $ ,以及正则化阈值 $ \alpha=min(\frac{C(T)-C(T_{t})}{|T_{t}|-1},\alpha _{min}) $ ,更新 $ \alpha _{min} = \alpha $ .</li>
<li>(3) 得到所有节点的α值的集合M。</li>
<li>(4) 从M中选择最大的值 $ \alpha _{k} $, 自上而下的访问子树t的内部结点，如果 $ \frac{C(T)-C(T _{t})}{|T _{t}|-1} \leq \alpha _{k} $ 时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到 $ \alpha _{k} $ 对应的最优子树 $ T _{k} $ .</li>
<li>(5) 最优子树集合 $ \omega = \omega \cup T _{k}, M=M-\alpha _{k} $ .</li>
<li>(6) 如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合 $ \omega $ .</li>
<li>(7) 采用交叉验证在 $ \omega $ 选择最优子树 $ T _{\alpha} $ .</li>
</ul>
<h1 id="对3种不同的树进行总结"><a href="#对3种不同的树进行总结" class="headerlink" title="对3种不同的树进行总结"></a>对3种不同的树进行总结</h1><table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值处理</th>
<th>剪枝</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类，回归</td>
<td>二叉树</td>
<td>基尼系数，均方差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><ul>
<li>总结《统计学习方法》-李航博士笔记</li>
<li>之前看过刘建平大佬写过的系列博客，在此强烈推荐 <a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树原理</a>。总结过程中，会参考大佬的文章。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/06/Programming-Hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/06/Programming-Hive/" itemprop="url">Hive 编程指南</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-06T15:31:43+08:00">
                2019-07-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/big-data/" itemprop="url" rel="index">
                    <span itemprop="name">big data</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>前言</strong></p>
<p>Hive 是 Hadoop 生态系统中必不可少的一个工具，它提供了一种 SQL 语言，可以查询存储在 Hadoop 分布式文件系统(HDFS)中的数据或其他和 Hadoop 集成的文件系统， 如 MapR-FS，Amazon的S3和像 HBase(Hadoop数据库) 和 Cassandra 这样的数据库中的数据。</p>
<h1 id="第1章-基础知识"><a href="#第1章-基础知识" class="headerlink" title="第1章 基础知识"></a>第1章 基础知识</h1><p>搜索引擎公司、电子商务公司、社交网络等许多组织意识到他们所收集的数据是让他们了解他们的用户，提高业务在市场上的表现以及提高基础架构效率的一个宝贵的资源。</p>
<p>Hive提供了一个被称为 Hive 查询语言(HiveQL 或 HQL)的SQL语言，来查询存储在 Hadoop 集群中的数据。Hive 可以将大多数的查询转换为MapReduce任务(job)，进而在介绍一个令人熟悉的 SQL 抽象的同时，拓宽 Hadoop 的课扩展性。</p>
<p>Hive 的劣势：</p>
<ul>
<li>Hive 不支持记录级别的更新，插入或者删除操作。</li>
<li>因为Hadoop是一个面向批处理的系统，而MapReduce任务(job)的启动过程需要消耗较长时间，所以 Hive 查询延时比较严重。</li>
<li>Hive 不支持事务。因此，Hive 不支持OLTP(联机事务处理)所需的关键功能。</li>
</ul>
<p>如果用户需要对大规模数据使用OLTP功能的话，那么应该选择使用一个NoSQL数据库，例如，和 Hadoop 结合使用的 HBase 及 Cassandra。</p>
<h2 id="Hadoop-和-MapReduce-综述"><a href="#Hadoop-和-MapReduce-综述" class="headerlink" title="Hadoop 和 MapReduce 综述"></a>Hadoop 和 MapReduce 综述</h2><p>参考Tom White 《Hadoop权威指南》一书。</p>
<p>MapReduce</p>
<p>MapReduce是一种计算模型，该模型可以将大型数据处理任务分解为很多单个的，可以在服务器集群中并行执行的任务，这些任务的计算结果可以合并在一起来计算最终的结果。</p>
<p>MapReduce编程模型由谷歌开发，两篇经典的论文为：《MapReduce: 大数据之上的简化数据处理》，以及《Google 文件系统》。这两篇论文启发了道-卡丁开发了 Hadoop。</p>
<p>MapReduce这个术语来自于两个基本的数据转换操作: Map过程和reduce过程。MapReduce 计算框架中的输入和输出的基本数据结构是键-值对。</p>
<p>下图介绍了一种 Word Count程序，左边的每个 Input(输入) 框都表示一个单独的文件，默认情况下，每个文档都会触发一个 Mapper 进程进行处理。而在实际场景中，大文件可能会划分为多个部分，每个部分都会被发送给一个 Mapper 进行处理。同时，也有将多个小文件合并为一个部分供某个 Mapper进行处理。</p>
<p><img src="/images/2019/picture/Programming-Hive/1-1.png" alt></p>
<p>Hadoop 神奇的地方一部分在于后面要进行的Sort和Shuffle过程，Hadoop会按照键来对键-值进行排序，然后Shuffle，将所有具有相同键的键-值对分发到同一个Reducer中。这里有很多方式可以决定哪个Reducer获取哪个范围内的键对应的数据。</p>
<h2 id="Hadoop生态系统中的Hive"><a href="#Hadoop生态系统中的Hive" class="headerlink" title="Hadoop生态系统中的Hive"></a>Hadoop生态系统中的Hive</h2><p>下图显示了Hive的主要模块，以及Hive是如何与Hadoop交互工作的。有好几种方式可以与Hive进行交互，例如命令行。或者图形界面：Karmasphere发布的一个商业产品，Cloudera提供的开源 Hue 项目，以及 Qubole 提供的 “Hive即服务”。</p>
<p><img src="/images/2019/picture/Programming-Hive/1-2.png" alt></p>
<p>Hive 发行版中附带的模块有CLI(命令行)，一个称为 Hive 网页界面(HWI)的简单网页界面，以及可通过 JDBC、ODBC 和一个 Thrift 服务器进行编程访问的几个模块。</p>
<p>所有的命令和查询都会进入到Driver模块，通过该模块对输入进行解析编译，对需求的计算进行优化，然后按照指定的步骤执行(通常是启动多个MapReduce任务(job)来执行)。当需要启动 MapReduce job 时，Hive本身是不会生成 Java MapReduce 算法程序的。相反，Hive 通过一个表示 “job执行计划”的XML 文件驱动执行内置的、原生的 Mapper 和 Reducer 模块。</p>
<p>Hive通过和 JobTracker通信来初始化 MapReduce任务(job)，而不必部署在 JobTracker 所在的管理节点上执行。</p>
<p>Metastore(元数据存储)是一个独立的关系型数据库(通常是一个MySQL实例)，Hive 会在其中保存表模式和其他系统元数据。</p>
<h3 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a>Pig</h3><p>Hive的替代工具中最有名的就是 Pig，假设用户的输入数据具有一个或者多个源，而用户需要进行一组复杂的转换来生成一个或者多个输出数据集。如果使用 Hive, 用户可能会使用嵌套查询，但是在某些时刻会需要重新保存临时表来控制复杂度。</p>
<p>Pig被描述为一种数据流语言，而不是一种查询语言，因此，Pig常用于ETL(数据抽取，数据转换，和数据装载)过程的一部分.</p>
<p>参考 Alan Gates 《Pig 编程指南》</p>
<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>如果用户需要 Hive 无法提供的数据特性(如行级别的更新，快速查询响应时间，以及支持事务)的话，那么该怎么办呢？ HBase 是一个分布式的、可伸缩的数据存储，其支持行级别的数据更新，快速查询和行级事务(但不支持多行事务)。</p>
<p>HBase支持的一个重要特性就是列存储，可以像键-值存储一样来使用HBase。HBase使用HDFS(或其他某种分布式文件系统)来持久化存储数据。Hive 现在已经可以和 HBase 结合使用了。</p>
<h3 id="Cascading、Crunch-及其他"><a href="#Cascading、Crunch-及其他" class="headerlink" title="Cascading、Crunch 及其他"></a>Cascading、Crunch 及其他</h3><h2 id="Java-和-Hive-词频统计算法"><a href="#Java-和-Hive-词频统计算法" class="headerlink" title="Java 和 Hive: 词频统计算法"></a>Java 和 Hive: 词频统计算法</h2><p>统计词频：</p>
<pre><code>CREATE TABLE docs (line STRING);
LOAD DATA INPATH &apos;docs&apos; OVERWRITE INTO TABLE docs;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
    (SELECT explode(split(line, &apos;\s&apos;)) AS word FROM docs) w
GROUP BY word
ORDER BY word;
</code></pre><h1 id="第2章-基础操作"><a href="#第2章-基础操作" class="headerlink" title="第2章 基础操作"></a>第2章 基础操作</h1><h2 id="安装方式"><a href="#安装方式" class="headerlink" title="安装方式"></a>安装方式</h2><p><a href="https://xiaolongc929.github.io/2019/03/12/hadoop-spark-install/" target="_blank" rel="noopener">macOS hadoop-spark安装方式</a></p>
<h3 id="本地模式、伪分布式模式、分布式模式"><a href="#本地模式、伪分布式模式、分布式模式" class="headerlink" title="本地模式、伪分布式模式、分布式模式"></a>本地模式、伪分布式模式、分布式模式</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/23/Theory-of-Generalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/23/Theory-of-Generalization/" itemprop="url">6. Theory-of-Generalization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-23T09:55:02+08:00">
                2019-06-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><p>希望成长函数 m<sub>H</sub>(N) 来代替实际的 M ?</p>
<h2 id="Restriction-of-Break-Point"><a href="#Restriction-of-Break-Point" class="headerlink" title="Restriction of Break Point"></a>Restriction of Break Point</h2><p>growth function m<sub>H</sub>(N): max number of dichotomies（即最多能够产生几种不同的 xx 或 oo）。即 hypothesis set 在 N 个点上，最多能够产生多少种 dichotomy. </p>
<p>几种不同类型的成长函数形式：</p>
<p><img src="/images/2019/picture/Theory-of-Generalization/1.png" alt></p>
<p>上面的 2D perceptrons, 虽然不知道成长函数是什么样的类型，但是可以知道 break point 为4.</p>
<p> 下面一种情形为：当最小的 break point k = 2 时，当有 1个点，2个点，3个点的情形：可以证明，当有1个点 N = 1时，可以被成长函数 m<sub>H</sub> shatter(打散),当有2个点的时候，不能够被 shatter ，最大的 dichotomies 小于4，当 N = 3 时，不能被 shatter，最大的 dichotomies 等于4.</p>
<p> <img src="/images/2019/picture/Theory-of-Generalization/2.png" alt></p>
<p>小练习：</p>
<p> <img src="/images/2019/picture/Theory-of-Generalization/3.png" alt></p>
<h2 id="Bounding-Function-Basic-Cases"><a href="#Bounding-Function-Basic-Cases" class="headerlink" title="Bounding Function: Basic Cases"></a>Bounding Function: Basic Cases</h2><p>bounding function B(N, k): maximum possible m<sub>H</sub>(N) when point = k。这个成长函数在 K 那边漏出一线曙光，是它的 Break point, 那这个成长函数最多有多少种 dichotomy 的可能?</p>
<p>限制条件： 最大长度为 N 向量的(o, x)，然而 ‘no shatter’ 任何 长度为k 的子向量。就是不希望看到 2<sup>k</sup> 的组合，即不能出现 shatter，不能将 k 个点，统统都 ko 掉。</p>
<p>例如：上线函数 B(N, 3) 的地方边界有两种：① positive intervals(k=3)；② 1 D perceptrons(k=3)。即不去考虑 positive intervals 以及 1 D perceptrons 的成长函数，而是直接考虑他们的上线：B(N, 3)。</p>
<p>接下来就先证明，上线函数 bounding functions 是像多项式那样的成长。</p>
<p>new goal: B(N, k) ≤ ploy(N) ?</p>
<p>现在的 B function 有两个参数，一个是 N(有几个点), 一个是 K(一线曙光发生的地方).</p>
<p>前面我们知道的情形：</p>
<ul>
<li>B(N, 1) = 1</li>
<li>B(2, 2) = 3(maximum &lt; 4)</li>
<li>B(3, 2) = 4(‘pictorial’ proof previously)</li>
<li>容易知道，当 N &lt; K 时，B(N, k) = 2<sup>N</sup></li>
<li>当 N = K时，B(N, k) = 2<sup>N</sup> - 1</li>
</ul>
<p>B function(实际是成长函数的上线) 查表为：</p>
<p><img src="/images/2019/picture/Theory-of-Generalization/4.png" alt></p>
<h2 id="Bounding-Function-Inductive-Cases"><a href="#Bounding-Function-Inductive-Cases" class="headerlink" title="Bounding Function: Inductive Cases"></a>Bounding Function: Inductive Cases</h2><p>下面开始填上面的 B function 的左下角的部分。</p>
<h2 id="A-Pictorial-Proof"><a href="#A-Pictorial-Proof" class="headerlink" title="A Pictorial Proof"></a>A Pictorial Proof</h2><h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/26/Training-versus-Testing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/26/Training-versus-Testing/" itemprop="url">5. Training-versus-Testing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-26T10:48:34+08:00">
                2019-05-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><p><img src="/images/2019/picture/Training-versus-Testing/0.png" alt></p>
<h2 id="Recap-and-preview"><a href="#Recap-and-preview" class="headerlink" title="Recap and preview"></a>Recap and preview</h2><blockquote>
<p>复习：整个 learning 的流程图如下图所示， learning 从资料 training examples 出发，通过资料以及 hypothesis set，最终从里面选择了一个最终的 hypothesis g(assumption: 训练的资料和最终测试 hypothesis 方法，都来自于同样的一个 distribution)。</p>
</blockquote>
<ul>
<li><p>如果训练集和测试集都来自于同样的一个分布，且 hypothesis set 的集合是有限的，则对于 learning algorithm A 选择的任意一个 g，都可以保证 E<sup>out</sup>(g) ≈ E<sup>in</sup>(g)。</p>
</li>
<li><p>如果 learning algorithm A 找到一个 g, 使得 E<sup>in</sup>(g) ≈ 0，PAC 能够保证 E<sup>out</sup>(g) ≈ 0。（这使得机器学习时间可能的事情）。</p>
</li>
</ul>
<p><strong>这里的两个重要假设是：① 训练集和测试集都来自于同样的一个分布；② PAC 定理。</strong></p>
<p><img src="/images/2019/picture/Training-versus-Testing/1.png" alt></p>
<p>下图总结了前面的4节课中的主要内容：</p>
<ul>
<li>第一节课，期望找到一个 g，使得 g 与 f 很接近。</li>
<li>第二节课，期望找到使 E<sup>in(g) ≈ 0 的方法。</sup></li>
<li>第三节课，了解了批次数据、监督学习以及二元分类的情况(PLA)。</li>
<li>第四节课，将 E<sup>out</sup>(g) ≈ E<sup>in</sup>(g)，连接了一起。</li>
</ul>
<p><img src="/images/2019/picture/Training-versus-Testing/2.png" alt></p>
<p><strong>这里将 learning 拆分为了两个问题，如下图所示：</strong></p>
<p><img src="/images/2019/picture/Training-versus-Testing/3.png" alt></p>
<p>提出问题：M(hypothesis set 的大小) 在上述两个问题中扮演者什么样的角色？</p>
<p>Trade-off(权衡) M：</p>
<p>这里会有两种情况：</p>
<ul>
<li>M 很小的时候：①Yes -&gt; 坏事情 p[BAD] 发生的概率如下图所示，因此，当 M 很小的时候，坏事情(即 E<sup>out</sup>(g) 和 E<sup>in</sup>(g) 不接近)发生的概率就很小。②No -&gt; 当 M 很小的时候，可以选择的算法就很小，未必能够找到一个 g 使得 E<sup>in</sup> 足够小。</li>
<li>M 很大的时候：①No -&gt; 。②Yes -&gt; 选择更多了。</li>
</ul>
<p><img src="/images/2019/picture/Training-versus-Testing/4.png" alt></p>
<p>因此，如果 M 太大的话，未必是一件好事。</p>
<p>对比 M 是有限的还是无线的两种情况：</p>
<p><img src="/images/2019/picture/Training-versus-Testing/5.png" alt></p>
<p><strong>下面的问题也很有意思，他保证了要想 E<sup>out</sup>(g) 和 E<sup>in</sup>(g) 接近的话，至少需要的数据量。</strong></p>
<p><img src="/images/2019/picture/Training-versus-Testing/6.png" alt></p>
<h2 id="Effective-number-of-lines"><a href="#Effective-number-of-lines" class="headerlink" title="Effective number of lines"></a>Effective number of lines</h2><blockquote>
<p>复习：下面复习下 M 的来源，考虑 BAD events 的定义为  E<sup>out</sup>(g) 和 E<sup>in</sup>(g) 相差很远(大于 ε)的如下图所示，当有 M 个可用选择的 f，则可以使用联结的形式将这些 f 联结在一起。</p>
</blockquote>
<p><img src="/images/2019/picture/Training-versus-Testing/7.png" alt></p>
<p>下面的问题是，如果 M 的无限的，怎么办呢？(Uniform Bound Fail?)</p>
<p>现在想，如果有两个比较接近的 hypotheses h<sup>1</sup> ≈ h<sup>2</sup>(比如两条比较接近的线)，这两个比较接近的 h<sup>1</sup> 、 h<sup>2</sup> 的 E<sup>in</sup>是一样的，E<sup>out</sup> 也很接近。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/8.png" alt></p>
<p>这种坏事情是互相叠起来的，像上图中的右边的 B1, B2, B3 所示。但是，我们在使用 Uniform Bound 的时候，并没有考虑叠起来这件事情，所以造成上限是一个 over-estimating。</p>
<p>下面要做的事情就是找出这些坏事情重叠的部分：</p>
<ul>
<li>第一步，能否将无限多的 hypothesis set，分为有限多个类？</li>
<li>如何分类？</li>
</ul>
<p><img src="/images/2019/picture/Training-versus-Testing/9.png" alt><br><img src="/images/2019/picture/Training-versus-Testing/10.png" alt><br><img src="/images/2019/picture/Training-versus-Testing/11.png" alt><br><img src="/images/2019/picture/Training-versus-Testing/12.png" alt></p>
<p>在平面中的点和线而言，以将点划分为不同的类为区分：</p>
<ul>
<li>1个点，有 2 种线</li>
<li>2个点，有 4 种线</li>
<li>3个点，最多有 8 种线</li>
<li>4个点，最多有 14 种线</li>
</ul>
<p>如果将输入的点以线分开的话，得到的有效的线的数量是有限的。</p>
<p>使用 effective(N) 来代替 M。 effective(N) &lt;&lt; 2<sub>N</sub>。N 代表 N 个点(即 N 个样本)。</p>
<p>如果只从输入的点看的话，那么得到线的种类是有限的，把这种有效的线叫做： Effective Number of Lines。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/13.png" alt></p>
<p>就算有无限多条线 M，如果能够将无限多条线分为 effective(N), 比 2<sub>N</sub> 小很多的话，我们就可以保证可能学的到东西。</p>
<h2 id="Effective-number-of-hypotheses"><a href="#Effective-number-of-hypotheses" class="headerlink" title="Effective number of hypotheses"></a>Effective number of hypotheses</h2><h3 id="Dichotomies-Mini-hypotheses"><a href="#Dichotomies-Mini-hypotheses" class="headerlink" title="Dichotomies: Mini-hypotheses"></a>Dichotomies: Mini-hypotheses</h3><p>下面想象有这么一个 hypothesis，它将 X 空间分为 {x, o}，假如有 N 个点(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>)的话，这 N 个点产生了几种{x, 0}组合可能性，将这种组合叫做 <strong>Dichotomy</strong>。</p>
<ul>
<li>a dichotomy(二分的意思): </li>
<li>下面想看看一个 hypothesis set 可以做出多少种 dichotomy 出来，例如其中一个 dichotomy 将所有的点都分为 o。</li>
<li>H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>) :将所有的 dichotomy 放在一起，形成一个集合，用 H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>) 表示。这是 dichotomies，而不是原来的 hypothesis set。</li>
<li>具体的情况，如下图所示：hypotheses H 的大小最多有无限多个，而 dichotomies H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>) 的上限为 2<sub>N</sub>。</li>
<li>接着用 |H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>)| 的大小来代替 M.</li>
</ul>
<p><img src="/images/2019/picture/Training-versus-Testing/14.png" alt></p>
<h3 id="Growth-Function"><a href="#Growth-Function" class="headerlink" title="Growth Function"></a>Growth Function</h3><p>其实，dichotomies size |H(x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>)| 的大小，是取决于先选好的 (x<sup>1</sup>, x<sup>2</sup>, … , x<sup>N</sup>)(想下前面当有3个点时，可能有 8 种组合，在一条线上时有6种组合)。</p>
<p>如果想要剔除掉对先选好的点(即对 X 的依赖)，就算下最终最大有几种就可以了：</p>
<p><img src="/images/2019/picture/Training-versus-Testing/15.png" alt></p>
<p>将最大的 dichotomies size 记录为 m<sup>H</sup>(N)，并将 m<sup>H</sup>(N) 成为 Growth Function(成长函数)。Growth Function 的输出一定是有限的，因为最多为 2<sub>N</sub>。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/16.png" alt></p>
<p>下面要解决的问题是，如何计算 Growth function?</p>
<h4 id="Growth-Function-for-positive-rays"><a href="#Growth-Function-for-positive-rays" class="headerlink" title="Growth Function for positive rays"></a>Growth Function for positive rays</h4><p>输入：一维的实数，<br>hypothesis: 大于阈值为正，小于阈值的为负。</p>
<p>给N个点，最多可以切出多少不一样的 dichotomy？ 可以有 N+1 种 dichotomy。即成长函数为：m<sup>H</sup>(N) = N + 1</p>
<p><img src="/images/2019/picture/Training-versus-Testing/17.png" alt></p>
<p>核心目标是：利用成长函数 m<sup>H</sup>(N) 来代替 M。</p>
<h4 id="Growth-Function-for-positive-intervals"><a href="#Growth-Function-for-positive-intervals" class="headerlink" title="Growth Function for positive intervals"></a>Growth Function for positive intervals</h4><p>m<sup>H</sup>(N) = 1/2 * N<sup>2</sup> + 1/2 N  + 1</p>
<h4 id="Growth-Function-for-convex-sets"><a href="#Growth-Function-for-convex-sets" class="headerlink" title="Growth Function for convex sets"></a>Growth Function for convex sets</h4><p>m<sup>H</sup>(N) = 2<sup>N</sup>，即 “shattered”, 即这 N 个点，被下面这种 hypothesis 打散了。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/18.png" alt></p>
<p><strong>四个不同的成长函数为：</strong></p>
<p><img src="/images/2019/picture/Training-versus-Testing/19.png" alt></p>
<h2 id="Break-point"><a href="#Break-point" class="headerlink" title="Break point"></a>Break point</h2><p>成长函数第一个看起来有一些希望的点在哪里？把这个点叫做 Break point。例如在 perceptrons 中，3个点的时候有8种情况，4个点的时候有14种情况，将第4个点叫做 perceptrons 的 break point。</p>
<p>从 k 是 break point 的时候，k 以后都是 break point。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/20.png" alt></p>
<p>2D perceptrons 的 break point 为：4</p>
<p>break point 和成长函数有一定的关系，见下图的左下角。</p>
<p><img src="/images/2019/picture/Training-versus-Testing/21.png" alt></p>
<h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/05/Feasibility-of-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenxiaolong">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaolongc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/05/Feasibility-of-learning/" itemprop="url">4. Feasibility-of-learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-05T08:25:43+08:00">
                2019-04-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning-foundation/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning foundation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="When-can-machines-learn-illustrative-technical"><a href="#When-can-machines-learn-illustrative-technical" class="headerlink" title="When can machines learn? (illustrative + technical)"></a>When can machines learn? (illustrative + technical)</h1><h2 id="learning-is-impossible"><a href="#learning-is-impossible" class="headerlink" title="learning is impossible?"></a>learning is impossible?</h2><p>No free lunch: NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论”什么学习算法更好“毫无意义，因为若考虑所有潜在的问题，则所有的算法一样好. 要谈论算法的相对优劣，必须要针对具体问题；在某些问题上表现好的学习算法，在另一问题上却可能不尽如人意，学习算法自身的归纳偏好与问题是否相配，往往会起到决定性作用. </p>
<h2 id="probability-to-the-rescue"><a href="#probability-to-the-rescue" class="headerlink" title="probability to the rescue"></a>probability to the rescue</h2><p>inferring something unknow:</p>
<p>在不知道瓶子中各种颜色弹珠数量的前提下，考虑一个瓶子中橘色弹珠的比例。</p>
<p>解决方式： sample:</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/1.png" alt></p>
<p>霍夫定理：</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/2.png" alt></p>
<h2 id="connection-to-learning"><a href="#connection-to-learning" class="headerlink" title="connection to learning"></a>connection to learning</h2><p>将下图中的一颗一颗的弹珠，想象为一个一个的样本x, 如果 hypothesis h 与 target f 对样本 x 表现不一致，则将弹珠染为橘色的，否则染为绿色的。</p>
<p>假设现在有一个固定的 h 在手上，则就可以按照上面的规则将弹珠染为橘色或者绿色。</p>
<p>从瓶子中将染好颜色的弹珠抓出来，比如抓出100个弹珠，就相当于抓取出了100个样本 x 出来了，这个做的好处是可以检验 h 在数据集 D 上的表现，有多少样本 x 与 f(x) 与真实值 y 变现的不同。</p>
<p>这样就能从看见的资料中，衡量 hypothesis  h 与 target f 之间的偏离程度。</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/3.png" alt></p>
<blockquote>
<p>个人理解，林老师想表达的意思是：瓶子中的弹珠就像整个数据集，我们没有办法将整个瓶子的弹珠倒出来检查一遍颜色(即不能在整个数据集上验证 hypothesis h )，因此采用独立同分布的方式从瓶子中取出一部分弹珠(即在一部分数据集上验证 h)，来评价 h 与 f 之间的相似程度。</p>
</blockquote>
<p>下图中，有很多的原件，其中 unknown P on X(相当于瓶子)，以一定的几率从瓶子中取样会用在两个地方(即图中的两个虚线)，一个是 training examples，另一个是 h ≈ f，其中 E<sub>in</sub> 表示 in-sample(即从瓶子中取出来的弹珠，手上的弹珠)，E<sub>out</sub> 表示 out-of-sample(即没有采用出来的弹珠，手上以外的瓶子里面的弹珠)，通过 E<sub>in</sub> 能够根据霍夫不等式求出 E<sub>out</sub>。</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/4.png" alt></p>
<ul>
<li>霍夫不等式对所有的 N 与 ε 都是正确的</li>
<li>霍夫不等式不依赖于 E<sub>out</sub>， 即不需要知道 E<sub>out</sub>，也不需要 f 和 P.</li>
<li>根据 probably approximately correct(PAC) 定理， E<sub>in</sub> = E<sub>out</sub>。这个保证来源于E<sub>in</sub> 和 E<sub>out</sub> 都是从分布 P 中产生的。</li>
</ul>
<p><img src="/images/2019/picture/Feasibility-of-learning/5.png" alt></p>
<p>如果在组合的 h 中， E<sub>in</sub>(h) 足够小， learning algorithm A 拿出来一个 h 做为 g  =&gt;  ‘g = f’ PAC。</p>
<p>如果只有一个 hypothesis h，怎么来验证 g 与 f 是否接近呢？ 首先可以将唯一的 h 作为 g, 然后从 P 中抽取一部分样本，来验证 g 与 f 之间是否接近。</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/6.png" alt></p>
<h2 id="connection-to-real-learning"><a href="#connection-to-real-learning" class="headerlink" title="connection to real learning"></a>connection to real learning</h2><p>上面假设了只有 1 个 hypothesis h 时候的验证方式，假设现在有多个 hypothesis h，应该怎么处理呢？</p>
<p>例如下图，有 M 个 hypothesis，其中 h<sub>M</sub> 在 E<sub>in</sub>(h<sub>M</sub>) 中表现都是正确的(即训练集的误差为0)，你要不要选 h<sub>M</sub>。</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/7.png" alt></p>
<blockquote>
<p>图中也形象的说明了， h<sub>2</sub> 中的绿色弹珠最对，即  h<sub>2</sub> 在所有数据上误差最小。因此，如果选择 h<sub>M</sub> 尽管 E<sub>in</sub>(h<sub>M</sub>) 全是正确的(即训练集误差为0), 但是 h<sub>M</sub> 确不是最好的 g（过拟合了）。</p>
</blockquote>
<p>Bad sample (不好的抽样): E<sub>in</sub> 与 E<sub>out</sub> 非常不接近。 hypothesis 变大的时候(即 h 变多的时候)，是会加大 Bad sample 的概率。例如，只有 1 个硬币(即只有1个 hypothesis)，丢5次全为正面的概率为 1/32，但是如果有 150 个硬币(即有150个 h)，其中有5次全为正面的概率为 1 - (31/32)<sup>150</sup>。</p>
<p>霍夫不等式给出了 Bad sample 的概率为，下图是只有一个 h 时候的概率，说明 Bad sample 的概率很小：</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/8.png" alt></p>
<p>但是，如果有多个 h 的时候，Bad sample 的概率就变大很多：</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/9.png" alt></p>
<p>从上图可以看出，霍夫不等式是对一个一个的 h 的保证，即每行的 Bad sample 的概率都很小，但是 learning algorithm A，是从多个 h 中选出一个做为 g(即通过资料 D<sub>1</sub> ，从 h<sub>1</sub> 到 h<sub>M</sub> 中，选择一个 g)，即每列遇到 Bad sample 的概率。</p>
<p>对于 M 个 h，遇到 Bad Data 的上限为：</p>
<p><img src="/images/2019/picture/Feasibility-of-learning/10.png" alt></p>
<p>上面的图片也是表明了，选择 g 的原则为，使 E<sub>in</sub>(g) 尽量小。</p>
<p>下图是一个学习的过程，即主要由两点组成。</p>
<ul>
<li>if |H| = M 是有限的，并且样本数据个数 N 足够大，无论算法 A 怎么选择到的 g, 都能保证 E<sub>out</sub>(g) ≈ E<sub>in</sub>(g)。</li>
<li>if 算法A找到一个 g，使得 E<sub>in</sub>(g) ≈ 0，则 PAC 能够保证 E<sub>out</sub>(g) ≈ 0.</li>
</ul>
<p><img src="/images/2019/picture/Feasibility-of-learning/11.png" alt></p>
<blockquote>
<p>本节主要解决了，在有限的 hypothesis h 的条件下，机器学习有可能做到的证明。下面的几个将处理在无限的 hypothesis h 的条件下，机器学习的证明。</p>
</blockquote>
<h1 id="Why-can-machines-learn-theoretical-illustrative"><a href="#Why-can-machines-learn-theoretical-illustrative" class="headerlink" title="Why can machines learn? (theoretical +illustrative)"></a>Why can machines learn? (theoretical +illustrative)</h1><h1 id="How-can-machines-learn-technical-practical"><a href="#How-can-machines-learn-technical-practical" class="headerlink" title="How can machines learn? (technical +practical)"></a>How can machines learn? (technical +practical)</h1><h1 id="How-can-machines-learn-better-practical-theoretical"><a href="#How-can-machines-learn-better-practical-theoretical" class="headerlink" title="How can machines learn better? (practical + theoretical)"></a>How can machines learn better? (practical + theoretical)</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="chenxiaolong">
            
              <p class="site-author-name" itemprop="name">chenxiaolong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xiaolongc929" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:xiaolongc929@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://plus.google.com/xiaolongc929" target="_blank" title="Google">
                      
                        <i class="fa fa-fw fa-google"></i>Google</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenxiaolong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
