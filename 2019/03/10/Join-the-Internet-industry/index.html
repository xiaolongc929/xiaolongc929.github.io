<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.15.6/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.15.6/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.15.6/dist/index.js"></script><script>(r => {
                setTimeout(r);
              })(() => {
  const {
    markmap,
    mm
  } = window;
  const {
    el
  } = markmap.Toolbar.create(mm);
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
                const markmap = getMarkmap();
                window.mm = markmap.Markmap.create(
                  "svg#mindmap",
                  (getOptions || markmap.deriveOptions)(jsonOptions),
                  root2
                );
              })(() => window.markmap,null,{"type":"root","depth":0,"content":"","children":[{"type":"heading","depth":1,"payload":{"lines":[0,1]},"content":"LM","children":[{"type":"heading","depth":2,"payload":{"lines":[2,3]},"content":"SLM","children":[]},{"type":"heading","depth":2,"payload":{"lines":[4,5]},"content":"NLM","children":[{"type":"list_item","depth":3,"payload":{"lines":[6,7]},"content":"word2vec","children":[]}]},{"type":"heading","depth":2,"payload":{"lines":[8,9]},"content":"PLM","children":[{"type":"list_item","depth":3,"payload":{"lines":[10,11]},"content":"ELMo(biLSTM)、Bert(Transform)、 GPT2","children":[]}]},{"type":"heading","depth":2,"payload":{"lines":[12,13]},"content":"LLM","children":[{"type":"list_item","depth":3,"payload":{"lines":[14,15]},"content":"GPT3(1750亿参数)、PaLM(5400亿参数)","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[15,16]},"content":"Planning for AGI and beyond(讨论了实现 AGI 的短期和长期计划)","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[16,17]},"content":"How does gpt obtain its ability? tracing emergent abilities of language models to their sources !!!","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[17,18]},"content":"Emergent abilities of large language models !!!","children":[]}]},{"type":"heading","depth":2,"payload":{"lines":[19,20]},"content":"Background","children":[{"type":"list_item","depth":3,"payload":{"lines":[21,22]},"content":"Scaling Laws for LLMs: KM scaling law、 Chinchilla scaling law","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[22,23]},"content":"Emergent Abilities of LLM: In-context learning、Instruction following、Step-by-step reasoning","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[23,24]},"content":"Key Techniques for LLMs: Scaling、Training、Ability eliciting、Alignment tuning、Tools manipulation","children":[]}]}]},{"type":"heading","depth":1,"payload":{"lines":[25,26]},"content":"pre-training","children":[{"type":"heading","depth":2,"payload":{"lines":[27,28]},"content":"数据收集","children":[{"type":"list_item","depth":3,"payload":{"lines":[29,30]},"content":"数据来源","children":[{"type":"list_item","depth":4,"payload":{"lines":[30,31]},"content":"通用文本数据: 网页、对话文本、书籍","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[31,32]},"content":"专用文本数据: 多语言文本、科学文本、代码","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[33,34]},"content":"数据预处理","children":[{"type":"list_item","depth":4,"payload":{"lines":[34,35]},"content":"过滤: 基于分类器方法，基于启发式方法","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[35,36]},"content":"去重: 在句子级、文档级、数据集三个维度上进行去重操作","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[36,37]},"content":"隐私去除: 基于规则: 关键字识别","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[37,38]},"content":"分词: 常用 SentencePiece 以及字节级的 Byte Pair Encoding 算法","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[39,40]},"content":"预训练数据的影响","children":[{"type":"list_item","depth":4,"payload":{"lines":[40,41]},"content":"混合来源: 注意小心确认不同来源的数据比例","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[41,42]},"content":"预训练数据的数量: 数据量一般多大合适？","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[42,43]},"content":"预训练数据的质量: 如何评价数据的质量?","children":[]}]}]},{"type":"heading","depth":2,"payload":{"lines":[44,45]},"content":"架构","children":[{"type":"list_item","depth":3,"payload":{"lines":[46,47]},"content":"主流架构","children":[{"type":"list_item","depth":4,"payload":{"lines":[47,48]},"content":"encoder-decoder 结构、因果解码器架、前缀解码器结构。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[48,49]},"content":"可以通过 MoE 对结构进行扩展","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[49,50]},"content":"详细配置","children":[{"type":"list_item","depth":4,"payload":{"lines":[50,51]},"content":"normalization: (前置)Layer Norm、RMS Norm、DeepNorm","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[51,52]},"content":"activation: GeLU、SwiGLU、GeGLU","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[52,53]},"content":"position encoder: Absolute、Relative、RoPE、Alibi","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[53,54]},"content":"注意力机制和偏置","children":[{"type":"list_item","depth":4,"payload":{"lines":[54,55]},"content":"FlashAttention","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[55,56]},"content":"预训练任务","children":[{"type":"list_item","depth":4,"payload":{"lines":[56,57]},"content":"LM: 自回归的建模，最大化似然函数 logP(xi|x &lt; i)","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[57,58]},"content":"DAM: 去噪自编码","children":[]}]}]},{"type":"heading","depth":2,"payload":{"lines":[59,60]},"content":"模型训练","children":[{"type":"list_item","depth":3,"payload":{"lines":[61,62]},"content":"优化设置","children":[{"type":"list_item","depth":4,"payload":{"lines":[62,63]},"content":"batch_size: 一般2048个例子或者400万个token。像GPT3动态调增大小。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[63,64]},"content":"learning_rate: 5e-5 ~ e-4","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[64,65]},"content":"optimizer: Adam(β1 = 0.9, β2 = 0.95, ε = 10−8), Adafactor","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[65,66]},"content":"稳定训练: weight decay、gradient clipping","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[66,67]},"content":"可扩展的训练技术","children":[{"type":"list_item","depth":4,"payload":{"lines":[67,68]},"content":"3D 并行: 数据并行、流水线并行、张量并行。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[68,69]},"content":"ZeRO: 优化器状态分区、梯度分区和参数分区。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[69,70]},"content":"混合精度训练: Brain Floating Point (BF16)","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[70,71]},"content":"整体训练建议: GPT-4引入堆栈新机制大模型进行性能预测。","children":[]}]}]}]},{"type":"heading","depth":1,"payload":{"lines":[72,73]},"content":"adaptation","children":[{"type":"heading","depth":2,"payload":{"lines":[74,75]},"content":"指令微调","children":[{"type":"list_item","depth":3,"payload":{"lines":[76,77]},"content":"目的: 增强 LLM 的能力","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[77,78]},"content":"做法: 构建指令格式实例，通过有监督的方式微调。","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[78,79]},"content":"格式化实例构建方法","children":[{"type":"list_item","depth":4,"payload":{"lines":[79,80]},"content":"格式化已有数据集: PromptSource、反转输入-输出对。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[80,81]},"content":"格式化人类需求: 将指令和人工答案作为训练实例。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[81,82]},"content":"构建实例的关键因素","children":[{"type":"list_item","depth":5,"payload":{"lines":[82,83]},"content":"增加指令: 扩大任务数量、合理选择每个任务对应的实例数量","children":[]},{"type":"list_item","depth":5,"payload":{"lines":[83,84]},"content":"设计格式: 添加任务描述和可选的示例。","children":[]}]}]},{"type":"list_item","depth":3,"payload":{"lines":[84,85]},"content":"指定微调策略","children":[{"type":"list_item","depth":4,"payload":{"lines":[85,86]},"content":"平衡数据分布: 比例采样每种实例、提高高质量数据比例、置一个最大容量(几千或几万)。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[86,87]},"content":"结合指令微调和预训练: 指令微调期间加入了预训练数据","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[87,88]},"content":"指令微调的效果: 性能改进、任务泛化性。","children":[]}]},{"type":"heading","depth":2,"payload":{"lines":[89,90]},"content":"对齐微调","children":[{"type":"list_item","depth":3,"payload":{"lines":[91,92]},"content":"对齐背景和标准","children":[{"type":"list_item","depth":4,"payload":{"lines":[92,93]},"content":"优点: 避免预期外的行为、符合人类期望","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[93,95]},"content":"缺点: 由于标准(例 如有用性, 诚实性和无害性)十分不同，<br>\n已有研究表明对齐微调可能会在某种程度上损害 LLM 的通用能力，这在相关研究 中被称为对齐税","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[95,96]},"content":"对齐标准: 有用性、诚实性、无害性。有效技术: 红队攻防。","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[96,97]},"content":"人类反馈的收集","children":[{"type":"list_item","depth":4,"payload":{"lines":[97,98]},"content":"标注人员选择: 高学历、一致性测试。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[98,99]},"content":"人类反馈收集: 基于排序(Elo评分系统)、基于问题、基于规则方法。","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[99,100]},"content":"基于人类反馈的强化学习","children":[{"type":"list_item","depth":4,"payload":{"lines":[100,101]},"content":"强化学习系统: 要对齐的PLM、人类反馈的奖励模型、训练LM的RL算法。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[101,102]},"content":"关键步骤: 监督微调(可选)、训练奖励模型(一般是排名的RM系统)、强化学习微调。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[102,105]},"content":"RL问题的策略(policy) 由 PLM 给出(将提示作为输入并返回输出文本)，<br>\n行动空间(action space)是 LM 的词表, 状态(state)是目前生成的 token 序列,<br>\n奖励(reward)则由 RM 提供, 并且通过 KL 散度来约束当前LM和初始LM直接的差异。","children":[]}]}]},{"type":"heading","depth":2,"payload":{"lines":[106,107]},"content":"高效微调","children":[{"type":"list_item","depth":3,"payload":{"lines":[108,109]},"content":"参数高效微调方法","children":[{"type":"list_item","depth":4,"payload":{"lines":[109,110]},"content":"关于Transformer语言模型的高效微调: 适配器微调、前缀微调、提示微调、低秩适配","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[110,111]},"content":"大语言模型上的参数高效微调","children":[{"type":"list_item","depth":4,"payload":{"lines":[111,112]},"content":"PEFT 代码库(包括 LoRA/AdaLoRA、前缀微调、P- Tuning和提示微调)","children":[]}]}]}]},{"type":"heading","depth":1,"payload":{"lines":[113,114]},"content":"utilization","children":[{"type":"heading","depth":2,"payload":{"lines":[115,116]},"content":"上下文学习(in-context learning, ICL)","children":[{"type":"list_item","depth":3,"payload":{"lines":[117,118]},"content":"形式","children":[{"type":"list_item","depth":4,"payload":{"lines":[118,121]},"content":"设 Dk = {f(x1,y1),...,f(xk,yk)} 代表由k个样例组成的一组示范，<br>\n其中 f(xk,yk) 是将第k个任务样例转换为自然语言提示的函数。<br>\n给定任务描述I、示范Dk以及新的输入查询xk+1，LLM 生成的输出yˆk+1 的预测。","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[121,122]},"content":"示范设计","children":[{"type":"list_item","depth":4,"payload":{"lines":[122,123]},"content":"需要考虑: 示范选择、示范格式、示范顺序等关键因素","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[123,124]},"content":"底层机制","children":[{"type":"list_item","depth":4,"payload":{"lines":[124,125]},"content":"预训练如何影响上下文学习? 研究表明ICL的性能主要取决于预训练语料的来源而非规模。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[125,127]},"content":"大语言模型如何实现上下文学习？将ICL视为隐式微调，<br>\n通过前向计算，LLM 生成关于示范的元梯度， 并通过注意力机制隐式地执行梯度下降。","children":[]}]}]},{"type":"heading","depth":2,"payload":{"lines":[128,129]},"content":"思维链提示(chain-of-thought prompting)","children":[{"type":"list_item","depth":3,"payload":{"lines":[130,131]},"content":"使用思维连的上下文学习","children":[{"type":"list_item","depth":4,"payload":{"lines":[131,132]},"content":"基本步骤:通过加入CoT推理步骤将每个示范⟨输入,输出⟩扩充为⟨输入,CoT,输出⟩","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[132,133]},"content":"小样本思维链","children":[{"type":"list_item","depth":5,"payload":{"lines":[133,135]},"content":"思维链提示设计: 使用多样的、复杂的CoT推理路径，<br>\nZero-shot-CoT 通过特别提示 LLM 来生成 CoT 推理路径","children":[]},{"type":"list_item","depth":5,"payload":{"lines":[135,136]},"content":"增强的思维链策略: CoT提示提供了更多推断答案的选项。","children":[]}]},{"type":"list_item","depth":4,"payload":{"lines":[136,137]},"content":"零样本思维链","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[137,138]},"content":"关于思维链的进一步讨论","children":[{"type":"list_item","depth":4,"payload":{"lines":[138,139]},"content":"CoT 何时适用于 LLM？只能有效增强有100亿或更多参数的足够大的模型","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[139,142]},"content":"LLM 为什么能够进行 CoT 推理?<br>\n能力的来源于代码进行训练。<br>\nCoT提示中的三个关键组成部分，即符号(symbols)、模式(patterns)和文本(text)。","children":[]}]}]}]},{"type":"heading","depth":1,"payload":{"lines":[143,144]},"content":"capability evaluation","children":[{"type":"heading","depth":2,"payload":{"lines":[145,146]},"content":"基础评测任","children":[{"type":"list_item","depth":3,"payload":{"lines":[147,148]},"content":"语言生成","children":[{"type":"list_item","depth":4,"payload":{"lines":[148,149]},"content":"语言建模: Penn Treebank、WikiText-103和Pile。目的: 提高precision降低perplexity。","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[149,150]},"content":"条件文本生成: 机器翻译、文本摘要和问答系统。自动化指标(如准确率、BLEU和ROUGE)和人类评分来评估性能","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[150,151]},"content":"代码合成任务: pass@k","children":[]}]},{"type":"list_item","depth":3,"payload":{"lines":[151,152]},"content":"知识利用: 闭卷问答、开卷问答、知识补全","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[152,153]},"content":"复杂推理: 知识推理、符号推理、数学推理","children":[]}]},{"type":"heading","depth":2,"payload":{"lines":[154,155]},"content":"高级能力评估","children":[{"type":"list_item","depth":3,"payload":{"lines":[156,157]},"content":"human alignment","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[157,158]},"content":"外部环境互动:","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[158,159]},"content":"利用外部工具: 搜索引擎、计算器和编译器","children":[]}]},{"type":"heading","depth":2,"payload":{"lines":[160,161]},"content":"公开基准和经验性分析","children":[{"type":"list_item","depth":3,"payload":{"lines":[162,163]},"content":"评测基准: MMLU、BIG-bench、HELM。","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[163,164]},"content":"大语言模型能力的综合分析: 通用能力(精通度、鲁棒性)、专业能力(医疗、教育、法律)。","children":[]}]}]}],"payload":{}},{})</script>
</body>
</html>
